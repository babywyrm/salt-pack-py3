diff -Naur a/doc/conf.py c/doc/conf.py
--- a/doc/conf.py	2019-07-02 10:15:06.947874715 -0600
+++ c/doc/conf.py	2019-07-02 10:58:03.167938594 -0600
@@ -137,7 +137,21 @@
     'tornado.web',
     'tornado.websocket',
     'tornado.locks',
-
+    'tornado4',
+    'tornado4.concurrent',
+    'tornado4.escape',
+    'tornado4.gen',
+    'tornado4.httpclient',
+    'tornado4.httpserver',
+    'tornado4.httputil',
+    'tornado4.ioloop',
+    'tornado4.iostream',
+    'tornado4.netutil',
+    'tornado4.simple_httpclient',
+    'tornado4.stack_context',
+    'tornado4.web',
+    'tornado4.websocket',
+    'tornado4.locks',
     'ws4py',
     'ws4py.server',
     'ws4py.server.cherrypyserver',
@@ -210,6 +224,7 @@
     sys.modules[mod_name] = Mock(mapping=MOCK_MODULES_MAPPING.get(mod_name))
 
 # Define a fake version attribute for the following libs.
+sys.modules['tornado4'].version_info = (0, 0, 0)
 sys.modules['libcloud'].__version__ = '0.0.0'
 sys.modules['msgpack'].version = (1, 0, 0)
 sys.modules['psutil'].version_info = (3, 0, 0)
diff -Naur a/salt/client/__init__.py c/salt/client/__init__.py
--- a/salt/client/__init__.py	2019-07-02 10:15:07.055874718 -0600
+++ c/salt/client/__init__.py	2019-07-02 10:58:03.167938594 -0600
@@ -68,7 +68,10 @@
 # pylint: enable=import-error
 
 # Import tornado
-import tornado.gen  # pylint: disable=F0401
+try:
+    import tornado4.gen as tornado_gen  # pylint: disable=F0401
+except ImportError:
+    import tornado.gen as tornado_gen   # pylint: disable=F0401
 
 log = logging.getLogger(__name__)
 
@@ -354,7 +357,7 @@
         _res = salt.utils.minions.CkMinions(self.opts).check_minions(tgt, tgt_type=expr_form)
         return _res['minions']
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def run_job_async(
             self,
             tgt,
@@ -409,7 +412,7 @@
             # Convert to generic client error and pass along message
             raise SaltClientError(general_exception)
 
-        raise tornado.gen.Return(self._check_pub_data(pub_data, listen=listen))
+        raise tornado_gen.Return(self._check_pub_data(pub_data, listen=listen))
 
     def cmd_async(
             self,
@@ -1774,7 +1777,7 @@
         return {'jid': payload['load']['jid'],
                 'minions': payload['load']['minions']}
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def pub_async(self,
                   tgt,
                   fun,
@@ -1856,7 +1859,7 @@
             # and try again if the key has changed
             key = self.__read_master_key()
             if key == self.key:
-                raise tornado.gen.Return(payload)
+                raise tornado_gen.Return(payload)
             self.key = key
             payload_kwargs['key'] = self.key
             payload = yield channel.send(payload_kwargs)
@@ -1874,12 +1877,12 @@
             raise PublishError(error)
 
         if not payload:
-            raise tornado.gen.Return(payload)
+            raise tornado_gen.Return(payload)
 
         # We have the payload, let's get rid of the channel fast(GC'ed faster)
         channel.close()
 
-        raise tornado.gen.Return({'jid': payload['load']['jid'],
+        raise tornado_gen.Return({'jid': payload['load']['jid'],
                                   'minions': payload['load']['minions']})
 
     def __del__(self):
diff -Naur a/salt/client/mixins.py c/salt/client/mixins.py
--- a/salt/client/mixins.py	2019-07-02 10:15:07.055874718 -0600
+++ c/salt/client/mixins.py	2019-07-02 10:58:03.171938594 -0600
@@ -33,7 +33,11 @@
 from salt.ext import six
 
 # Import 3rd-party libs
-import tornado.stack_context
+try:
+    from tornado4.stack_context import StackContext
+except ImportError:
+    from tornado.stack_context import StackContext
+
 
 log = logging.getLogger(__name__)
 
@@ -371,7 +375,7 @@
             func_globals['__jid_event__'].fire_event(data, 'new')
 
             # Initialize a context for executing the method.
-            with tornado.stack_context.StackContext(self.functions.context_dict.clone):
+            with StackContext(self.functions.context_dict.clone):
                 func = self.functions[fun]
                 try:
                     data['return'] = func(*args, **kwargs)
diff -Naur a/salt/crypt.py c/salt/crypt.py
--- a/salt/crypt.py	2019-07-02 10:15:07.023874717 -0600
+++ c/salt/crypt.py	2019-07-02 10:58:03.171938594 -0600
@@ -22,7 +22,15 @@
 import binascii
 import weakref
 import getpass
-import tornado.gen
+try:
+    import tornado4.gen as tornado_gen
+    from tornado4.ioloop import IOLoop
+    from tornado4.concurrent import Future as TornadoFuture
+except ImportError:
+    import tornado.gen as tornado_gen
+    from tornado.ioloop import IOLoop
+    from tornado.concurrent import Future as TornadoFuture
+
 
 # Import third party libs
 from salt.ext.six.moves import zip  # pylint: disable=import-error,redefined-builtin
@@ -453,7 +461,7 @@
         Only create one instance of AsyncAuth per __key()
         '''
         # do we have any mapping for this io_loop
-        io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        io_loop = io_loop or IOLoop.current()
         if io_loop not in AsyncAuth.instance_map:
             AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary()
         loop_instance_map = AsyncAuth.instance_map[io_loop]
@@ -507,7 +515,7 @@
         if not os.path.isfile(self.pub_path):
             self.get_keys()
 
-        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        self.io_loop = io_loop or IOLoop.current()
 
         salt.utils.crypt.reinit_crypto()
         key = self.__key(self.opts)
@@ -516,7 +524,7 @@
             creds = AsyncAuth.creds_map[key]
             self._creds = creds
             self._crypticle = Crypticle(self.opts, creds['aes'])
-            self._authenticate_future = tornado.concurrent.Future()
+            self._authenticate_future = TornadoFuture()
             self._authenticate_future.set_result(True)
         else:
             self.authenticate()
@@ -566,7 +574,7 @@
         if hasattr(self, '_authenticate_future') and not self._authenticate_future.done():
             future = self._authenticate_future
         else:
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             self._authenticate_future = future
             self.io_loop.add_callback(self._authenticate)
 
@@ -578,7 +586,7 @@
 
         return future
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _authenticate(self):
         '''
         Authenticate with the master, this method breaks the functional
@@ -626,7 +634,7 @@
                         log.info(
                             'Waiting %s seconds before retry.', acceptance_wait_time
                         )
-                        yield tornado.gen.sleep(acceptance_wait_time)
+                        yield tornado_gen.sleep(acceptance_wait_time)
                     if acceptance_wait_time < acceptance_wait_time_max:
                         acceptance_wait_time += acceptance_wait_time
                         log.debug(
@@ -660,7 +668,7 @@
         finally:
             channel.close()
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def sign_in(self, timeout=60, safe=True, tries=1, channel=None):
         '''
         Send a sign in request to the master, sets the key information and
@@ -710,9 +718,9 @@
         except SaltReqTimeoutError as e:
             if safe:
                 log.warning('SaltReqTimeoutError: %s', e)
-                raise tornado.gen.Return('retry')
+                raise tornado_gen.Return('retry')
             if self.opts.get('detect_mode') is True:
-                raise tornado.gen.Return('retry')
+                raise tornado_gen.Return('retry')
             else:
                 raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')
         finally:
@@ -721,7 +729,7 @@
 
         if not isinstance(payload, dict):
             log.error('Sign-in attempt failed: %s', payload)
-            raise tornado.gen.Return(False)
+            raise tornado_gen.Return(False)
         if 'load' in payload:
             if 'ret' in payload['load']:
                 if not payload['load']['ret']:
@@ -732,7 +740,7 @@
                             'for this minion on the Salt Master.\nThe Salt '
                             'Minion will attempt to to re-authenicate.'
                         )
-                        raise tornado.gen.Return('retry')
+                        raise tornado_gen.Return('retry')
                     else:
                         log.critical(
                             'The Salt Master has rejected this minion\'s public '
@@ -748,7 +756,7 @@
                         sys.exit(salt.defaults.exitcodes.EX_NOPERM)
                 # has the master returned that its maxed out with minions?
                 elif payload['load']['ret'] == 'full':
-                    raise tornado.gen.Return('full')
+                    raise tornado_gen.Return('full')
                 else:
                     log.error(
                         'The Salt Master has cached the public key for this '
@@ -756,7 +764,7 @@
                         'before attempting to re-authenticate',
                         self.opts['acceptance_wait_time']
                     )
-                    raise tornado.gen.Return('retry')
+                    raise tornado_gen.Return('retry')
         auth['aes'] = self.verify_master(payload, master_pub='token' in sign_in_payload)
         if not auth['aes']:
             log.critical(
@@ -779,7 +787,7 @@
                 if salt.utils.crypt.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != self.opts['master_finger']:
                     self._finger_fail(self.opts['master_finger'], m_pub_fn)
         auth['publish_port'] = payload['publish_port']
-        raise tornado.gen.Return(auth)
+        raise tornado_gen.Return(auth)
 
     def get_keys(self):
         '''
diff -Naur a/salt/crypt.py.orig c/salt/crypt.py.orig
--- a/salt/crypt.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/crypt.py.orig	2019-07-02 10:57:25.387937657 -0600
@@ -0,0 +1,1481 @@
+# -*- coding: utf-8 -*-
+'''
+The crypt module manages all of the cryptography functions for minions and
+masters, encrypting and decrypting payloads, preparing messages, and
+authenticating peers
+'''
+# Import python libs
+# NOTE: We can't use unicode_literals because this module implicitly uses
+# the Array class, which has incompatibilities with it.
+from __future__ import absolute_import, print_function
+import os
+import random
+import sys
+import copy
+import time
+import hmac
+import base64
+import hashlib
+import logging
+import stat
+import traceback
+import binascii
+import weakref
+import getpass
+import tornado.gen
+
+# Import third party libs
+from salt.ext.six.moves import zip  # pylint: disable=import-error,redefined-builtin
+from salt.ext import six
+
+try:
+    from M2Crypto import RSA, EVP, BIO
+    HAS_M2 = True
+except ImportError:
+    HAS_M2 = False
+
+if not HAS_M2:
+    try:
+        from Cryptodome.Cipher import AES, PKCS1_OAEP
+        from Cryptodome.Hash import SHA
+        from Cryptodome.PublicKey import RSA
+        from Cryptodome.Signature import PKCS1_v1_5
+        import Cryptodome.Random  # pylint: disable=W0611
+        HAS_CDOME = True
+    except ImportError:
+        HAS_CDOME = False
+
+if not HAS_M2 and not HAS_CDOME:
+    try:
+        from Crypto.Cipher import AES, PKCS1_OAEP
+        from Crypto.Hash import SHA
+        from Crypto.PublicKey import RSA
+        from Crypto.Signature import PKCS1_v1_5
+        # let this be imported, if possible
+        import Crypto.Random  # pylint: disable=W0611
+    except ImportError:
+        # No need for crypt in local mode
+        pass
+
+# Import salt libs
+import salt.defaults.exitcodes
+import salt.payload
+import salt.transport.client
+import salt.transport.frame
+import salt.utils.crypt
+import salt.utils.decorators
+import salt.utils.event
+import salt.utils.files
+import salt.utils.rsax931
+import salt.utils.sdb
+import salt.utils.stringutils
+import salt.utils.user
+import salt.utils.verify
+import salt.version
+from salt.exceptions import (
+    AuthenticationError, SaltClientError, SaltReqTimeoutError, MasterExit
+)
+
+log = logging.getLogger(__name__)
+
+
+def dropfile(cachedir, user=None):
+    '''
+    Set an AES dropfile to request the master update the publish session key
+    '''
+    dfn = os.path.join(cachedir, '.dfn')
+    # set a mask (to avoid a race condition on file creation) and store original.
+    with salt.utils.files.set_umask(0o277):
+        log.info('Rotating AES key')
+        if os.path.isfile(dfn):
+            log.info('AES key rotation already requested')
+            return
+
+        if os.path.isfile(dfn) and not os.access(dfn, os.W_OK):
+            os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
+        with salt.utils.files.fopen(dfn, 'wb+') as fp_:
+            fp_.write(b'')
+        os.chmod(dfn, stat.S_IRUSR)
+        if user:
+            try:
+                import pwd
+                uid = pwd.getpwnam(user).pw_uid
+                os.chown(dfn, uid, -1)
+            except (KeyError, ImportError, OSError, IOError):
+                pass
+
+
+def gen_keys(keydir, keyname, keysize, user=None, passphrase=None):
+    '''
+    Generate a RSA public keypair for use with salt
+
+    :param str keydir: The directory to write the keypair to
+    :param str keyname: The type of salt server for whom this key should be written. (i.e. 'master' or 'minion')
+    :param int keysize: The number of bits in the key
+    :param str user: The user on the system who should own this keypair
+    :param str passphrase: The passphrase which should be used to encrypt the private key
+
+    :rtype: str
+    :return: Path on the filesystem to the RSA private key
+    '''
+    base = os.path.join(keydir, keyname)
+    priv = '{0}.pem'.format(base)
+    pub = '{0}.pub'.format(base)
+
+    if HAS_M2:
+        gen = RSA.gen_key(keysize, 65537, lambda: None)
+    else:
+        salt.utils.crypt.reinit_crypto()
+        gen = RSA.generate(bits=keysize, e=65537)
+    if os.path.isfile(priv):
+        # Between first checking and the generation another process has made
+        # a key! Use the winner's key
+        return priv
+
+    # Do not try writing anything, if directory has no permissions.
+    if not os.access(keydir, os.W_OK):
+        raise IOError('Write access denied to "{0}" for user "{1}".'.format(os.path.abspath(keydir), getpass.getuser()))
+
+    with salt.utils.files.set_umask(0o277):
+        if HAS_M2:
+            # if passphrase is empty or None use no cipher
+            if not passphrase:
+                gen.save_pem(priv, cipher=None)
+            else:
+                gen.save_pem(
+                    priv,
+                    cipher='des_ede3_cbc',
+                    callback=lambda x: salt.utils.stringutils.to_bytes(passphrase))
+        else:
+            with salt.utils.files.fopen(priv, 'wb+') as f:
+                f.write(gen.exportKey('PEM', passphrase))
+    if HAS_M2:
+        gen.save_pub_key(pub)
+    else:
+        with salt.utils.files.fopen(pub, 'wb+') as f:
+            f.write(gen.publickey().exportKey('PEM'))
+    os.chmod(priv, 0o400)
+    if user:
+        try:
+            import pwd
+            uid = pwd.getpwnam(user).pw_uid
+            os.chown(priv, uid, -1)
+            os.chown(pub, uid, -1)
+        except (KeyError, ImportError, OSError):
+            # The specified user was not found, allow the backup systems to
+            # report the error
+            pass
+    return priv
+
+
+@salt.utils.decorators.memoize
+def _get_key_with_evict(path, timestamp, passphrase):
+    '''
+    Load a private key from disk.  `timestamp` above is intended to be the
+    timestamp of the file's last modification. This fn is memoized so if it is
+    called with the same path and timestamp (the file's last modified time) the
+    second time the result is returned from the memoiziation.  If the file gets
+    modified then the params are different and the key is loaded from disk.
+    '''
+    log.debug('salt.crypt._get_key_with_evict: Loading private key')
+    if HAS_M2:
+        key = RSA.load_key(path, lambda x: six.b(passphrase))
+    else:
+        with salt.utils.files.fopen(path) as f:
+            key = RSA.importKey(f.read(), passphrase)
+    return key
+
+
+def get_rsa_key(path, passphrase):
+    '''
+    Read a private key off the disk.  Poor man's simple cache in effect here,
+    we memoize the result of calling _get_rsa_with_evict.  This means the first
+    time _get_key_with_evict is called with a path and a timestamp the result
+    is cached.  If the file (the private key) does not change then its
+    timestamp will not change and the next time the result is returned from the
+    cache.  If the key DOES change the next time _get_rsa_with_evict is called
+    it is called with different parameters and the fn is run fully to retrieve
+    the key from disk.
+    '''
+    log.debug('salt.crypt.get_rsa_key: Loading private key')
+    return _get_key_with_evict(path, six.text_type(os.path.getmtime(path)), passphrase)
+
+
+def get_rsa_pub_key(path):
+    '''
+    Read a public key off the disk.
+    '''
+    log.debug('salt.crypt.get_rsa_pub_key: Loading public key')
+    if HAS_M2:
+        with salt.utils.files.fopen(path, 'rb') as f:
+            data = f.read().replace(b'RSA ', b'')
+        bio = BIO.MemoryBuffer(data)
+        key = RSA.load_pub_key_bio(bio)
+    else:
+        with salt.utils.files.fopen(path) as f:
+            key = RSA.importKey(f.read())
+    return key
+
+
+def sign_message(privkey_path, message, passphrase=None):
+    '''
+    Use Crypto.Signature.PKCS1_v1_5 to sign a message. Returns the signature.
+    '''
+    key = get_rsa_key(privkey_path, passphrase)
+    log.debug('salt.crypt.sign_message: Signing message.')
+    if HAS_M2:
+        md = EVP.MessageDigest('sha1')
+        md.update(salt.utils.stringutils.to_bytes(message))
+        digest = md.final()
+        return key.sign(digest)
+    else:
+        signer = PKCS1_v1_5.new(key)
+        return signer.sign(SHA.new(salt.utils.stringutils.to_bytes(message)))
+
+
+def verify_signature(pubkey_path, message, signature):
+    '''
+    Use Crypto.Signature.PKCS1_v1_5 to verify the signature on a message.
+    Returns True for valid signature.
+    '''
+    log.debug('salt.crypt.verify_signature: Loading public key')
+    pubkey = get_rsa_pub_key(pubkey_path)
+    log.debug('salt.crypt.verify_signature: Verifying signature')
+    if HAS_M2:
+        md = EVP.MessageDigest('sha1')
+        md.update(salt.utils.stringutils.to_bytes(message))
+        digest = md.final()
+        return pubkey.verify(digest, signature)
+    else:
+        verifier = PKCS1_v1_5.new(pubkey)
+        return verifier.verify(SHA.new(salt.utils.stringutils.to_bytes(message)), signature)
+
+
+def gen_signature(priv_path, pub_path, sign_path, passphrase=None):
+    '''
+    creates a signature for the given public-key with
+    the given private key and writes it to sign_path
+    '''
+
+    with salt.utils.files.fopen(pub_path) as fp_:
+        mpub_64 = fp_.read()
+
+    mpub_sig = sign_message(priv_path, mpub_64, passphrase)
+    mpub_sig_64 = binascii.b2a_base64(mpub_sig)
+    if os.path.isfile(sign_path):
+        return False
+    log.trace(
+        'Calculating signature for %s with %s',
+        os.path.basename(pub_path), os.path.basename(priv_path)
+    )
+
+    if os.path.isfile(sign_path):
+        log.trace(
+            'Signature file %s already exists, please remove it first and '
+            'try again', sign_path
+        )
+    else:
+        with salt.utils.files.fopen(sign_path, 'wb+') as sig_f:
+            sig_f.write(salt.utils.stringutils.to_bytes(mpub_sig_64))
+        log.trace('Wrote signature to %s', sign_path)
+    return True
+
+
+def private_encrypt(key, message):
+    '''
+    Generate an M2Crypto-compatible signature
+
+    :param Crypto.PublicKey.RSA._RSAobj key: The RSA key object
+    :param str message: The message to sign
+    :rtype: str
+    :return: The signature, or an empty string if the signature operation failed
+    '''
+    if HAS_M2:
+        return key.private_encrypt(message, salt.utils.rsax931.RSA_X931_PADDING)
+    else:
+        signer = salt.utils.rsax931.RSAX931Signer(key.exportKey('PEM'))
+        return signer.sign(message)
+
+
+def public_decrypt(pub, message):
+    '''
+    Verify an M2Crypto-compatible signature
+
+    :param Crypto.PublicKey.RSA._RSAobj key: The RSA public key object
+    :param str message: The signed message to verify
+    :rtype: str
+    :return: The message (or digest) recovered from the signature, or an
+        empty string if the verification failed
+    '''
+    if HAS_M2:
+        return pub.public_decrypt(message, salt.utils.rsax931.RSA_X931_PADDING)
+    else:
+        verifier = salt.utils.rsax931.RSAX931Verifier(pub.exportKey('PEM'))
+        return verifier.verify(message)
+
+
+class MasterKeys(dict):
+    '''
+    The Master Keys class is used to manage the RSA public key pair used for
+    authentication by the master.
+
+    It also generates a signing key-pair if enabled with master_sign_key_name.
+    '''
+    def __init__(self, opts):
+        super(MasterKeys, self).__init__()
+        self.opts = opts
+        self.pub_path = os.path.join(self.opts['pki_dir'], 'master.pub')
+        self.rsa_path = os.path.join(self.opts['pki_dir'], 'master.pem')
+
+        key_pass = salt.utils.sdb.sdb_get(self.opts['key_pass'], self.opts)
+        self.key = self.__get_keys(passphrase=key_pass)
+
+        self.pub_signature = None
+
+        # set names for the signing key-pairs
+        if opts['master_sign_pubkey']:
+
+            # if only the signature is available, use that
+            if opts['master_use_pubkey_signature']:
+                self.sig_path = os.path.join(self.opts['pki_dir'],
+                                             opts['master_pubkey_signature'])
+                if os.path.isfile(self.sig_path):
+                    with salt.utils.files.fopen(self.sig_path) as fp_:
+                        self.pub_signature = fp_.read()
+                    log.info(
+                        'Read %s\'s signature from %s',
+                        os.path.basename(self.pub_path),
+                        self.opts['master_pubkey_signature']
+                    )
+                else:
+                    log.error(
+                        'Signing the master.pub key with a signature is '
+                        'enabled but no signature file found at the defined '
+                        'location %s', self.sig_path
+                    )
+                    log.error(
+                        'The signature-file may be either named differently '
+                        'or has to be created with \'salt-key --gen-signature\''
+                    )
+                    sys.exit(1)
+
+            # create a new signing key-pair to sign the masters
+            # auth-replies when a minion tries to connect
+            else:
+                key_pass = salt.utils.sdb.sdb_get(self.opts['signing_key_pass'], self.opts)
+                self.pub_sign_path = os.path.join(self.opts['pki_dir'],
+                                                  opts['master_sign_key_name'] + '.pub')
+                self.rsa_sign_path = os.path.join(self.opts['pki_dir'],
+                                                  opts['master_sign_key_name'] + '.pem')
+                self.sign_key = self.__get_keys(name=opts['master_sign_key_name'])
+
+    # We need __setstate__ and __getstate__ to avoid pickling errors since
+    # some of the member variables correspond to Cython objects which are
+    # not picklable.
+    # These methods are only used when pickling so will not be used on
+    # non-Windows platforms.
+    def __setstate__(self, state):
+        self.__init__(state['opts'])
+
+    def __getstate__(self):
+        return {'opts': self.opts}
+
+    def __get_keys(self, name='master', passphrase=None):
+        '''
+        Returns a key object for a key in the pki-dir
+        '''
+        path = os.path.join(self.opts['pki_dir'],
+                            name + '.pem')
+        if not os.path.exists(path):
+            log.info('Generating %s keys: %s', name, self.opts['pki_dir'])
+            gen_keys(self.opts['pki_dir'],
+                     name,
+                     self.opts['keysize'],
+                     self.opts.get('user'),
+                     passphrase)
+        if HAS_M2:
+            key_error = RSA.RSAError
+        else:
+            key_error = ValueError
+        try:
+            key = get_rsa_key(path, passphrase)
+        except key_error as e:
+            message = 'Unable to read key: {0}; passphrase may be incorrect'.format(path)
+            log.error(message)
+            raise MasterExit(message)
+        log.debug('Loaded %s key: %s', name, path)
+        return key
+
+    def get_pub_str(self, name='master'):
+        '''
+        Return the string representation of a public key
+        in the pki-directory
+        '''
+        path = os.path.join(self.opts['pki_dir'],
+                            name + '.pub')
+        if not os.path.isfile(path):
+            key = self.__get_keys()
+            if HAS_M2:
+                key.save_pub_key(path)
+            else:
+                with salt.utils.files.fopen(path, 'wb+') as wfh:
+                    wfh.write(key.publickey().exportKey('PEM'))
+        with salt.utils.files.fopen(path) as rfh:
+            return rfh.read()
+
+    def get_mkey_paths(self):
+        return self.pub_path, self.rsa_path
+
+    def get_sign_paths(self):
+        return self.pub_sign_path, self.rsa_sign_path
+
+    def pubkey_signature(self):
+        '''
+        returns the base64 encoded signature from the signature file
+        or None if the master has its own signing keys
+        '''
+        return self.pub_signature
+
+
+class AsyncAuth(object):
+    '''
+    Set up an Async object to maintain authentication with the salt master
+    '''
+    # This class is only a singleton per minion/master pair
+    # mapping of io_loop -> {key -> auth}
+    instance_map = weakref.WeakKeyDictionary()
+
+    # mapping of key -> creds
+    creds_map = {}
+
+    def __new__(cls, opts, io_loop=None):
+        '''
+        Only create one instance of AsyncAuth per __key()
+        '''
+        # do we have any mapping for this io_loop
+        io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        if io_loop not in AsyncAuth.instance_map:
+            AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary()
+        loop_instance_map = AsyncAuth.instance_map[io_loop]
+
+        key = cls.__key(opts)
+        auth = loop_instance_map.get(key)
+        if auth is None:
+            log.debug('Initializing new AsyncAuth for %s', key)
+            # we need to make a local variable for this, as we are going to store
+            # it in a WeakValueDictionary-- which will remove the item if no one
+            # references it-- this forces a reference while we return to the caller
+            auth = object.__new__(cls)
+            auth.__singleton_init__(opts, io_loop=io_loop)
+            loop_instance_map[key] = auth
+        else:
+            log.debug('Re-using AsyncAuth for %s', key)
+        return auth
+
+    @classmethod
+    def __key(cls, opts, io_loop=None):
+        return (opts['pki_dir'],     # where the keys are stored
+                opts['id'],          # minion ID
+                opts['master_uri'],  # master ID
+                )
+
+    # has to remain empty for singletons, since __init__ will *always* be called
+    def __init__(self, opts, io_loop=None):
+        pass
+
+    # an init for the singleton instance to call
+    def __singleton_init__(self, opts, io_loop=None):
+        '''
+        Init an Auth instance
+
+        :param dict opts: Options for this server
+        :return: Auth instance
+        :rtype: Auth
+        '''
+        self.opts = opts
+        if six.PY2:
+            self.token = Crypticle.generate_key_string()
+        else:
+            self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())
+        self.serial = salt.payload.Serial(self.opts)
+        self.pub_path = os.path.join(self.opts['pki_dir'], 'minion.pub')
+        self.rsa_path = os.path.join(self.opts['pki_dir'], 'minion.pem')
+        if self.opts['__role'] == 'syndic':
+            self.mpub = 'syndic_master.pub'
+        else:
+            self.mpub = 'minion_master.pub'
+        if not os.path.isfile(self.pub_path):
+            self.get_keys()
+
+        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+
+        salt.utils.crypt.reinit_crypto()
+        key = self.__key(self.opts)
+        # TODO: if we already have creds for this key, lets just re-use
+        if key in AsyncAuth.creds_map:
+            creds = AsyncAuth.creds_map[key]
+            self._creds = creds
+            self._crypticle = Crypticle(self.opts, creds['aes'])
+            self._authenticate_future = tornado.concurrent.Future()
+            self._authenticate_future.set_result(True)
+        else:
+            self.authenticate()
+
+    def __deepcopy__(self, memo):
+        cls = self.__class__
+        result = cls.__new__(cls, copy.deepcopy(self.opts, memo), io_loop=None)
+        memo[id(self)] = result
+        for key in self.__dict__:
+            if key in ('io_loop',):
+                # The io_loop has a thread Lock which will fail to be deep
+                # copied. Skip it because it will just be recreated on the
+                # new copy.
+                continue
+            setattr(result, key, copy.deepcopy(self.__dict__[key], memo))
+        return result
+
+    @property
+    def creds(self):
+        return self._creds
+
+    @property
+    def crypticle(self):
+        return self._crypticle
+
+    @property
+    def authenticated(self):
+        return hasattr(self, '_authenticate_future') and \
+               self._authenticate_future.done() and \
+               self._authenticate_future.exception() is None
+
+    def invalidate(self):
+        if self.authenticated:
+            del self._authenticate_future
+            key = self.__key(self.opts)
+            if key in AsyncAuth.creds_map:
+                del AsyncAuth.creds_map[key]
+
+    def authenticate(self, callback=None):
+        '''
+        Ask for this client to reconnect to the origin
+
+        This function will de-dupe all calls here and return a *single* future
+        for the sign-in-- whis way callers can all assume there aren't others
+        '''
+        # if an auth is in flight-- and not done-- just pass that back as the future to wait on
+        if hasattr(self, '_authenticate_future') and not self._authenticate_future.done():
+            future = self._authenticate_future
+        else:
+            future = tornado.concurrent.Future()
+            self._authenticate_future = future
+            self.io_loop.add_callback(self._authenticate)
+
+        if callback is not None:
+            def handle_future(future):
+                response = future.result()
+                self.io_loop.add_callback(callback, response)
+            future.add_done_callback(handle_future)
+
+        return future
+
+    @tornado.gen.coroutine
+    def _authenticate(self):
+        '''
+        Authenticate with the master, this method breaks the functional
+        paradigm, it will update the master information from a fresh sign
+        in, signing in can occur as often as needed to keep up with the
+        revolving master AES key.
+
+        :rtype: Crypticle
+        :returns: A crypticle used for encryption operations
+        '''
+        acceptance_wait_time = self.opts['acceptance_wait_time']
+        acceptance_wait_time_max = self.opts['acceptance_wait_time_max']
+        if not acceptance_wait_time_max:
+            acceptance_wait_time_max = acceptance_wait_time
+        creds = None
+
+        channel = salt.transport.client.AsyncReqChannel.factory(self.opts,
+                                                                crypt='clear',
+                                                                io_loop=self.io_loop)
+        try:
+            error = None
+            while True:
+                try:
+                    creds = yield self.sign_in(channel=channel)
+                except SaltClientError as exc:
+                    error = exc
+                    break
+                if creds == 'retry':
+                    if self.opts.get('detect_mode') is True:
+                        error = SaltClientError('Detect mode is on')
+                        break
+                    if self.opts.get('caller'):
+                        # We have a list of masters, so we should break
+                        # and try the next one in the list.
+                        if self.opts.get('local_masters', None):
+                            error = SaltClientError('Minion failed to authenticate'
+                                                    ' with the master, has the '
+                                                    'minion key been accepted?')
+                            break
+                        else:
+                            print('Minion failed to authenticate with the master, '
+                                  'has the minion key been accepted?')
+                            sys.exit(2)
+                    if acceptance_wait_time:
+                        log.info(
+                            'Waiting %s seconds before retry.', acceptance_wait_time
+                        )
+                        yield tornado.gen.sleep(acceptance_wait_time)
+                    if acceptance_wait_time < acceptance_wait_time_max:
+                        acceptance_wait_time += acceptance_wait_time
+                        log.debug(
+                            'Authentication wait time is %s', acceptance_wait_time
+                        )
+                    continue
+                break
+            if not isinstance(creds, dict) or 'aes' not in creds:
+                if self.opts.get('detect_mode') is True:
+                    error = SaltClientError('-|RETRY|-')
+                try:
+                    del AsyncAuth.creds_map[self.__key(self.opts)]
+                except KeyError:
+                    pass
+                if not error:
+                    error = SaltClientError('Attempt to authenticate with the salt master failed')
+                self._authenticate_future.set_exception(error)
+            else:
+                key = self.__key(self.opts)
+                AsyncAuth.creds_map[key] = creds
+                self._creds = creds
+                self._crypticle = Crypticle(self.opts, creds['aes'])
+                self._authenticate_future.set_result(True)  # mark the sign-in as complete
+                # Notify the bus about creds change
+                if self.opts.get('auth_events') is True:
+                    event = salt.utils.event.get_event(self.opts.get('__role'), opts=self.opts, listen=False)
+                    event.fire_event(
+                        {'key': key, 'creds': creds},
+                        salt.utils.event.tagify(prefix='auth', suffix='creds')
+                    )
+        finally:
+            channel.close()
+
+    @tornado.gen.coroutine
+    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):
+        '''
+        Send a sign in request to the master, sets the key information and
+        returns a dict containing the master publish interface to bind to
+        and the decrypted aes key for transport decryption.
+
+        :param int timeout: Number of seconds to wait before timing out the sign-in request
+        :param bool safe: If True, do not raise an exception on timeout. Retry instead.
+        :param int tries: The number of times to try to authenticate before giving up.
+
+        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set
+
+        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary
+        with the publication port and the shared AES key.
+
+        '''
+        auth = {}
+
+        auth_timeout = self.opts.get('auth_timeout', None)
+        if auth_timeout is not None:
+            timeout = auth_timeout
+        auth_safemode = self.opts.get('auth_safemode', None)
+        if auth_safemode is not None:
+            safe = auth_safemode
+        auth_tries = self.opts.get('auth_tries', None)
+        if auth_tries is not None:
+            tries = auth_tries
+
+        m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)
+
+        auth['master_uri'] = self.opts['master_uri']
+
+        close_channel = False
+        if not channel:
+            close_channel = True
+            channel = salt.transport.client.AsyncReqChannel.factory(self.opts,
+                                                                    crypt='clear',
+                                                                    io_loop=self.io_loop)
+
+        sign_in_payload = self.minion_sign_in_payload()
+        try:
+            payload = yield channel.send(
+                sign_in_payload,
+                tries=tries,
+                timeout=timeout
+            )
+        except SaltReqTimeoutError as e:
+            if safe:
+                log.warning('SaltReqTimeoutError: %s', e)
+                raise tornado.gen.Return('retry')
+            if self.opts.get('detect_mode') is True:
+                raise tornado.gen.Return('retry')
+            else:
+                raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')
+        finally:
+            if close_channel:
+                channel.close()
+
+        if not isinstance(payload, dict):
+            log.error('Sign-in attempt failed: %s', payload)
+            raise tornado.gen.Return(False)
+        if 'load' in payload:
+            if 'ret' in payload['load']:
+                if not payload['load']['ret']:
+                    if self.opts['rejected_retry']:
+                        log.error(
+                            'The Salt Master has rejected this minion\'s public '
+                            'key.\nTo repair this issue, delete the public key '
+                            'for this minion on the Salt Master.\nThe Salt '
+                            'Minion will attempt to to re-authenicate.'
+                        )
+                        raise tornado.gen.Return('retry')
+                    else:
+                        log.critical(
+                            'The Salt Master has rejected this minion\'s public '
+                            'key!\nTo repair this issue, delete the public key '
+                            'for this minion on the Salt Master and restart this '
+                            'minion.\nOr restart the Salt Master in open mode to '
+                            'clean out the keys. The Salt Minion will now exit.'
+                        )
+                        # Add a random sleep here for systems that are using a
+                        # a service manager to immediately restart the service
+                        # to avoid overloading the system
+                        time.sleep(random.randint(10, 20))
+                        sys.exit(salt.defaults.exitcodes.EX_NOPERM)
+                # has the master returned that its maxed out with minions?
+                elif payload['load']['ret'] == 'full':
+                    raise tornado.gen.Return('full')
+                else:
+                    log.error(
+                        'The Salt Master has cached the public key for this '
+                        'node, this salt minion will wait for %s seconds '
+                        'before attempting to re-authenticate',
+                        self.opts['acceptance_wait_time']
+                    )
+                    raise tornado.gen.Return('retry')
+        auth['aes'] = self.verify_master(payload, master_pub='token' in sign_in_payload)
+        if not auth['aes']:
+            log.critical(
+                'The Salt Master server\'s public key did not authenticate!\n'
+                'The master may need to be updated if it is a version of Salt '
+                'lower than %s, or\n'
+                'If you are confident that you are connecting to a valid Salt '
+                'Master, then remove the master public key and restart the '
+                'Salt Minion.\nThe master public key can be found '
+                'at:\n%s', salt.version.__version__, m_pub_fn
+            )
+            raise SaltClientError('Invalid master key')
+        if self.opts.get('syndic_master', False):  # Is syndic
+            syndic_finger = self.opts.get('syndic_finger', self.opts.get('master_finger', False))
+            if syndic_finger:
+                if salt.utils.crypt.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != syndic_finger:
+                    self._finger_fail(syndic_finger, m_pub_fn)
+        else:
+            if self.opts.get('master_finger', False):
+                if salt.utils.crypt.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != self.opts['master_finger']:
+                    self._finger_fail(self.opts['master_finger'], m_pub_fn)
+        auth['publish_port'] = payload['publish_port']
+        raise tornado.gen.Return(auth)
+
+    def get_keys(self):
+        '''
+        Return keypair object for the minion.
+
+        :rtype: Crypto.PublicKey.RSA._RSAobj
+        :return: The RSA keypair
+        '''
+        # Make sure all key parent directories are accessible
+        user = self.opts.get('user', 'root')
+        salt.utils.verify.check_path_traversal(self.opts['pki_dir'], user)
+
+        if not os.path.exists(self.rsa_path):
+            log.info('Generating keys: %s', self.opts['pki_dir'])
+            gen_keys(self.opts['pki_dir'],
+                     'minion',
+                     self.opts['keysize'],
+                     self.opts.get('user'))
+        key = get_rsa_key(self.rsa_path, None)
+        log.debug('Loaded minion key: %s', self.rsa_path)
+        return key
+
+    def gen_token(self, clear_tok):
+        '''
+        Encrypt a string with the minion private key to verify identity
+        with the master.
+
+        :param str clear_tok: A plaintext token to encrypt
+        :return: Encrypted token
+        :rtype: str
+        '''
+        return private_encrypt(self.get_keys(), clear_tok)
+
+    def minion_sign_in_payload(self):
+        '''
+        Generates the payload used to authenticate with the master
+        server. This payload consists of the passed in id_ and the ssh
+        public key to encrypt the AES key sent back from the master.
+
+        :return: Payload dictionary
+        :rtype: dict
+        '''
+        payload = {}
+        payload['cmd'] = '_auth'
+        payload['id'] = self.opts['id']
+        if 'autosign_grains' in self.opts:
+            autosign_grains = {}
+            for grain in self.opts['autosign_grains']:
+                autosign_grains[grain] = self.opts['grains'].get(grain, None)
+            payload['autosign_grains'] = autosign_grains
+        try:
+            pubkey_path = os.path.join(self.opts['pki_dir'], self.mpub)
+            pub = get_rsa_pub_key(pubkey_path)
+            if HAS_M2:
+                payload['token'] = pub.public_encrypt(self.token, RSA.pkcs1_oaep_padding)
+            else:
+                cipher = PKCS1_OAEP.new(pub)
+                payload['token'] = cipher.encrypt(self.token)
+        except Exception:
+            pass
+        with salt.utils.files.fopen(self.pub_path) as f:
+            payload['pub'] = f.read()
+        return payload
+
+    def decrypt_aes(self, payload, master_pub=True):
+        '''
+        This function is used to decrypt the AES seed phrase returned from
+        the master server. The seed phrase is decrypted with the SSH RSA
+        host key.
+
+        Pass in the encrypted AES key.
+        Returns the decrypted AES seed key, a string
+
+        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:
+            'aes': The shared AES key
+            'enc': The format of the message. ('clear', 'pub', etc)
+            'sig': The message signature
+            'publish_port': The TCP port which published the message
+            'token': The encrypted token used to verify the message.
+            'pub_key': The public key of the sender.
+
+        :rtype: str
+        :return: The decrypted token that was provided, with padding.
+
+        :rtype: str
+        :return: The decrypted AES seed key
+        '''
+        if self.opts.get('auth_trb', False):
+            log.warning('Auth Called: %s', ''.join(traceback.format_stack()))
+        else:
+            log.debug('Decrypting the current master AES key')
+        key = self.get_keys()
+        if HAS_M2:
+            key_str = key.private_decrypt(payload['aes'],
+                                          RSA.pkcs1_oaep_padding)
+        else:
+            cipher = PKCS1_OAEP.new(key)
+            key_str = cipher.decrypt(payload['aes'])
+        if 'sig' in payload:
+            m_path = os.path.join(self.opts['pki_dir'], self.mpub)
+            if os.path.exists(m_path):
+                try:
+                    mkey = get_rsa_pub_key(m_path)
+                except Exception:
+                    return '', ''
+                digest = hashlib.sha256(key_str).hexdigest()
+                if six.PY3:
+                    digest = salt.utils.stringutils.to_bytes(digest)
+                if HAS_M2:
+                    m_digest = public_decrypt(mkey, payload['sig'])
+                else:
+                    m_digest = public_decrypt(mkey.publickey(), payload['sig'])
+                if m_digest != digest:
+                    return '', ''
+        else:
+            return '', ''
+
+        if six.PY3:
+            key_str = salt.utils.stringutils.to_str(key_str)
+
+        if '_|-' in key_str:
+            return key_str.split('_|-')
+        else:
+            if 'token' in payload:
+                if HAS_M2:
+                    token = key.private_decrypt(payload['token'],
+                                                RSA.pkcs1_oaep_padding)
+                else:
+                    token = cipher.decrypt(payload['token'])
+                return key_str, token
+            elif not master_pub:
+                return key_str, ''
+        return '', ''
+
+    def verify_pubkey_sig(self, message, sig):
+        '''
+        Wraps the verify_signature method so we have
+        additional checks.
+
+        :rtype: bool
+        :return: Success or failure of public key verification
+        '''
+        if self.opts['master_sign_key_name']:
+            path = os.path.join(self.opts['pki_dir'],
+                                self.opts['master_sign_key_name'] + '.pub')
+
+            if os.path.isfile(path):
+                res = verify_signature(path,
+                                       message,
+                                       binascii.a2b_base64(sig))
+            else:
+                log.error(
+                    'Verification public key %s does not exist. You need to '
+                    'copy it from the master to the minions pki directory',
+                    os.path.basename(path)
+                )
+                return False
+            if res:
+                log.debug(
+                    'Successfully verified signature of master public key '
+                    'with verification public key %s',
+                    self.opts['master_sign_key_name'] + '.pub'
+                )
+                return True
+            else:
+                log.debug('Failed to verify signature of public key')
+                return False
+        else:
+            log.error(
+                'Failed to verify the signature of the message because the '
+                'verification key-pairs name is not defined. Please make '
+                'sure that master_sign_key_name is defined.'
+            )
+            return False
+
+    def verify_signing_master(self, payload):
+        try:
+            if self.verify_pubkey_sig(payload['pub_key'],
+                                      payload['pub_sig']):
+                log.info(
+                    'Received signed and verified master pubkey from master %s',
+                    self.opts['master']
+                )
+                m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)
+                uid = salt.utils.user.get_uid(self.opts.get('user', None))
+                with salt.utils.files.fpopen(m_pub_fn, 'wb+', uid=uid) as wfh:
+                    wfh.write(salt.utils.stringutils.to_bytes(payload['pub_key']))
+                return True
+            else:
+                log.error(
+                    'Received signed public-key from master %s but signature '
+                    'verification failed!', self.opts['master']
+                )
+                return False
+        except Exception as sign_exc:
+            log.error(
+                'There was an error while verifying the masters public-key '
+                'signature'
+            )
+            raise Exception(sign_exc)
+
+    def check_auth_deps(self, payload):
+        '''
+        Checks if both master and minion either sign (master) and
+        verify (minion). If one side does not, it should fail.
+
+        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:
+            'aes': The shared AES key
+            'enc': The format of the message. ('clear', 'pub', 'aes')
+            'publish_port': The TCP port which published the message
+            'token': The encrypted token used to verify the message.
+            'pub_key': The RSA public key of the sender.
+        '''
+        # master and minion sign and verify
+        if 'pub_sig' in payload and self.opts['verify_master_pubkey_sign']:
+            return True
+        # master and minion do NOT sign and do NOT verify
+        elif 'pub_sig' not in payload and not self.opts['verify_master_pubkey_sign']:
+            return True
+
+        # master signs, but minion does NOT verify
+        elif 'pub_sig' in payload and not self.opts['verify_master_pubkey_sign']:
+            log.error('The masters sent its public-key signature, but signature '
+                      'verification is not enabled on the minion. Either enable '
+                      'signature verification on the minion or disable signing '
+                      'the public key on the master!')
+            return False
+        # master does NOT sign but minion wants to verify
+        elif 'pub_sig' not in payload and self.opts['verify_master_pubkey_sign']:
+            log.error('The master did not send its public-key signature, but '
+                      'signature verification is enabled on the minion. Either '
+                      'disable signature verification on the minion or enable '
+                      'signing the public on the master!')
+            return False
+
+    def extract_aes(self, payload, master_pub=True):
+        '''
+        Return the AES key received from the master after the minion has been
+        successfully authenticated.
+
+        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:
+            'aes': The shared AES key
+            'enc': The format of the message. ('clear', 'pub', etc)
+            'publish_port': The TCP port which published the message
+            'token': The encrypted token used to verify the message.
+            'pub_key': The RSA public key of the sender.
+
+        :rtype: str
+        :return: The shared AES key received from the master.
+        '''
+        if master_pub:
+            try:
+                aes, token = self.decrypt_aes(payload, master_pub)
+                if token != self.token:
+                    log.error(
+                        'The master failed to decrypt the random minion token'
+                    )
+                    return ''
+            except Exception:
+                log.error(
+                    'The master failed to decrypt the random minion token'
+                )
+                return ''
+            return aes
+        else:
+            aes, token = self.decrypt_aes(payload, master_pub)
+            return aes
+
+    def verify_master(self, payload, master_pub=True):
+        '''
+        Verify that the master is the same one that was previously accepted.
+
+        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:
+            'aes': The shared AES key
+            'enc': The format of the message. ('clear', 'pub', etc)
+            'publish_port': The TCP port which published the message
+            'token': The encrypted token used to verify the message.
+            'pub_key': The RSA public key of the sender.
+        :param bool master_pub: Operate as if minion had no master pubkey when it sent auth request, i.e. don't verify
+        the minion signature
+
+        :rtype: str
+        :return: An empty string on verification failure. On success, the decrypted AES message in the payload.
+        '''
+        m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)
+        m_pub_exists = os.path.isfile(m_pub_fn)
+        if m_pub_exists and master_pub and not self.opts['open_mode']:
+            with salt.utils.files.fopen(m_pub_fn) as fp_:
+                local_master_pub = fp_.read()
+
+            if payload['pub_key'].replace('\n', '').replace('\r', '') != \
+                    local_master_pub.replace('\n', '').replace('\r', ''):
+                if not self.check_auth_deps(payload):
+                    return ''
+
+                if self.opts['verify_master_pubkey_sign']:
+                    if self.verify_signing_master(payload):
+                        return self.extract_aes(payload, master_pub=False)
+                    else:
+                        return ''
+                else:
+                    # This is not the last master we connected to
+                    log.error(
+                        'The master key has changed, the salt master could '
+                        'have been subverted, verify salt master\'s public '
+                        'key'
+                    )
+                    return ''
+
+            else:
+                if not self.check_auth_deps(payload):
+                    return ''
+                # verify the signature of the pubkey even if it has
+                # not changed compared with the one we already have
+                if self.opts['always_verify_signature']:
+                    if self.verify_signing_master(payload):
+                        return self.extract_aes(payload)
+                    else:
+                        log.error(
+                            'The masters public could not be verified. Is the '
+                            'verification pubkey %s up to date?',
+                            self.opts['master_sign_key_name'] + '.pub'
+                        )
+                        return ''
+
+                else:
+                    return self.extract_aes(payload)
+        else:
+            if not self.check_auth_deps(payload):
+                return ''
+
+            # verify the masters pubkey signature if the minion
+            # has not received any masters pubkey before
+            if self.opts['verify_master_pubkey_sign']:
+                if self.verify_signing_master(payload):
+                    return self.extract_aes(payload, master_pub=False)
+                else:
+                    return ''
+            else:
+                if not m_pub_exists:
+                    # the minion has not received any masters pubkey yet, write
+                    # the newly received pubkey to minion_master.pub
+                    with salt.utils.files.fopen(m_pub_fn, 'wb+') as fp_:
+                        fp_.write(salt.utils.stringutils.to_bytes(payload['pub_key']))
+                return self.extract_aes(payload, master_pub=False)
+
+    def _finger_fail(self, finger, master_key):
+        log.critical(
+            'The specified fingerprint in the master configuration '
+            'file:\n%s\nDoes not match the authenticating master\'s '
+            'key:\n%s\nVerify that the configured fingerprint '
+            'matches the fingerprint of the correct master and that '
+            'this minion is not subject to a man-in-the-middle attack.',
+            finger,
+            salt.utils.crypt.pem_finger(master_key, sum_type=self.opts['hash_type'])
+        )
+        sys.exit(42)
+
+
+# TODO: remove, we should just return a sync wrapper of AsyncAuth
+class SAuth(AsyncAuth):
+    '''
+    Set up an object to maintain authentication with the salt master
+    '''
+    # This class is only a singleton per minion/master pair
+    instances = weakref.WeakValueDictionary()
+
+    def __new__(cls, opts, io_loop=None):
+        '''
+        Only create one instance of SAuth per __key()
+        '''
+        key = cls.__key(opts)
+        auth = SAuth.instances.get(key)
+        if auth is None:
+            log.debug('Initializing new SAuth for %s', key)
+            auth = object.__new__(cls)
+            auth.__singleton_init__(opts)
+            SAuth.instances[key] = auth
+        else:
+            log.debug('Re-using SAuth for %s', key)
+        return auth
+
+    @classmethod
+    def __key(cls, opts, io_loop=None):
+        return (opts['pki_dir'],     # where the keys are stored
+                opts['id'],          # minion ID
+                opts['master_uri'],  # master ID
+                )
+
+    # has to remain empty for singletons, since __init__ will *always* be called
+    def __init__(self, opts, io_loop=None):
+        super(SAuth, self).__init__(opts, io_loop=io_loop)
+
+    # an init for the singleton instance to call
+    def __singleton_init__(self, opts, io_loop=None):
+        '''
+        Init an Auth instance
+
+        :param dict opts: Options for this server
+        :return: Auth instance
+        :rtype: Auth
+        '''
+        self.opts = opts
+        if six.PY2:
+            self.token = Crypticle.generate_key_string()
+        else:
+            self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())
+        self.serial = salt.payload.Serial(self.opts)
+        self.pub_path = os.path.join(self.opts['pki_dir'], 'minion.pub')
+        self.rsa_path = os.path.join(self.opts['pki_dir'], 'minion.pem')
+        if 'syndic_master' in self.opts:
+            self.mpub = 'syndic_master.pub'
+        elif 'alert_master' in self.opts:
+            self.mpub = 'monitor_master.pub'
+        else:
+            self.mpub = 'minion_master.pub'
+        if not os.path.isfile(self.pub_path):
+            self.get_keys()
+
+    @property
+    def creds(self):
+        if not hasattr(self, '_creds'):
+            self.authenticate()
+        return self._creds
+
+    @property
+    def crypticle(self):
+        if not hasattr(self, '_crypticle'):
+            self.authenticate()
+        return self._crypticle
+
+    def authenticate(self, _=None):  # TODO: remove unused var
+        '''
+        Authenticate with the master, this method breaks the functional
+        paradigm, it will update the master information from a fresh sign
+        in, signing in can occur as often as needed to keep up with the
+        revolving master AES key.
+
+        :rtype: Crypticle
+        :returns: A crypticle used for encryption operations
+        '''
+        acceptance_wait_time = self.opts['acceptance_wait_time']
+        acceptance_wait_time_max = self.opts['acceptance_wait_time_max']
+        channel = salt.transport.client.ReqChannel.factory(self.opts, crypt='clear')
+        if not acceptance_wait_time_max:
+            acceptance_wait_time_max = acceptance_wait_time
+        try:
+            while True:
+                creds = self.sign_in(channel=channel)
+                if creds == 'retry':
+                    if self.opts.get('caller'):
+                        # We have a list of masters, so we should break
+                        # and try the next one in the list.
+                        if self.opts.get('local_masters', None):
+                            error = SaltClientError('Minion failed to authenticate'
+                                                    ' with the master, has the '
+                                                    'minion key been accepted?')
+                            break
+                        else:
+                            print('Minion failed to authenticate with the master, '
+                                  'has the minion key been accepted?')
+                            sys.exit(2)
+                    if acceptance_wait_time:
+                        log.info('Waiting %s seconds before retry.', acceptance_wait_time)
+                        time.sleep(acceptance_wait_time)
+                    if acceptance_wait_time < acceptance_wait_time_max:
+                        acceptance_wait_time += acceptance_wait_time
+                        log.debug('Authentication wait time is %s', acceptance_wait_time)
+                    continue
+                break
+            self._creds = creds
+            self._crypticle = Crypticle(self.opts, creds['aes'])
+        finally:
+            channel.close()
+
+    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):
+        '''
+        Send a sign in request to the master, sets the key information and
+        returns a dict containing the master publish interface to bind to
+        and the decrypted aes key for transport decryption.
+
+        :param int timeout: Number of seconds to wait before timing out the sign-in request
+        :param bool safe: If True, do not raise an exception on timeout. Retry instead.
+        :param int tries: The number of times to try to authenticate before giving up.
+
+        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set
+
+        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary
+        with the publication port and the shared AES key.
+
+        '''
+        auth = {}
+
+        auth_timeout = self.opts.get('auth_timeout', None)
+        if auth_timeout is not None:
+            timeout = auth_timeout
+        auth_safemode = self.opts.get('auth_safemode', None)
+        if auth_safemode is not None:
+            safe = auth_safemode
+        auth_tries = self.opts.get('auth_tries', None)
+        if auth_tries is not None:
+            tries = auth_tries
+
+        m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)
+
+        auth['master_uri'] = self.opts['master_uri']
+
+        close_channel = False
+        if not channel:
+            close_channel = True
+            channel = salt.transport.client.ReqChannel.factory(self.opts, crypt='clear')
+
+        sign_in_payload = self.minion_sign_in_payload()
+        try:
+            payload = channel.send(
+                sign_in_payload,
+                tries=tries,
+                timeout=timeout
+            )
+        except SaltReqTimeoutError as e:
+            if safe:
+                log.warning('SaltReqTimeoutError: %s', e)
+                return 'retry'
+            raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')
+        finally:
+            if close_channel:
+                channel.close()
+
+        if 'load' in payload:
+            if 'ret' in payload['load']:
+                if not payload['load']['ret']:
+                    if self.opts['rejected_retry']:
+                        log.error(
+                            'The Salt Master has rejected this minion\'s public '
+                            'key.\nTo repair this issue, delete the public key '
+                            'for this minion on the Salt Master.\nThe Salt '
+                            'Minion will attempt to to re-authenicate.'
+                        )
+                        return 'retry'
+                    else:
+                        log.critical(
+                            'The Salt Master has rejected this minion\'s public '
+                            'key!\nTo repair this issue, delete the public key '
+                            'for this minion on the Salt Master and restart this '
+                            'minion.\nOr restart the Salt Master in open mode to '
+                            'clean out the keys. The Salt Minion will now exit.'
+                        )
+                        sys.exit(salt.defaults.exitcodes.EX_NOPERM)
+                # has the master returned that its maxed out with minions?
+                elif payload['load']['ret'] == 'full':
+                    return 'full'
+                else:
+                    log.error(
+                        'The Salt Master has cached the public key for this '
+                        'node. If this is the first time connecting to this '
+                        'master then this key may need to be accepted using '
+                        '\'salt-key -a %s\' on the salt master. This salt '
+                        'minion will wait for %s seconds before attempting '
+                        'to re-authenticate.',
+                        self.opts['id'], self.opts['acceptance_wait_time']
+                    )
+                    return 'retry'
+        auth['aes'] = self.verify_master(payload, master_pub='token' in sign_in_payload)
+        if not auth['aes']:
+            log.critical(
+                'The Salt Master server\'s public key did not authenticate!\n'
+                'The master may need to be updated if it is a version of Salt '
+                'lower than %s, or\n'
+                'If you are confident that you are connecting to a valid Salt '
+                'Master, then remove the master public key and restart the '
+                'Salt Minion.\nThe master public key can be found '
+                'at:\n%s', salt.version.__version__, m_pub_fn
+            )
+            sys.exit(42)
+        if self.opts.get('syndic_master', False):  # Is syndic
+            syndic_finger = self.opts.get('syndic_finger', self.opts.get('master_finger', False))
+            if syndic_finger:
+                if salt.utils.crypt.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != syndic_finger:
+                    self._finger_fail(syndic_finger, m_pub_fn)
+        else:
+            if self.opts.get('master_finger', False):
+                if salt.utils.crypt.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != self.opts['master_finger']:
+                    self._finger_fail(self.opts['master_finger'], m_pub_fn)
+        auth['publish_port'] = payload['publish_port']
+        return auth
+
+
+class Crypticle(object):
+    '''
+    Authenticated encryption class
+
+    Encryption algorithm: AES-CBC
+    Signing algorithm: HMAC-SHA256
+    '''
+
+    PICKLE_PAD = b'pickle::'
+    AES_BLOCK_SIZE = 16
+    SIG_SIZE = hashlib.sha256().digest_size
+
+    def __init__(self, opts, key_string, key_size=192):
+        self.key_string = key_string
+        self.keys = self.extract_keys(self.key_string, key_size)
+        self.key_size = key_size
+        self.serial = salt.payload.Serial(opts)
+
+    @classmethod
+    def generate_key_string(cls, key_size=192):
+        key = os.urandom(key_size // 8 + cls.SIG_SIZE)
+        b64key = base64.b64encode(key)
+        if six.PY3:
+            b64key = b64key.decode('utf-8')
+        # Return data must be a base64-encoded string, not a unicode type
+        return b64key.replace('\n', '')
+
+    @classmethod
+    def extract_keys(cls, key_string, key_size):
+        if six.PY2:
+            key = key_string.decode('base64')
+        else:
+            key = salt.utils.stringutils.to_bytes(base64.b64decode(key_string))
+        assert len(key) == key_size / 8 + cls.SIG_SIZE, 'invalid key'
+        return key[:-cls.SIG_SIZE], key[-cls.SIG_SIZE:]
+
+    def encrypt(self, data):
+        '''
+        encrypt data with AES-CBC and sign it with HMAC-SHA256
+        '''
+        aes_key, hmac_key = self.keys
+        pad = self.AES_BLOCK_SIZE - len(data) % self.AES_BLOCK_SIZE
+        if six.PY2:
+            data = data + pad * chr(pad)
+        else:
+            data = data + salt.utils.stringutils.to_bytes(pad * chr(pad))
+        iv_bytes = os.urandom(self.AES_BLOCK_SIZE)
+        if HAS_M2:
+            cypher = EVP.Cipher(alg='aes_192_cbc', key=aes_key, iv=iv_bytes, op=1, padding=False)
+            encr = cypher.update(data)
+            encr += cypher.final()
+        else:
+            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)
+            encr = cypher.encrypt(data)
+        data = iv_bytes + encr
+        sig = hmac.new(hmac_key, data, hashlib.sha256).digest()
+        return data + sig
+
+    def decrypt(self, data):
+        '''
+        verify HMAC-SHA256 signature and decrypt data with AES-CBC
+        '''
+        aes_key, hmac_key = self.keys
+        sig = data[-self.SIG_SIZE:]
+        data = data[:-self.SIG_SIZE]
+        if six.PY3 and not isinstance(data, bytes):
+            data = salt.utils.stringutils.to_bytes(data)
+        mac_bytes = hmac.new(hmac_key, data, hashlib.sha256).digest()
+        if len(mac_bytes) != len(sig):
+            log.debug('Failed to authenticate message')
+            raise AuthenticationError('message authentication failed')
+        result = 0
+
+        if six.PY2:
+            for zipped_x, zipped_y in zip(mac_bytes, sig):
+                result |= ord(zipped_x) ^ ord(zipped_y)
+        else:
+            for zipped_x, zipped_y in zip(mac_bytes, sig):
+                result |= zipped_x ^ zipped_y
+        if result != 0:
+            log.debug('Failed to authenticate message')
+            raise AuthenticationError('message authentication failed')
+        iv_bytes = data[:self.AES_BLOCK_SIZE]
+        data = data[self.AES_BLOCK_SIZE:]
+        if HAS_M2:
+            cypher = EVP.Cipher(alg='aes_192_cbc', key=aes_key, iv=iv_bytes, op=0, padding=False)
+            encr = cypher.update(data)
+            data = encr + cypher.final()
+        else:
+            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)
+            data = cypher.decrypt(data)
+        if six.PY2:
+            return data[:-ord(data[-1])]
+        else:
+            return data[:-data[-1]]
+
+    def dumps(self, obj):
+        '''
+        Serialize and encrypt a python object
+        '''
+        return self.encrypt(self.PICKLE_PAD + self.serial.dumps(obj))
+
+    def loads(self, data, raw=False):
+        '''
+        Decrypt and un-serialize a python object
+        '''
+        data = self.decrypt(data)
+        # simple integrity check to verify that we got meaningful data
+        if not data.startswith(self.PICKLE_PAD):
+            return {}
+        load = self.serial.loads(data[len(self.PICKLE_PAD):], raw=raw)
+        return load
diff -Naur a/salt/engines/ircbot.py c/salt/engines/ircbot.py
--- a/salt/engines/ircbot.py	2019-07-02 10:15:07.027874717 -0600
+++ c/salt/engines/ircbot.py	2019-07-02 10:58:03.171938594 -0600
@@ -64,8 +64,12 @@
 import ssl
 from collections import namedtuple
 
-import tornado.ioloop
-import tornado.iostream
+try:
+    from tornado4.ioloop import IOLoop
+    from tornado4.iostream import IOStream, SSLIOStream
+except ImportError:
+    from tornado.ioloop import IOLoop
+    from tornado.iostream import IOStream, SSLIOStream
 
 import logging
 log = logging.getLogger(__name__)
@@ -96,16 +100,16 @@
         self.allow_hosts = allow_hosts
         self.allow_nicks = allow_nicks
         self.disable_query = disable_query
-        self.io_loop = tornado.ioloop.IOLoop(make_current=False)
+        self.io_loop = IOLoop(make_current=False)
         self.io_loop.make_current()
         self._connect()
 
     def _connect(self):
         _sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
         if self.ssl is True:
-            self._stream = tornado.iostream.SSLIOStream(_sock, ssl_options={'cert_reqs': ssl.CERT_NONE})
+            self._stream = SSLIOStream(_sock, ssl_options={'cert_reqs': ssl.CERT_NONE})
         else:
-            self._stream = tornado.iostream.IOStream(_sock)
+            self._stream = IOStream(_sock)
         self._stream.set_close_callback(self.on_closed)
         self._stream.connect((self.host, self.port), self.on_connect)
 
@@ -184,9 +188,9 @@
         event = self._event(raw)
 
         if event.code == "PING":
-            tornado.ioloop.IOLoop.current().spawn_callback(self.send_message, "PONG {0}".format(event.line))
+            IOLoop.current().spawn_callback(self.send_message, "PONG {0}".format(event.line))
         elif event.code == 'PRIVMSG':
-            tornado.ioloop.IOLoop.current().spawn_callback(self._privmsg, event)
+            IOLoop.current().spawn_callback(self._privmsg, event)
         self.read_messages()
 
     def join_channel(self, channel):
diff -Naur a/salt/engines/webhook.py c/salt/engines/webhook.py
--- a/salt/engines/webhook.py	2019-07-02 10:15:07.027874717 -0600
+++ c/salt/engines/webhook.py	2019-07-02 10:58:03.171938594 -0600
@@ -5,9 +5,14 @@
 from __future__ import absolute_import, print_function, unicode_literals
 
 # import tornado library
-import tornado.httpserver
-import tornado.ioloop
-import tornado.web
+try:
+    from tornado4.httpserver import HTTPServer
+    from tornado4.ioloop import IOLoop
+    from tornado4.web import Application, RequestHandler
+except ImportError:
+    from tornado.httpserver import HTTPServer
+    from tornado.ioloop import IOLoop
+    from tornado.web import Application, RequestHandler
 
 # import salt libs
 import salt.utils.event
@@ -66,7 +71,7 @@
         else:
             __salt__['event.send'](tag, msg)
 
-    class WebHook(tornado.web.RequestHandler):  # pylint: disable=abstract-method
+    class WebHook(RequestHandler):  # pylint: disable=abstract-method
         def post(self, tag):  # pylint: disable=arguments-differ
             body = self.request.body
             headers = self.request.headers
@@ -76,12 +81,12 @@
             }
             fire('salt/engines/hook/' + tag, payload)
 
-    application = tornado.web.Application([(r"/(.*)", WebHook), ])
+    application = Application([(r"/(.*)", WebHook), ])
     ssl_options = None
     if all([ssl_crt, ssl_key]):
         ssl_options = {"certfile": ssl_crt, "keyfile": ssl_key}
-    io_loop = tornado.ioloop.IOLoop(make_current=False)
+    io_loop = IOLoop(make_current=False)
     io_loop.make_current()
-    http_server = tornado.httpserver.HTTPServer(application, ssl_options=ssl_options)
+    http_server = HTTPServer(application, ssl_options=ssl_options)
     http_server.listen(port, address=address)
     io_loop.start()
diff -Naur a/salt/fileclient.py c/salt/fileclient.py
--- a/salt/fileclient.py	2019-07-02 10:15:07.027874717 -0600
+++ c/salt/fileclient.py	2019-07-02 10:58:03.171938594 -0600
@@ -12,7 +12,10 @@
 import string
 import shutil
 import ftplib
-from tornado.httputil import parse_response_start_line, HTTPHeaders, HTTPInputError
+try:
+    from tornado4.httputil import parse_response_start_line, HTTPHeaders, HTTPInputError
+except ImportError:
+    from tornado.httputil import parse_response_start_line, HTTPHeaders, HTTPInputError
 import salt.utils.atomicfile
 
 # Import salt libs
diff -Naur a/salt/master.py c/salt/master.py
--- a/salt/master.py	2019-07-02 10:15:07.023874717 -0600
+++ c/salt/master.py	2019-07-02 10:58:03.171938594 -0600
@@ -28,7 +28,10 @@
 from salt.utils.zeromq import zmq, ZMQDefaultLoop, install_zmq, ZMQ_VERSION_INFO
 # pylint: enable=import-error,no-name-in-module,redefined-builtin
 
-import tornado.gen  # pylint: disable=F0401
+try:
+    import tornado4.gen as tornado_gen  # pylint: disable=F0401
+except ImportError:
+    import tornado.gen as tornado_gen  # pylint: disable=F0401
 
 # Import salt libs
 import salt.crypt
@@ -91,7 +94,11 @@
 except ImportError:
     HAS_HALITE = False
 
-from tornado.stack_context import StackContext
+try:
+    from tornado4.stack_context import StackContext
+except ImportError:
+    from tornado.stack_context import StackContext
+
 from salt.utils.ctx import RequestContext
 
 
@@ -1036,7 +1043,7 @@
             # Tornado knows what to do
             pass
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(self, payload):
         '''
         The _handle_payload method is the key method used to figure out what
@@ -1062,7 +1069,7 @@
         load = payload['load']
         ret = {'aes': self._handle_aes,
                'clear': self._handle_clear}[key](load)
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
     def _post_stats(self, start, cmd):
         '''
diff -Naur a/salt/minion.py c/salt/minion.py
--- a/salt/minion.py	2019-07-02 10:15:07.043874718 -0600
+++ c/salt/minion.py	2019-07-02 11:19:45.431970885 -0600
@@ -35,7 +35,10 @@
 from salt.utils.ctx import RequestContext
 
 # pylint: enable=no-name-in-module,redefined-builtin
-import tornado
+try:
+    import tornado4 as tornado
+except ImportError:
+    import tornado
 
 HAS_PSUTIL = False
 try:
@@ -115,9 +118,14 @@
     SaltMasterUnresolvableError
 )
 
-
-import tornado.gen  # pylint: disable=F0401
-import tornado.ioloop  # pylint: disable=F0401
+try:
+    import tornado4.gen as tornado_gen  # pylint: disable=F0401
+    from tornado4.ioloop import IOLoop, PeriodicCallback  # pylint: disable=F0401
+    from tornado4.stack_context import ExceptionStackContext, StackContext
+except ImportError:
+    import tornado.gen as tornado_gen  # pylint: disable=F0401
+    from tornado.ioloop import IOLoop, PeriodicCallback  # pylint: disable=F0401
+    from tornado.stack_context import ExceptionStackContext, StackContext
 
 log = logging.getLogger(__name__)
 
@@ -453,7 +461,7 @@
                 return self.beacons.process(b_conf, self.opts['grains'])  # pylint: disable=no-member
         return []
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def eval_master(self,
                     opts,
                     timeout=60,
@@ -479,7 +487,7 @@
         if opts['master_type'] == 'disable':
             log.warning('Master is set to disable, skipping connection')
             self.connected = False
-            raise tornado.gen.Return((None, None))
+            raise tornado_gen.Return((None, None))
 
         # Run masters discovery over SSDP. This may modify the whole configuration,
         # depending of the networking and sets of masters.
@@ -614,7 +622,7 @@
                 if attempts != 0:
                     # Give up a little time between connection attempts
                     # to allow the IOLoop to run any other scheduled tasks.
-                    yield tornado.gen.sleep(opts['acceptance_wait_time'])
+                    yield tornado_gen.sleep(opts['acceptance_wait_time'])
                 attempts += 1
                 if tries > 0:
                     log.debug(
@@ -672,7 +680,7 @@
                 else:
                     self.tok = pub_channel.auth.gen_token(b'salt')
                     self.connected = True
-                    raise tornado.gen.Return((opts['master'], pub_channel))
+                    raise tornado_gen.Return((opts['master'], pub_channel))
 
         # single master sign in
         else:
@@ -682,7 +690,7 @@
                 if attempts != 0:
                     # Give up a little time between connection attempts
                     # to allow the IOLoop to run any other scheduled tasks.
-                    yield tornado.gen.sleep(opts['acceptance_wait_time'])
+                    yield tornado_gen.sleep(opts['acceptance_wait_time'])
                 attempts += 1
                 if tries > 0:
                     log.debug(
@@ -714,7 +722,7 @@
                         yield pub_channel.connect()
                     self.tok = pub_channel.auth.gen_token(b'salt')
                     self.connected = True
-                    raise tornado.gen.Return((opts['master'], pub_channel))
+                    raise tornado_gen.Return((opts['master'], pub_channel))
                 except SaltClientError as exc:
                     if attempts == tries:
                         # Exhausted all attempts. Return exception.
@@ -957,7 +965,7 @@
         self.event.subscribe('')
         self.event.set_event_handler(self.handle_event)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def handle_event(self, package):
         yield [minion.handle_event(package) for minion in self.minions]
 
@@ -1004,7 +1012,7 @@
             self.io_loop.spawn_callback(self._connect_minion, minion)
         self.io_loop.call_later(timeout, self._check_minions)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _connect_minion(self, minion):
         '''
         Create a minion, and asynchronously connect it to a master
@@ -1031,7 +1039,7 @@
                 last = time.time()
                 if auth_wait < self.max_auth_wait:
                     auth_wait += self.auth_wait
-                yield tornado.gen.sleep(auth_wait)  # TODO: log?
+                yield tornado_gen.sleep(auth_wait)  # TODO: log?
             except SaltMasterUnresolvableError:
                 err = 'Master address: \'{0}\' could not be resolved. Invalid or unresolveable address. ' \
                       'Set \'master\' value in minion config.'.format(minion.opts['master'])
@@ -1210,7 +1218,7 @@
         if timeout and self._sync_connect_master_success is False:
             raise SaltDaemonNotRunning('Failed to connect to the salt-master')
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def connect_master(self, failed=False):
         '''
         Return a future which will complete when you are connected to a master
@@ -1219,7 +1227,7 @@
         yield self._post_master_init(master)
 
     # TODO: better name...
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _post_master_init(self, master):
         '''
         Function to finish init after connecting to a master
@@ -1398,7 +1406,7 @@
         finally:
             channel.close()
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _send_req_async(self, load, timeout):
 
         if self.opts['minion_sign_messages']:
@@ -1410,7 +1418,7 @@
         channel = salt.transport.client.AsyncReqChannel.factory(self.opts)
         try:
             ret = yield channel.send(load, timeout=timeout)
-            raise tornado.gen.Return(ret)
+            raise tornado_gen.Return(ret)
         finally:
             channel.close()
 
@@ -1449,11 +1457,11 @@
                     return True
                 timeout_handler = handle_timeout
 
-            with tornado.stack_context.ExceptionStackContext(timeout_handler):
+            with ExceptionStackContext(timeout_handler):
                 self._send_req_async(load, timeout, callback=lambda f: None)  # pylint: disable=unexpected-keyword-arg
         return True
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_decoded_payload(self, data):
         '''
         Override this method if you wish to handle the decoded data
@@ -1495,7 +1503,7 @@
             process_count = len(salt.utils.minion.running(self.opts))
             while process_count >= process_count_max:
                 log.warning("Maximum number of processes reached while executing jid %s, waiting...", data['jid'])
-                yield tornado.gen.sleep(10)
+                yield tornado_gen.sleep(10)
                 process_count = len(salt.utils.minion.running(self.opts))
 
         # We stash an instance references to allow for the socket
@@ -1579,9 +1587,9 @@
             else:
                 return Minion._thread_return(minion_instance, opts, data)
 
-        with tornado.stack_context.StackContext(functools.partial(RequestContext,
+        with StackContext(functools.partial(RequestContext,
                                                                   {'data': data, 'opts': opts})):
-            with tornado.stack_context.StackContext(minion_instance.ctx):
+            with StackContext(minion_instance.ctx):
                 run_func(minion_instance, opts, data)
 
     @classmethod
@@ -2006,7 +2014,7 @@
                 timeout_handler()
                 return ''
         else:
-            with tornado.stack_context.ExceptionStackContext(timeout_handler):
+            with ExceptionStackContext(timeout_handler):
                 ret_val = self._send_req_async(load, timeout=timeout, callback=lambda f: None)  # pylint: disable=unexpected-keyword-arg
 
         log.trace('ret_val = %s', ret_val)  # pylint: disable=no-member
@@ -2092,7 +2100,7 @@
                 timeout_handler()
                 return ''
         else:
-            with tornado.stack_context.ExceptionStackContext(timeout_handler):
+            with ExceptionStackContext(timeout_handler):
                 ret_val = self._send_req_async(load, timeout=timeout, callback=lambda f: None)  # pylint: disable=unexpected-keyword-arg
 
         log.trace('ret_val = %s', ret_val)  # pylint: disable=no-member
@@ -2186,7 +2194,7 @@
         self.matchers = salt.loader.matchers(self.opts)
 
     # TODO: only allow one future in flight at a time?
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def pillar_refresh(self, force_refresh=False):
         '''
         Refresh the pillar
@@ -2347,13 +2355,13 @@
         finally:
             channel.close()
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def handle_event(self, package):
         '''
         Handle an event from the epull_sock (all local minion events)
         '''
         if not self.ready:
-            raise tornado.gen.Return()
+            raise tornado_gen.Return()
         tag, data = salt.utils.event.SaltEvent.unpack(package)
         log.debug(
             'Minion of \'%s\' is handling event tag \'%s\'',
@@ -2600,14 +2608,14 @@
                 if beacons and self.connected:
                     self._fire_master(events=beacons)
 
-            new_periodic_callbacks['beacons'] = tornado.ioloop.PeriodicCallback(
+            new_periodic_callbacks['beacons'] = PeriodicCallback(
                     handle_beacons, loop_interval * 1000)
             if before_connect:
                 # Make sure there is a chance for one iteration to occur before connect
                 handle_beacons()
 
         if 'cleanup' not in self.periodic_callbacks:
-            new_periodic_callbacks['cleanup'] = tornado.ioloop.PeriodicCallback(
+            new_periodic_callbacks['cleanup'] = PeriodicCallback(
                     self._fallback_cleanups, loop_interval * 1000)
 
         # start all the other callbacks
@@ -2653,14 +2661,14 @@
             # TODO: actually listen to the return and change period
             def handle_schedule():
                 self.process_schedule(self, loop_interval)
-            new_periodic_callbacks['schedule'] = tornado.ioloop.PeriodicCallback(handle_schedule, 1000)
+            new_periodic_callbacks['schedule'] = PeriodicCallback(handle_schedule, 1000)
 
             if before_connect:
                 # Make sure there is a chance for one iteration to occur before connect
                 handle_schedule()
 
         if 'cleanup' not in self.periodic_callbacks:
-            new_periodic_callbacks['cleanup'] = tornado.ioloop.PeriodicCallback(
+            new_periodic_callbacks['cleanup'] = PeriodicCallback(
                     self._fallback_cleanups, loop_interval * 1000)
 
         # start all the other callbacks
@@ -2727,7 +2735,7 @@
                     self._fire_master('ping', 'minion_ping', sync=False, timeout_handler=ping_timeout_handler)
                 except Exception:
                     log.warning('Attempt to ping master failed.', exc_on_loglevel=logging.DEBUG)
-            self.periodic_callbacks['ping'] = tornado.ioloop.PeriodicCallback(ping_master, ping_interval * 1000)
+            self.periodic_callbacks['ping'] = PeriodicCallback(ping_master, ping_interval * 1000)
             self.periodic_callbacks['ping'].start()
 
         # add handler to subscriber
@@ -2859,7 +2867,7 @@
             log.warning('Unable to forward pub data: %s', args[1])
             return True
 
-        with tornado.stack_context.ExceptionStackContext(timeout_handler):
+        with ExceptionStackContext(timeout_handler):
             self.local.pub_async(data['tgt'],
                                  data['fun'],
                                  data['arg'],
@@ -2914,7 +2922,7 @@
         # In the future, we could add support for some clearfuncs, but
         # the syndic currently has no need.
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def reconnect(self):
         if hasattr(self, 'pub_channel'):
             self.pub_channel.on_recv(None)
@@ -2931,7 +2939,7 @@
             self.pub_channel.on_recv(self._process_cmd_socket)
             log.info('Minion is ready to receive requests!')
 
-        raise tornado.gen.Return(self)
+        raise tornado_gen.Return(self)
 
     def destroy(self):
         '''
@@ -3014,7 +3022,7 @@
             s_opts['master'] = master
             self._syndics[master] = self._connect_syndic(s_opts)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _connect_syndic(self, opts):
         '''
         Create a syndic, and asynchronously connect it to a master
@@ -3054,7 +3062,7 @@
                 last = time.time()
                 if auth_wait < self.max_auth_wait:
                     auth_wait += self.auth_wait
-                yield tornado.gen.sleep(auth_wait)  # TODO: log?
+                yield tornado_gen.sleep(auth_wait)  # TODO: log?
             except (KeyboardInterrupt, SystemExit):
                 raise
             except Exception:
@@ -3064,7 +3072,7 @@
                     opts['master'], exc_info=True
                 )
 
-        raise tornado.gen.Return(syndic)
+        raise tornado_gen.Return(syndic)
 
     def _mark_master_dead(self, master):
         '''
@@ -3198,7 +3206,7 @@
         self.io_loop.add_future(future, self.reconnect_event_bus)
 
         # forward events every syndic_event_forward_timeout
-        self.forward_events = tornado.ioloop.PeriodicCallback(self._forward_events,
+        self.forward_events = PeriodicCallback(self._forward_events,
                                                               self.opts['syndic_event_forward_timeout'] * 1000,
                                                               )
         self.forward_events.start()
@@ -3308,7 +3316,7 @@
     '''
 
     # TODO: better name...
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _post_master_init(self, master):
         '''
         Function to finish init after connecting to a master
@@ -3577,7 +3585,7 @@
                     get_proc_dir(opts['cachedir'], uid=uid)
                     )
 
-        with tornado.stack_context.StackContext(minion_instance.ctx):
+        with StackContext(minion_instance.ctx):
             if isinstance(data['fun'], tuple) or isinstance(data['fun'], list):
                 Minion._thread_multi_return(minion_instance, opts, data)
             else:
diff -Naur a/salt/minion.py.orig c/salt/minion.py.orig
--- a/salt/minion.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/minion.py.orig	2019-07-02 10:57:25.407937658 -0600
@@ -0,0 +1,3673 @@
+# -*- coding: utf-8 -*-
+'''
+Routines to set up a minion
+'''
+# Import python libs
+from __future__ import absolute_import, print_function, with_statement, unicode_literals
+import functools
+import os
+import sys
+import copy
+import time
+import types
+import signal
+import random
+import logging
+import threading
+import traceback
+import contextlib
+import multiprocessing
+from random import randint, shuffle
+from stat import S_IMODE
+import salt.serializers.msgpack
+from binascii import crc32
+
+# Import Salt Libs
+# pylint: disable=import-error,no-name-in-module,redefined-builtin
+from salt.ext import six
+from salt._compat import ipaddress
+from salt.utils.network import parse_host_port
+from salt.ext.six.moves import range
+from salt.utils.zeromq import zmq, ZMQDefaultLoop, install_zmq, ZMQ_VERSION_INFO
+import salt.transport.client
+import salt.defaults.exitcodes
+
+from salt.utils.ctx import RequestContext
+
+# pylint: enable=no-name-in-module,redefined-builtin
+import tornado
+
+HAS_PSUTIL = False
+try:
+    import salt.utils.psutil_compat as psutil
+    HAS_PSUTIL = True
+except ImportError:
+    pass
+
+HAS_RESOURCE = False
+try:
+    import resource
+    HAS_RESOURCE = True
+except ImportError:
+    pass
+
+try:
+    import zmq.utils.monitor
+    HAS_ZMQ_MONITOR = True
+except ImportError:
+    HAS_ZMQ_MONITOR = False
+
+try:
+    import salt.utils.win_functions
+    HAS_WIN_FUNCTIONS = True
+except ImportError:
+    HAS_WIN_FUNCTIONS = False
+# pylint: enable=import-error
+
+# Import salt libs
+import salt
+import salt.client
+import salt.crypt
+import salt.loader
+import salt.beacons
+import salt.engines
+import salt.payload
+import salt.pillar
+import salt.syspaths
+import salt.utils.args
+import salt.utils.context
+import salt.utils.data
+import salt.utils.error
+import salt.utils.event
+import salt.utils.files
+import salt.utils.jid
+import salt.utils.minion
+import salt.utils.minions
+import salt.utils.network
+import salt.utils.platform
+import salt.utils.process
+import salt.utils.schedule
+import salt.utils.ssdp
+import salt.utils.user
+import salt.utils.zeromq
+import salt.defaults.exitcodes
+import salt.cli.daemons
+import salt.log.setup
+
+import salt.utils.dictupdate
+from salt.config import DEFAULT_MINION_OPTS
+from salt.defaults import DEFAULT_TARGET_DELIM
+from salt.utils.debug import enable_sigusr1_handler
+from salt.utils.event import tagify
+from salt.utils.odict import OrderedDict
+from salt.utils.process import (default_signals,
+                                SignalHandlingMultiprocessingProcess,
+                                ProcessManager)
+from salt.exceptions import (
+    CommandExecutionError,
+    CommandNotFoundError,
+    SaltInvocationError,
+    SaltReqTimeoutError,
+    SaltClientError,
+    SaltSystemExit,
+    SaltDaemonNotRunning,
+    SaltException,
+    SaltMasterUnresolvableError
+)
+
+
+import tornado.gen  # pylint: disable=F0401
+import tornado.ioloop  # pylint: disable=F0401
+
+log = logging.getLogger(__name__)
+
+# To set up a minion:
+# 1. Read in the configuration
+# 2. Generate the function mapping dict
+# 3. Authenticate with the master
+# 4. Store the AES key
+# 5. Connect to the publisher
+# 6. Handle publications
+
+
+def resolve_dns(opts, fallback=True):
+    '''
+    Resolves the master_ip and master_uri options
+    '''
+    ret = {}
+    check_dns = True
+    if (opts.get('file_client', 'remote') == 'local' and
+            not opts.get('use_master_when_local', False)):
+        check_dns = False
+    # Since salt.log is imported below, salt.utils.network needs to be imported here as well
+    import salt.utils.network
+
+    if check_dns is True:
+        try:
+            if opts['master'] == '':
+                raise SaltSystemExit
+            ret['master_ip'] = salt.utils.network.dns_check(
+                opts['master'],
+                int(opts['master_port']),
+                True,
+                opts['ipv6'])
+        except SaltClientError:
+            retry_dns_count = opts.get('retry_dns_count', None)
+            if opts['retry_dns']:
+                while True:
+                    if retry_dns_count is not None:
+                        if retry_dns_count == 0:
+                            raise SaltMasterUnresolvableError
+                        retry_dns_count -= 1
+                    import salt.log
+                    msg = ('Master hostname: \'{0}\' not found or not responsive. '
+                           'Retrying in {1} seconds').format(opts['master'], opts['retry_dns'])
+                    if salt.log.setup.is_console_configured():
+                        log.error(msg)
+                    else:
+                        print('WARNING: {0}'.format(msg))
+                    time.sleep(opts['retry_dns'])
+                    try:
+                        ret['master_ip'] = salt.utils.network.dns_check(
+                            opts['master'],
+                            int(opts['master_port']),
+                            True,
+                            opts['ipv6'])
+                        break
+                    except SaltClientError:
+                        pass
+            else:
+                if fallback:
+                    ret['master_ip'] = '127.0.0.1'
+                else:
+                    raise
+        except SaltSystemExit:
+            unknown_str = 'unknown address'
+            master = opts.get('master', unknown_str)
+            if master == '':
+                master = unknown_str
+            if opts.get('__role') == 'syndic':
+                err = 'Master address: \'{0}\' could not be resolved. Invalid or unresolveable address. ' \
+                      'Set \'syndic_master\' value in minion config.'.format(master)
+            else:
+                err = 'Master address: \'{0}\' could not be resolved. Invalid or unresolveable address. ' \
+                      'Set \'master\' value in minion config.'.format(master)
+            log.error(err)
+            raise SaltSystemExit(code=42, msg=err)
+    else:
+        ret['master_ip'] = '127.0.0.1'
+
+    if 'master_ip' in ret and 'master_ip' in opts:
+        if ret['master_ip'] != opts['master_ip']:
+            log.warning(
+                'Master ip address changed from %s to %s',
+                opts['master_ip'], ret['master_ip']
+            )
+    if opts['source_interface_name']:
+        log.trace('Custom source interface required: %s', opts['source_interface_name'])
+        interfaces = salt.utils.network.interfaces()
+        log.trace('The following interfaces are available on this Minion:')
+        log.trace(interfaces)
+        if opts['source_interface_name'] in interfaces:
+            if interfaces[opts['source_interface_name']]['up']:
+                addrs = interfaces[opts['source_interface_name']]['inet'] if not opts['ipv6'] else\
+                        interfaces[opts['source_interface_name']]['inet6']
+                ret['source_ip'] = addrs[0]['address']
+                log.debug('Using %s as source IP address', ret['source_ip'])
+            else:
+                log.warning('The interface %s is down so it cannot be used as source to connect to the Master',
+                            opts['source_interface_name'])
+        else:
+            log.warning('%s is not a valid interface. Ignoring.', opts['source_interface_name'])
+    elif opts['source_address']:
+        ret['source_ip'] = salt.utils.network.dns_check(
+            opts['source_address'],
+            int(opts['source_ret_port']),
+            True,
+            opts['ipv6'])
+        log.debug('Using %s as source IP address', ret['source_ip'])
+    if opts['source_ret_port']:
+        ret['source_ret_port'] = int(opts['source_ret_port'])
+        log.debug('Using %d as source port for the ret server', ret['source_ret_port'])
+    if opts['source_publish_port']:
+        ret['source_publish_port'] = int(opts['source_publish_port'])
+        log.debug('Using %d as source port for the master pub', ret['source_publish_port'])
+    ret['master_uri'] = 'tcp://{ip}:{port}'.format(
+        ip=ret['master_ip'], port=opts['master_port'])
+    log.debug('Master URI: %s', ret['master_uri'])
+
+    return ret
+
+
+def prep_ip_port(opts):
+    '''
+    parse host:port values from opts['master'] and return valid:
+        master: ip address or hostname as a string
+        master_port: (optional) master returner port as integer
+
+    e.g.:
+      - master: 'localhost:1234' -> {'master': 'localhost', 'master_port': 1234}
+      - master: '127.0.0.1:1234' -> {'master': '127.0.0.1', 'master_port' :1234}
+      - master: '[::1]:1234' -> {'master': '::1', 'master_port': 1234}
+      - master: 'fe80::a00:27ff:fedc:ba98' -> {'master': 'fe80::a00:27ff:fedc:ba98'}
+    '''
+    ret = {}
+    # Use given master IP if "ip_only" is set or if master_ip is an ipv6 address without
+    # a port specified. The is_ipv6 check returns False if brackets are used in the IP
+    # definition such as master: '[::1]:1234'.
+    if opts['master_uri_format'] == 'ip_only':
+        ret['master'] = ipaddress.ip_address(opts['master'])
+    else:
+        host, port = parse_host_port(opts['master'])
+        ret = {'master': host}
+        if port:
+            ret.update({'master_port': port})
+
+    return ret
+
+
+def get_proc_dir(cachedir, **kwargs):
+    '''
+    Given the cache directory, return the directory that process data is
+    stored in, creating it if it doesn't exist.
+    The following optional Keyword Arguments are handled:
+
+    mode: which is anything os.makedir would accept as mode.
+
+    uid: the uid to set, if not set, or it is None or -1 no changes are
+         made. Same applies if the directory is already owned by this
+         uid. Must be int. Works only on unix/unix like systems.
+
+    gid: the gid to set, if not set, or it is None or -1 no changes are
+         made. Same applies if the directory is already owned by this
+         gid. Must be int. Works only on unix/unix like systems.
+    '''
+    fn_ = os.path.join(cachedir, 'proc')
+    mode = kwargs.pop('mode', None)
+
+    if mode is None:
+        mode = {}
+    else:
+        mode = {'mode': mode}
+
+    if not os.path.isdir(fn_):
+        # proc_dir is not present, create it with mode settings
+        os.makedirs(fn_, **mode)
+
+    d_stat = os.stat(fn_)
+
+    # if mode is not an empty dict then we have an explicit
+    # dir mode. So lets check if mode needs to be changed.
+    if mode:
+        mode_part = S_IMODE(d_stat.st_mode)
+        if mode_part != mode['mode']:
+            os.chmod(fn_, (d_stat.st_mode ^ mode_part) | mode['mode'])
+
+    if hasattr(os, 'chown'):
+        # only on unix/unix like systems
+        uid = kwargs.pop('uid', -1)
+        gid = kwargs.pop('gid', -1)
+
+        # if uid and gid are both -1 then go ahead with
+        # no changes at all
+        if (d_stat.st_uid != uid or d_stat.st_gid != gid) and \
+                [i for i in (uid, gid) if i != -1]:
+            os.chown(fn_, uid, gid)
+
+    return fn_
+
+
+def load_args_and_kwargs(func, args, data=None, ignore_invalid=False):
+    '''
+    Detect the args and kwargs that need to be passed to a function call, and
+    check them against what was passed.
+    '''
+    argspec = salt.utils.args.get_function_argspec(func)
+    _args = []
+    _kwargs = {}
+    invalid_kwargs = []
+
+    for arg in args:
+        if isinstance(arg, dict) and arg.pop('__kwarg__', False) is True:
+            # if the arg is a dict with __kwarg__ == True, then its a kwarg
+            for key, val in six.iteritems(arg):
+                if argspec.keywords or key in argspec.args:
+                    # Function supports **kwargs or is a positional argument to
+                    # the function.
+                    _kwargs[key] = val
+                else:
+                    # **kwargs not in argspec and parsed argument name not in
+                    # list of positional arguments. This keyword argument is
+                    # invalid.
+                    invalid_kwargs.append('{0}={1}'.format(key, val))
+            continue
+
+        else:
+            string_kwarg = salt.utils.args.parse_input([arg], condition=False)[1]  # pylint: disable=W0632
+            if string_kwarg:
+                if argspec.keywords or next(six.iterkeys(string_kwarg)) in argspec.args:
+                    # Function supports **kwargs or is a positional argument to
+                    # the function.
+                    _kwargs.update(string_kwarg)
+                else:
+                    # **kwargs not in argspec and parsed argument name not in
+                    # list of positional arguments. This keyword argument is
+                    # invalid.
+                    for key, val in six.iteritems(string_kwarg):
+                        invalid_kwargs.append('{0}={1}'.format(key, val))
+            else:
+                _args.append(arg)
+
+    if invalid_kwargs and not ignore_invalid:
+        salt.utils.args.invalid_kwargs(invalid_kwargs)
+
+    if argspec.keywords and isinstance(data, dict):
+        # this function accepts **kwargs, pack in the publish data
+        for key, val in six.iteritems(data):
+            _kwargs['__pub_{0}'.format(key)] = val
+
+    return _args, _kwargs
+
+
+def eval_master_func(opts):
+    '''
+    Evaluate master function if master type is 'func'
+    and save it result in opts['master']
+    '''
+    if '__master_func_evaluated' not in opts:
+        # split module and function and try loading the module
+        mod_fun = opts['master']
+        mod, fun = mod_fun.split('.')
+        try:
+            master_mod = salt.loader.raw_mod(opts, mod, fun)
+            if not master_mod:
+                raise KeyError
+            # we take whatever the module returns as master address
+            opts['master'] = master_mod[mod_fun]()
+            # Check for valid types
+            if not isinstance(opts['master'], (six.string_types, list)):
+                raise TypeError
+            opts['__master_func_evaluated'] = True
+        except KeyError:
+            log.error('Failed to load module %s', mod_fun)
+            sys.exit(salt.defaults.exitcodes.EX_GENERIC)
+        except TypeError:
+            log.error('%s returned from %s is not a string', opts['master'], mod_fun)
+            sys.exit(salt.defaults.exitcodes.EX_GENERIC)
+        log.info('Evaluated master from module: %s', mod_fun)
+
+
+def master_event(type, master=None):
+    '''
+    Centralized master event function which will return event type based on event_map
+    '''
+    event_map = {'connected': '__master_connected',
+                 'disconnected': '__master_disconnected',
+                 'failback': '__master_failback',
+                 'alive': '__master_alive'}
+
+    if type == 'alive' and master is not None:
+        return '{0}_{1}'.format(event_map.get(type), master)
+
+    return event_map.get(type, None)
+
+
+def service_name():
+    '''
+    Return the proper service name based on platform
+    '''
+    return 'salt_minion' if 'bsd' in sys.platform else 'salt-minion'
+
+
+class MinionBase(object):
+    def __init__(self, opts):
+        self.opts = opts
+
+    @staticmethod
+    def process_schedule(minion, loop_interval):
+        try:
+            if hasattr(minion, 'schedule'):
+                minion.schedule.eval()
+            else:
+                log.error('Minion scheduler not initialized. Scheduled jobs will not be run.')
+                return
+            # Check if scheduler requires lower loop interval than
+            # the loop_interval setting
+            if minion.schedule.loop_interval < loop_interval:
+                loop_interval = minion.schedule.loop_interval
+                log.debug(
+                    'Overriding loop_interval because of scheduled jobs.'
+                )
+        except Exception as exc:
+            log.error('Exception %s occurred in scheduled job', exc)
+        return loop_interval
+
+    def process_beacons(self, functions):
+        '''
+        Evaluate all of the configured beacons, grab the config again in case
+        the pillar or grains changed
+        '''
+        if 'config.merge' in functions:
+            b_conf = functions['config.merge']('beacons', self.opts['beacons'], omit_opts=True)
+            if b_conf:
+                return self.beacons.process(b_conf, self.opts['grains'])  # pylint: disable=no-member
+        return []
+
+    @tornado.gen.coroutine
+    def eval_master(self,
+                    opts,
+                    timeout=60,
+                    safe=True,
+                    failed=False,
+                    failback=False):
+        '''
+        Evaluates and returns a tuple of the current master address and the pub_channel.
+
+        In standard mode, just creates a pub_channel with the given master address.
+
+        With master_type=func evaluates the current master address from the given
+        module and then creates a pub_channel.
+
+        With master_type=failover takes the list of masters and loops through them.
+        The first one that allows the minion to create a pub_channel is then
+        returned. If this function is called outside the minions initialization
+        phase (for example from the minions main event-loop when a master connection
+        loss was detected), 'failed' should be set to True. The current
+        (possibly failed) master will then be removed from the list of masters.
+        '''
+        # return early if we are not connecting to a master
+        if opts['master_type'] == 'disable':
+            log.warning('Master is set to disable, skipping connection')
+            self.connected = False
+            raise tornado.gen.Return((None, None))
+
+        # Run masters discovery over SSDP. This may modify the whole configuration,
+        # depending of the networking and sets of masters.
+        self._discover_masters()
+
+        # check if master_type was altered from its default
+        if opts['master_type'] != 'str' and opts['__role'] != 'syndic':
+            # check for a valid keyword
+            if opts['master_type'] == 'func':
+                eval_master_func(opts)
+
+            # if failover or distributed is set, master has to be of type list
+            elif opts['master_type'] in ('failover', 'distributed'):
+                if isinstance(opts['master'], list):
+                    log.info(
+                        'Got list of available master addresses: %s',
+                        opts['master']
+                    )
+
+                    if opts['master_type'] == 'distributed':
+                        master_len = len(opts['master'])
+                        if master_len > 1:
+                            secondary_masters = opts['master'][1:]
+                            master_idx = crc32(opts['id']) % master_len
+                            try:
+                                preferred_masters = opts['master']
+                                preferred_masters[0] = opts['master'][master_idx]
+                                preferred_masters[1:] = [m for m in opts['master'] if m != preferred_masters[0]]
+                                opts['master'] = preferred_masters
+                                log.info('Distributed to the master at \'%s\'.', opts['master'][0])
+                            except (KeyError, AttributeError, TypeError):
+                                log.warning('Failed to distribute to a specific master.')
+                        else:
+                            log.warning('master_type = distributed needs more than 1 master.')
+
+                    if opts['master_shuffle']:
+                        log.warning(
+                            'Use of \'master_shuffle\' detected. \'master_shuffle\' is deprecated in favor '
+                            'of \'random_master\'. Please update your minion config file.'
+                        )
+                        opts['random_master'] = opts['master_shuffle']
+
+                    opts['auth_tries'] = 0
+                    if opts['master_failback'] and opts['master_failback_interval'] == 0:
+                        opts['master_failback_interval'] = opts['master_alive_interval']
+                # if opts['master'] is a str and we have never created opts['master_list']
+                elif isinstance(opts['master'], six.string_types) and ('master_list' not in opts):
+                    # We have a string, but a list was what was intended. Convert.
+                    # See issue 23611 for details
+                    opts['master'] = [opts['master']]
+                elif opts['__role'] == 'syndic':
+                    log.info('Syndic setting master_syndic to \'%s\'', opts['master'])
+
+                # if failed=True, the minion was previously connected
+                # we're probably called from the minions main-event-loop
+                # because a master connection loss was detected. remove
+                # the possibly failed master from the list of masters.
+                elif failed:
+                    if failback:
+                        # failback list of masters to original config
+                        opts['master'] = opts['master_list']
+                    else:
+                        log.info(
+                            'Moving possibly failed master %s to the end of '
+                            'the list of masters', opts['master']
+                        )
+                        if opts['master'] in opts['local_masters']:
+                            # create new list of master with the possibly failed
+                            # one moved to the end
+                            failed_master = opts['master']
+                            opts['master'] = [x for x in opts['local_masters'] if opts['master'] != x]
+                            opts['master'].append(failed_master)
+                        else:
+                            opts['master'] = opts['master_list']
+                else:
+                    msg = ('master_type set to \'failover\' but \'master\' '
+                           'is not of type list but of type '
+                           '{0}'.format(type(opts['master'])))
+                    log.error(msg)
+                    sys.exit(salt.defaults.exitcodes.EX_GENERIC)
+                # If failover is set, minion have to failover on DNS errors instead of retry DNS resolve.
+                # See issue 21082 for details
+                if opts['retry_dns'] and opts['master_type'] == 'failover':
+                    msg = ('\'master_type\' set to \'failover\' but \'retry_dns\' is not 0. '
+                           'Setting \'retry_dns\' to 0 to failover to the next master on DNS errors.')
+                    log.critical(msg)
+                    opts['retry_dns'] = 0
+            else:
+                msg = ('Invalid keyword \'{0}\' for variable '
+                       '\'master_type\''.format(opts['master_type']))
+                log.error(msg)
+                sys.exit(salt.defaults.exitcodes.EX_GENERIC)
+
+        # FIXME: if SMinion don't define io_loop, it can't switch master see #29088
+        # Specify kwargs for the channel factory so that SMinion doesn't need to define an io_loop
+        # (The channel factories will set a default if the kwarg isn't passed)
+        factory_kwargs = {'timeout': timeout, 'safe': safe}
+        if getattr(self, 'io_loop', None):
+            factory_kwargs['io_loop'] = self.io_loop  # pylint: disable=no-member
+
+        tries = opts.get('master_tries', 1)
+        attempts = 0
+
+        # if we have a list of masters, loop through them and be
+        # happy with the first one that allows us to connect
+        if isinstance(opts['master'], list):
+            conn = False
+            last_exc = None
+            opts['master_uri_list'] = []
+            opts['local_masters'] = copy.copy(opts['master'])
+
+            # shuffle the masters and then loop through them
+            if opts['random_master']:
+                # master_failback is only used when master_type is set to failover
+                if opts['master_type'] == 'failover' and opts['master_failback']:
+                    secondary_masters = opts['local_masters'][1:]
+                    shuffle(secondary_masters)
+                    opts['local_masters'][1:] = secondary_masters
+                else:
+                    shuffle(opts['local_masters'])
+
+            # This sits outside of the connection loop below because it needs to set
+            # up a list of master URIs regardless of which masters are available
+            # to connect _to_. This is primarily used for masterless mode, when
+            # we need a list of master URIs to fire calls back to.
+            for master in opts['local_masters']:
+                opts['master'] = master
+                opts.update(prep_ip_port(opts))
+                opts['master_uri_list'].append(resolve_dns(opts)['master_uri'])
+
+            while True:
+                if attempts != 0:
+                    # Give up a little time between connection attempts
+                    # to allow the IOLoop to run any other scheduled tasks.
+                    yield tornado.gen.sleep(opts['acceptance_wait_time'])
+                attempts += 1
+                if tries > 0:
+                    log.debug(
+                        'Connecting to master. Attempt %s of %s',
+                        attempts, tries
+                    )
+                else:
+                    log.debug(
+                        'Connecting to master. Attempt %s (infinite attempts)',
+                        attempts
+                    )
+                for master in opts['local_masters']:
+                    opts['master'] = master
+                    opts.update(prep_ip_port(opts))
+                    opts.update(resolve_dns(opts))
+
+                    # on first run, update self.opts with the whole master list
+                    # to enable a minion to re-use old masters if they get fixed
+                    if 'master_list' not in opts:
+                        opts['master_list'] = copy.copy(opts['local_masters'])
+
+                    self.opts = opts
+
+                    pub_channel = salt.transport.client.AsyncPubChannel.factory(opts, **factory_kwargs)
+                    try:
+                        yield pub_channel.connect()
+                        conn = True
+                        break
+                    except SaltClientError as exc:
+                        last_exc = exc
+                        if exc.strerror.startswith('Could not access'):
+                            msg = (
+                                'Failed to initiate connection with Master '
+                                '%s: check ownership/permissions. Error '
+                                'message: %s', opts['master'], exc
+                            )
+                        else:
+                            msg = ('Master %s could not be reached, trying next '
+                                   'next master (if any)', opts['master'])
+                        log.info(msg)
+                        continue
+
+                if not conn:
+                    if attempts == tries:
+                        # Exhausted all attempts. Return exception.
+                        self.connected = False
+                        self.opts['master'] = copy.copy(self.opts['local_masters'])
+                        log.error(
+                            'No master could be reached or all masters '
+                            'denied the minion\'s connection attempt.'
+                        )
+                        # If the code reaches this point, 'last_exc'
+                        # should already be set.
+                        raise last_exc  # pylint: disable=E0702
+                else:
+                    self.tok = pub_channel.auth.gen_token(b'salt')
+                    self.connected = True
+                    raise tornado.gen.Return((opts['master'], pub_channel))
+
+        # single master sign in
+        else:
+            if opts['random_master']:
+                log.warning('random_master is True but there is only one master specified. Ignoring.')
+            while True:
+                if attempts != 0:
+                    # Give up a little time between connection attempts
+                    # to allow the IOLoop to run any other scheduled tasks.
+                    yield tornado.gen.sleep(opts['acceptance_wait_time'])
+                attempts += 1
+                if tries > 0:
+                    log.debug(
+                        'Connecting to master. Attempt %s of %s',
+                        attempts, tries
+                    )
+                else:
+                    log.debug(
+                        'Connecting to master. Attempt %s (infinite attempts)',
+                        attempts
+                    )
+                opts.update(prep_ip_port(opts))
+                opts.update(resolve_dns(opts))
+                try:
+                    if self.opts['transport'] == 'detect':
+                        self.opts['detect_mode'] = True
+                        for trans in ('zeromq', 'tcp'):
+                            if trans == 'zeromq' and not zmq:
+                                continue
+                            self.opts['transport'] = trans
+                            pub_channel = salt.transport.client.AsyncPubChannel.factory(self.opts, **factory_kwargs)
+                            yield pub_channel.connect()
+                            if not pub_channel.auth.authenticated:
+                                continue
+                            del self.opts['detect_mode']
+                            break
+                    else:
+                        pub_channel = salt.transport.client.AsyncPubChannel.factory(self.opts, **factory_kwargs)
+                        yield pub_channel.connect()
+                    self.tok = pub_channel.auth.gen_token(b'salt')
+                    self.connected = True
+                    raise tornado.gen.Return((opts['master'], pub_channel))
+                except SaltClientError as exc:
+                    if attempts == tries:
+                        # Exhausted all attempts. Return exception.
+                        self.connected = False
+                        raise exc
+
+    def _discover_masters(self):
+        '''
+        Discover master(s) and decide where to connect, if SSDP is around.
+        This modifies the configuration on the fly.
+        :return:
+        '''
+        if self.opts['master'] == DEFAULT_MINION_OPTS['master'] and self.opts['discovery'] is not False:
+            master_discovery_client = salt.utils.ssdp.SSDPDiscoveryClient()
+            masters = {}
+            for att in range(self.opts['discovery'].get('attempts', 3)):
+                try:
+                    att += 1
+                    log.info('Attempting %s time(s) to discover masters', att)
+                    masters.update(master_discovery_client.discover())
+                    if not masters:
+                        time.sleep(self.opts['discovery'].get('pause', 5))
+                    else:
+                        break
+                except Exception as err:
+                    log.error('SSDP discovery failure: %s', err)
+                    break
+
+            if masters:
+                policy = self.opts.get('discovery', {}).get('match', 'any')
+                if policy not in ['any', 'all']:
+                    log.error('SSDP configuration matcher failure: unknown value "%s". '
+                              'Should be "any" or "all"', policy)
+                else:
+                    mapping = self.opts['discovery'].get('mapping', {})
+                    for addr, mappings in masters.items():
+                        for proto_data in mappings:
+                            cnt = len([key for key, value in mapping.items()
+                                       if proto_data.get('mapping', {}).get(key) == value])
+                            if policy == 'any' and bool(cnt) or cnt == len(mapping):
+                                self.opts['master'] = proto_data['master']
+                                return
+
+    def _return_retry_timer(self):
+        '''
+        Based on the minion configuration, either return a randomized timer or
+        just return the value of the return_retry_timer.
+        '''
+        msg = 'Minion return retry timer set to %s seconds'
+        if self.opts.get('return_retry_timer_max'):
+            try:
+                random_retry = randint(self.opts['return_retry_timer'], self.opts['return_retry_timer_max'])
+                retry_msg = msg % random_retry
+                log.debug('%s (randomized)', msg % random_retry)
+                return random_retry
+            except ValueError:
+                # Catch wiseguys using negative integers here
+                log.error(
+                    'Invalid value (return_retry_timer: %s or '
+                    'return_retry_timer_max: %s). Both must be positive '
+                    'integers.',
+                    self.opts['return_retry_timer'],
+                    self.opts['return_retry_timer_max'],
+                )
+                log.debug(msg, DEFAULT_MINION_OPTS['return_retry_timer'])
+                return DEFAULT_MINION_OPTS['return_retry_timer']
+        else:
+            log.debug(msg, self.opts.get('return_retry_timer'))
+            return self.opts.get('return_retry_timer')
+
+
+class SMinion(MinionBase):
+    '''
+    Create an object that has loaded all of the minion module functions,
+    grains, modules, returners etc.  The SMinion allows developers to
+    generate all of the salt minion functions and present them with these
+    functions for general use.
+    '''
+    def __init__(self, opts):
+        # Late setup of the opts grains, so we can log from the grains module
+        import salt.loader
+        opts['grains'] = salt.loader.grains(opts)
+        super(SMinion, self).__init__(opts)
+
+        # Clean out the proc directory (default /var/cache/salt/minion/proc)
+        if (self.opts.get('file_client', 'remote') == 'remote'
+                or self.opts.get('use_master_when_local', False)):
+            install_zmq()
+            io_loop = ZMQDefaultLoop.current()
+            io_loop.run_sync(
+                lambda: self.eval_master(self.opts, failed=True)
+            )
+        self.gen_modules(initial_load=True)
+
+        # If configured, cache pillar data on the minion
+        if self.opts['file_client'] == 'remote' and self.opts.get('minion_pillar_cache', False):
+            import salt.utils.yaml
+            pdir = os.path.join(self.opts['cachedir'], 'pillar')
+            if not os.path.isdir(pdir):
+                os.makedirs(pdir, 0o700)
+            ptop = os.path.join(pdir, 'top.sls')
+            if self.opts['saltenv'] is not None:
+                penv = self.opts['saltenv']
+            else:
+                penv = 'base'
+            cache_top = {penv: {self.opts['id']: ['cache']}}
+            with salt.utils.files.fopen(ptop, 'wb') as fp_:
+                salt.utils.yaml.safe_dump(cache_top, fp_)
+                os.chmod(ptop, 0o600)
+            cache_sls = os.path.join(pdir, 'cache.sls')
+            with salt.utils.files.fopen(cache_sls, 'wb') as fp_:
+                salt.utils.yaml.safe_dump(self.opts['pillar'], fp_)
+                os.chmod(cache_sls, 0o600)
+
+    def gen_modules(self, initial_load=False):
+        '''
+        Tell the minion to reload the execution modules
+
+        CLI Example:
+
+        .. code-block:: bash
+
+            salt '*' sys.reload_modules
+        '''
+        self.opts['pillar'] = salt.pillar.get_pillar(
+            self.opts,
+            self.opts['grains'],
+            self.opts['id'],
+            self.opts['saltenv'],
+            pillarenv=self.opts.get('pillarenv'),
+        ).compile_pillar()
+
+        self.utils = salt.loader.utils(self.opts)
+        self.functions = salt.loader.minion_mods(self.opts, utils=self.utils)
+        self.serializers = salt.loader.serializers(self.opts)
+        self.returners = salt.loader.returners(self.opts, self.functions)
+        self.proxy = salt.loader.proxy(self.opts, self.functions, self.returners, None)
+        # TODO: remove
+        self.function_errors = {}  # Keep the funcs clean
+        self.states = salt.loader.states(self.opts,
+                self.functions,
+                self.utils,
+                self.serializers)
+        self.rend = salt.loader.render(self.opts, self.functions)
+#        self.matcher = Matcher(self.opts, self.functions)
+        self.matchers = salt.loader.matchers(self.opts)
+        self.functions['sys.reload_modules'] = self.gen_modules
+        self.executors = salt.loader.executors(self.opts)
+
+
+class MasterMinion(object):
+    '''
+    Create a fully loaded minion function object for generic use on the
+    master. What makes this class different is that the pillar is
+    omitted, otherwise everything else is loaded cleanly.
+    '''
+    def __init__(
+            self,
+            opts,
+            returners=True,
+            states=True,
+            rend=True,
+            matcher=True,
+            whitelist=None,
+            ignore_config_errors=True):
+        self.opts = salt.config.minion_config(
+            opts['conf_file'],
+            ignore_config_errors=ignore_config_errors,
+            role='master'
+        )
+        self.opts.update(opts)
+        self.whitelist = whitelist
+        self.opts['grains'] = salt.loader.grains(opts)
+        self.opts['pillar'] = {}
+        self.mk_returners = returners
+        self.mk_states = states
+        self.mk_rend = rend
+        self.mk_matcher = matcher
+        self.gen_modules(initial_load=True)
+
+    def gen_modules(self, initial_load=False):
+        '''
+        Tell the minion to reload the execution modules
+
+        CLI Example:
+
+        .. code-block:: bash
+
+            salt '*' sys.reload_modules
+        '''
+        self.utils = salt.loader.utils(self.opts)
+        self.functions = salt.loader.minion_mods(
+            self.opts,
+            utils=self.utils,
+            whitelist=self.whitelist,
+            initial_load=initial_load)
+        self.serializers = salt.loader.serializers(self.opts)
+        if self.mk_returners:
+            self.returners = salt.loader.returners(self.opts, self.functions)
+        if self.mk_states:
+            self.states = salt.loader.states(self.opts,
+                                             self.functions,
+                                             self.utils,
+                                             self.serializers)
+        if self.mk_rend:
+            self.rend = salt.loader.render(self.opts, self.functions)
+        if self.mk_matcher:
+            self.matchers = salt.loader.matchers(self.opts)
+        self.functions['sys.reload_modules'] = self.gen_modules
+
+
+class MinionManager(MinionBase):
+    '''
+    Create a multi minion interface, this creates as many minions as are
+    defined in the master option and binds each minion object to a respective
+    master.
+    '''
+    def __init__(self, opts):
+        super(MinionManager, self).__init__(opts)
+        self.auth_wait = self.opts['acceptance_wait_time']
+        self.max_auth_wait = self.opts['acceptance_wait_time_max']
+        self.minions = []
+        self.jid_queue = []
+
+        install_zmq()
+        self.io_loop = ZMQDefaultLoop.current()
+        self.process_manager = ProcessManager(name='MultiMinionProcessManager')
+        self.io_loop.spawn_callback(self.process_manager.run, **{'asynchronous': True})  # Tornado backward compat
+
+    def __del__(self):
+        self.destroy()
+
+    def _bind(self):
+        # start up the event publisher, so we can see events during startup
+        self.event_publisher = salt.utils.event.AsyncEventPublisher(
+            self.opts,
+            io_loop=self.io_loop,
+        )
+        self.event = salt.utils.event.get_event('minion', opts=self.opts, io_loop=self.io_loop)
+        self.event.subscribe('')
+        self.event.set_event_handler(self.handle_event)
+
+    @tornado.gen.coroutine
+    def handle_event(self, package):
+        yield [minion.handle_event(package) for minion in self.minions]
+
+    def _create_minion_object(self, opts, timeout, safe,
+                              io_loop=None, loaded_base_name=None,
+                              jid_queue=None):
+        '''
+        Helper function to return the correct type of object
+        '''
+        return Minion(opts,
+                      timeout,
+                      safe,
+                      io_loop=io_loop,
+                      loaded_base_name=loaded_base_name,
+                      jid_queue=jid_queue)
+
+    def _check_minions(self):
+        '''
+        Check the size of self.minions and raise an error if it's empty
+        '''
+        if not self.minions:
+            err = ('Minion unable to successfully connect to '
+                   'a Salt Master.')
+            log.error(err)
+
+    def _spawn_minions(self, timeout=60):
+        '''
+        Spawn all the coroutines which will sign in to masters
+        '''
+        masters = self.opts['master']
+        if (self.opts['master_type'] in ('failover', 'distributed')) or not isinstance(self.opts['master'], list):
+            masters = [masters]
+
+        for master in masters:
+            s_opts = copy.deepcopy(self.opts)
+            s_opts['master'] = master
+            s_opts['multimaster'] = True
+            minion = self._create_minion_object(s_opts,
+                                                s_opts['auth_timeout'],
+                                                False,
+                                                io_loop=self.io_loop,
+                                                loaded_base_name='salt.loader.{0}'.format(s_opts['master']),
+                                                jid_queue=self.jid_queue)
+            self.io_loop.spawn_callback(self._connect_minion, minion)
+        self.io_loop.call_later(timeout, self._check_minions)
+
+    @tornado.gen.coroutine
+    def _connect_minion(self, minion):
+        '''
+        Create a minion, and asynchronously connect it to a master
+        '''
+        last = 0  # never have we signed in
+        auth_wait = minion.opts['acceptance_wait_time']
+        failed = False
+        while True:
+            try:
+                if minion.opts.get('beacons_before_connect', False):
+                    minion.setup_beacons(before_connect=True)
+                if minion.opts.get('scheduler_before_connect', False):
+                    minion.setup_scheduler(before_connect=True)
+                yield minion.connect_master(failed=failed)
+                minion.tune_in(start=False)
+                self.minions.append(minion)
+                break
+            except SaltClientError as exc:
+                failed = True
+                log.error(
+                    'Error while bringing up minion for multi-master. Is '
+                    'master at %s responding?', minion.opts['master']
+                )
+                last = time.time()
+                if auth_wait < self.max_auth_wait:
+                    auth_wait += self.auth_wait
+                yield tornado.gen.sleep(auth_wait)  # TODO: log?
+            except SaltMasterUnresolvableError:
+                err = 'Master address: \'{0}\' could not be resolved. Invalid or unresolveable address. ' \
+                      'Set \'master\' value in minion config.'.format(minion.opts['master'])
+                log.error(err)
+                break
+            except Exception as e:
+                failed = True
+                log.critical(
+                    'Unexpected error while connecting to %s',
+                    minion.opts['master'], exc_info=True
+                )
+
+    # Multi Master Tune In
+    def tune_in(self):
+        '''
+        Bind to the masters
+
+        This loop will attempt to create connections to masters it hasn't connected
+        to yet, but once the initial connection is made it is up to ZMQ to do the
+        reconnect (don't know of an API to get the state here in salt)
+        '''
+        self._bind()
+
+        # Fire off all the minion coroutines
+        self._spawn_minions()
+
+        # serve forever!
+        self.io_loop.start()
+
+    @property
+    def restart(self):
+        for minion in self.minions:
+            if minion.restart:
+                return True
+        return False
+
+    def stop(self, signum):
+        for minion in self.minions:
+            minion.process_manager.stop_restarting()
+            minion.process_manager.send_signal_to_processes(signum)
+            # kill any remaining processes
+            minion.process_manager.kill_children()
+            minion.destroy()
+
+    def destroy(self):
+        for minion in self.minions:
+            minion.destroy()
+
+
+class Minion(MinionBase):
+    '''
+    This class instantiates a minion, runs connections for a minion,
+    and loads all of the functions into the minion
+    '''
+    def __init__(self, opts, timeout=60, safe=True, loaded_base_name=None, io_loop=None, jid_queue=None):  # pylint: disable=W0231
+        '''
+        Pass in the options dict
+        '''
+        # this means that the parent class doesn't know *which* master we connect to
+        super(Minion, self).__init__(opts)
+        self.timeout = timeout
+        self.safe = safe
+
+        self._running = None
+        self.win_proc = []
+        self.loaded_base_name = loaded_base_name
+        self.connected = False
+        self.restart = False
+        # Flag meaning minion has finished initialization including first connect to the master.
+        # True means the Minion is fully functional and ready to handle events.
+        self.ready = False
+        self.jid_queue = [] if jid_queue is None else jid_queue
+        self.periodic_callbacks = {}
+
+        if io_loop is None:
+            install_zmq()
+            self.io_loop = ZMQDefaultLoop.current()
+        else:
+            self.io_loop = io_loop
+
+        # Warn if ZMQ < 3.2
+        if zmq:
+            if ZMQ_VERSION_INFO < (3, 2):
+                log.warning(
+                    'You have a version of ZMQ less than ZMQ 3.2! There are '
+                    'known connection keep-alive issues with ZMQ < 3.2 which '
+                    'may result in loss of contact with minions. Please '
+                    'upgrade your ZMQ!'
+                )
+        # Late setup of the opts grains, so we can log from the grains
+        # module.  If this is a proxy, however, we need to init the proxymodule
+        # before we can get the grains.  We do this for proxies in the
+        # post_master_init
+        if not salt.utils.platform.is_proxy():
+            self.opts['grains'] = salt.loader.grains(opts)
+        else:
+            if self.opts.get('beacons_before_connect', False):
+                log.warning(
+                    '\'beacons_before_connect\' is not supported '
+                    'for proxy minions. Setting to False'
+                )
+                self.opts['beacons_before_connect'] = False
+            if self.opts.get('scheduler_before_connect', False):
+                log.warning(
+                    '\'scheduler_before_connect\' is not supported '
+                    'for proxy minions. Setting to False'
+                )
+                self.opts['scheduler_before_connect'] = False
+
+        log.info('Creating minion process manager')
+
+        if self.opts['random_startup_delay']:
+            sleep_time = random.randint(0, self.opts['random_startup_delay'])
+            log.info(
+                'Minion sleeping for %s seconds due to configured '
+                'startup_delay between 0 and %s seconds',
+                sleep_time, self.opts['random_startup_delay']
+            )
+            time.sleep(sleep_time)
+
+        self.process_manager = ProcessManager(name='MinionProcessManager')
+        self.io_loop.spawn_callback(self.process_manager.run, **{'asynchronous': True})
+        # We don't have the proxy setup yet, so we can't start engines
+        # Engines need to be able to access __proxy__
+        if not salt.utils.platform.is_proxy():
+            self.io_loop.spawn_callback(salt.engines.start_engines, self.opts,
+                                        self.process_manager)
+
+        # Install the SIGINT/SIGTERM handlers if not done so far
+        if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:
+            # No custom signal handling was added, install our own
+            signal.signal(signal.SIGINT, self._handle_signals)
+
+        if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:
+            # No custom signal handling was added, install our own
+            signal.signal(signal.SIGTERM, self._handle_signals)
+
+    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument
+        self._running = False
+        # escalate the signals to the process manager
+        self.process_manager.stop_restarting()
+        self.process_manager.send_signal_to_processes(signum)
+        # kill any remaining processes
+        self.process_manager.kill_children()
+        time.sleep(1)
+        sys.exit(0)
+
+    def sync_connect_master(self, timeout=None, failed=False):
+        '''
+        Block until we are connected to a master
+        '''
+        self._sync_connect_master_success = False
+        log.debug("sync_connect_master")
+
+        def on_connect_master_future_done(future):
+            self._sync_connect_master_success = True
+            self.io_loop.stop()
+
+        self._connect_master_future = self.connect_master(failed=failed)
+        # finish connecting to master
+        self._connect_master_future.add_done_callback(on_connect_master_future_done)
+        if timeout:
+            self.io_loop.call_later(timeout, self.io_loop.stop)
+        try:
+            self.io_loop.start()
+        except KeyboardInterrupt:
+            self.destroy()
+        # I made the following 3 line oddity to preserve traceback.
+        # Please read PR #23978 before changing, hopefully avoiding regressions.
+        # Good luck, we're all counting on you.  Thanks.
+        if self._connect_master_future.done():
+            future_exception = self._connect_master_future.exception()
+            if future_exception:
+                # This needs to be re-raised to preserve restart_on_error behavior.
+                raise six.reraise(*future_exception)
+        if timeout and self._sync_connect_master_success is False:
+            raise SaltDaemonNotRunning('Failed to connect to the salt-master')
+
+    @tornado.gen.coroutine
+    def connect_master(self, failed=False):
+        '''
+        Return a future which will complete when you are connected to a master
+        '''
+        master, self.pub_channel = yield self.eval_master(self.opts, self.timeout, self.safe, failed)
+        yield self._post_master_init(master)
+
+    # TODO: better name...
+    @tornado.gen.coroutine
+    def _post_master_init(self, master):
+        '''
+        Function to finish init after connecting to a master
+
+        This is primarily loading modules, pillars, etc. (since they need
+        to know which master they connected to)
+
+        If this function is changed, please check ProxyMinion._post_master_init
+        to see if those changes need to be propagated.
+
+        Minions and ProxyMinions need significantly different post master setups,
+        which is why the differences are not factored out into separate helper
+        functions.
+        '''
+        if self.connected:
+            self.opts['master'] = master
+
+            # Initialize pillar before loader to make pillar accessible in modules
+            async_pillar = salt.pillar.get_async_pillar(
+                self.opts,
+                self.opts['grains'],
+                self.opts['id'],
+                self.opts['saltenv'],
+                pillarenv=self.opts.get('pillarenv')
+            )
+            self.opts['pillar'] = yield async_pillar.compile_pillar()
+            async_pillar.destroy()
+
+        if not self.ready:
+            self._setup_core()
+        elif self.connected and self.opts['pillar']:
+            # The pillar has changed due to the connection to the master.
+            # Reload the functions so that they can use the new pillar data.
+            self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+            if hasattr(self, 'schedule'):
+                self.schedule.functions = self.functions
+                self.schedule.returners = self.returners
+
+        if not hasattr(self, 'schedule'):
+            self.schedule = salt.utils.schedule.Schedule(
+                self.opts,
+                self.functions,
+                self.returners,
+                cleanup=[master_event(type='alive')])
+
+        # add default scheduling jobs to the minions scheduler
+        if self.opts['mine_enabled'] and 'mine.update' in self.functions:
+            self.schedule.add_job({
+                '__mine_interval':
+                {
+                    'function': 'mine.update',
+                    'minutes': self.opts['mine_interval'],
+                    'jid_include': True,
+                    'maxrunning': 2,
+                    'run_on_start': True,
+                    'return_job': self.opts.get('mine_return_job', False)
+                }
+            }, persist=True)
+            log.info('Added mine.update to scheduler')
+        else:
+            self.schedule.delete_job('__mine_interval', persist=True)
+
+        # add master_alive job if enabled
+        if (self.opts['transport'] != 'tcp' and
+                self.opts['master_alive_interval'] > 0 and
+                self.connected):
+            self.schedule.add_job({
+                master_event(type='alive', master=self.opts['master']):
+                {
+                    'function': 'status.master',
+                    'seconds': self.opts['master_alive_interval'],
+                    'jid_include': True,
+                    'maxrunning': 1,
+                    'return_job': False,
+                    'kwargs': {'master': self.opts['master'],
+                                'connected': True}
+                }
+            }, persist=True)
+            if self.opts['master_failback'] and \
+                    'master_list' in self.opts and \
+                    self.opts['master'] != self.opts['master_list'][0]:
+                self.schedule.add_job({
+                    master_event(type='failback'):
+                    {
+                        'function': 'status.ping_master',
+                        'seconds': self.opts['master_failback_interval'],
+                        'jid_include': True,
+                        'maxrunning': 1,
+                        'return_job': False,
+                        'kwargs': {'master': self.opts['master_list'][0]}
+                    }
+                }, persist=True)
+            else:
+                self.schedule.delete_job(master_event(type='failback'), persist=True)
+        else:
+            self.schedule.delete_job(master_event(type='alive', master=self.opts['master']), persist=True)
+            self.schedule.delete_job(master_event(type='failback'), persist=True)
+
+    def _prep_mod_opts(self):
+        '''
+        Returns a copy of the opts with key bits stripped out
+        '''
+        mod_opts = {}
+        for key, val in six.iteritems(self.opts):
+            if key == 'logger':
+                continue
+            mod_opts[key] = val
+        return mod_opts
+
+    def _load_modules(self, force_refresh=False, notify=False, grains=None):
+        '''
+        Return the functions and the returners loaded up from the loader
+        module
+        '''
+        # if this is a *nix system AND modules_max_memory is set, lets enforce
+        # a memory limit on module imports
+        # this feature ONLY works on *nix like OSs (resource module doesn't work on windows)
+        modules_max_memory = False
+        if self.opts.get('modules_max_memory', -1) > 0 and HAS_PSUTIL and HAS_RESOURCE:
+            log.debug(
+                'modules_max_memory set, enforcing a maximum of %s',
+                self.opts['modules_max_memory']
+            )
+            modules_max_memory = True
+            old_mem_limit = resource.getrlimit(resource.RLIMIT_AS)
+            rss, vms = psutil.Process(os.getpid()).memory_info()[:2]
+            mem_limit = rss + vms + self.opts['modules_max_memory']
+            resource.setrlimit(resource.RLIMIT_AS, (mem_limit, mem_limit))
+        elif self.opts.get('modules_max_memory', -1) > 0:
+            if not HAS_PSUTIL:
+                log.error('Unable to enforce modules_max_memory because psutil is missing')
+            if not HAS_RESOURCE:
+                log.error('Unable to enforce modules_max_memory because resource is missing')
+
+        # This might be a proxy minion
+        if hasattr(self, 'proxy'):
+            proxy = self.proxy
+        else:
+            proxy = None
+
+        if grains is None:
+            self.opts['grains'] = salt.loader.grains(self.opts, force_refresh, proxy=proxy)
+        self.utils = salt.loader.utils(self.opts, proxy=proxy)
+
+        if self.opts.get('multimaster', False):
+            s_opts = copy.deepcopy(self.opts)
+            functions = salt.loader.minion_mods(s_opts, utils=self.utils, proxy=proxy,
+                                                loaded_base_name=self.loaded_base_name, notify=notify)
+        else:
+            functions = salt.loader.minion_mods(self.opts, utils=self.utils, notify=notify, proxy=proxy)
+        returners = salt.loader.returners(self.opts, functions, proxy=proxy)
+        errors = {}
+        if '_errors' in functions:
+            errors = functions['_errors']
+            functions.pop('_errors')
+
+        # we're done, reset the limits!
+        if modules_max_memory is True:
+            resource.setrlimit(resource.RLIMIT_AS, old_mem_limit)
+
+        executors = salt.loader.executors(self.opts, functions, proxy=proxy)
+
+        return functions, returners, errors, executors
+
+    def _send_req_sync(self, load, timeout):
+
+        if self.opts['minion_sign_messages']:
+            log.trace('Signing event to be published onto the bus.')
+            minion_privkey_path = os.path.join(self.opts['pki_dir'], 'minion.pem')
+            sig = salt.crypt.sign_message(minion_privkey_path, salt.serializers.msgpack.serialize(load))
+            load['sig'] = sig
+
+        channel = salt.transport.client.ReqChannel.factory(self.opts)
+        try:
+            return channel.send(load, timeout=timeout)
+        finally:
+            channel.close()
+
+    @tornado.gen.coroutine
+    def _send_req_async(self, load, timeout):
+
+        if self.opts['minion_sign_messages']:
+            log.trace('Signing event to be published onto the bus.')
+            minion_privkey_path = os.path.join(self.opts['pki_dir'], 'minion.pem')
+            sig = salt.crypt.sign_message(minion_privkey_path, salt.serializers.msgpack.serialize(load))
+            load['sig'] = sig
+
+        channel = salt.transport.client.AsyncReqChannel.factory(self.opts)
+        try:
+            ret = yield channel.send(load, timeout=timeout)
+            raise tornado.gen.Return(ret)
+        finally:
+            channel.close()
+
+    def _fire_master(self, data=None, tag=None, events=None, pretag=None, timeout=60, sync=True, timeout_handler=None):
+        '''
+        Fire an event on the master, or drop message if unable to send.
+        '''
+        load = {'id': self.opts['id'],
+                'cmd': '_minion_event',
+                'pretag': pretag,
+                'tok': self.tok}
+        if events:
+            load['events'] = events
+        elif data and tag:
+            load['data'] = data
+            load['tag'] = tag
+        elif not data and tag:
+            load['data'] = {}
+            load['tag'] = tag
+        else:
+            return
+
+        if sync:
+            try:
+                self._send_req_sync(load, timeout)
+            except salt.exceptions.SaltReqTimeoutError:
+                log.info('fire_master failed: master could not be contacted. Request timed out.')
+                return False
+            except Exception:
+                log.info('fire_master failed: %s', traceback.format_exc())
+                return False
+        else:
+            if timeout_handler is None:
+                def handle_timeout(*_):
+                    log.info('fire_master failed: master could not be contacted. Request timed out.')
+                    return True
+                timeout_handler = handle_timeout
+
+            with tornado.stack_context.ExceptionStackContext(timeout_handler):
+                self._send_req_async(load, timeout, callback=lambda f: None)  # pylint: disable=unexpected-keyword-arg
+        return True
+
+    @tornado.gen.coroutine
+    def _handle_decoded_payload(self, data):
+        '''
+        Override this method if you wish to handle the decoded data
+        differently.
+        '''
+        # Ensure payload is unicode. Disregard failure to decode binary blobs.
+        if six.PY2:
+            data = salt.utils.data.decode(data, keep=True)
+        if 'user' in data:
+            log.info(
+                'User %s Executing command %s with jid %s',
+                data['user'], data['fun'], data['jid']
+            )
+        else:
+            log.info(
+                'Executing command %s with jid %s',
+                data['fun'], data['jid']
+            )
+        log.debug('Command details %s', data)
+
+        # Don't duplicate jobs
+        log.trace('Started JIDs: %s', self.jid_queue)
+        if self.jid_queue is not None:
+            if data['jid'] in self.jid_queue:
+                return
+            else:
+                self.jid_queue.append(data['jid'])
+                if len(self.jid_queue) > self.opts['minion_jid_queue_hwm']:
+                    self.jid_queue.pop(0)
+
+        if isinstance(data['fun'], six.string_types):
+            if data['fun'] == 'sys.reload_modules':
+                self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+                self.schedule.functions = self.functions
+                self.schedule.returners = self.returners
+
+        process_count_max = self.opts.get('process_count_max')
+        if process_count_max > 0:
+            process_count = len(salt.utils.minion.running(self.opts))
+            while process_count >= process_count_max:
+                log.warning("Maximum number of processes reached while executing jid %s, waiting...", data['jid'])
+                yield tornado.gen.sleep(10)
+                process_count = len(salt.utils.minion.running(self.opts))
+
+        # We stash an instance references to allow for the socket
+        # communication in Windows. You can't pickle functions, and thus
+        # python needs to be able to reconstruct the reference on the other
+        # side.
+        instance = self
+        multiprocessing_enabled = self.opts.get('multiprocessing', True)
+        if multiprocessing_enabled:
+            if sys.platform.startswith('win'):
+                # let python reconstruct the minion on the other side if we're
+                # running on windows
+                instance = None
+            with default_signals(signal.SIGINT, signal.SIGTERM):
+                process = SignalHandlingMultiprocessingProcess(
+                    target=self._target, args=(instance, self.opts, data, self.connected)
+                )
+        else:
+            process = threading.Thread(
+                target=self._target,
+                args=(instance, self.opts, data, self.connected),
+                name=data['jid']
+            )
+
+        if multiprocessing_enabled:
+            with default_signals(signal.SIGINT, signal.SIGTERM):
+                # Reset current signals before starting the process in
+                # order not to inherit the current signal handlers
+                process.start()
+        else:
+            process.start()
+
+        # TODO: remove the windows specific check?
+        if multiprocessing_enabled and not salt.utils.platform.is_windows():
+            # we only want to join() immediately if we are daemonizing a process
+            process.join()
+        else:
+            self.win_proc.append(process)
+
+    def ctx(self):
+        '''
+        Return a single context manager for the minion's data
+        '''
+        if six.PY2:
+            return contextlib.nested(
+                self.functions.context_dict.clone(),
+                self.returners.context_dict.clone(),
+                self.executors.context_dict.clone(),
+            )
+        else:
+            exitstack = contextlib.ExitStack()
+            exitstack.enter_context(self.functions.context_dict.clone())
+            exitstack.enter_context(self.returners.context_dict.clone())
+            exitstack.enter_context(self.executors.context_dict.clone())
+            return exitstack
+
+    @classmethod
+    def _target(cls, minion_instance, opts, data, connected):
+        if not minion_instance:
+            minion_instance = cls(opts)
+            minion_instance.connected = connected
+            if not hasattr(minion_instance, 'functions'):
+                functions, returners, function_errors, executors = (
+                    minion_instance._load_modules(grains=opts['grains'])
+                    )
+                minion_instance.functions = functions
+                minion_instance.returners = returners
+                minion_instance.function_errors = function_errors
+                minion_instance.executors = executors
+            if not hasattr(minion_instance, 'serial'):
+                minion_instance.serial = salt.payload.Serial(opts)
+            if not hasattr(minion_instance, 'proc_dir'):
+                uid = salt.utils.user.get_uid(user=opts.get('user', None))
+                minion_instance.proc_dir = (
+                    get_proc_dir(opts['cachedir'], uid=uid)
+                    )
+
+        def run_func(minion_instance, opts, data):
+            if isinstance(data['fun'], tuple) or isinstance(data['fun'], list):
+                return Minion._thread_multi_return(minion_instance, opts, data)
+            else:
+                return Minion._thread_return(minion_instance, opts, data)
+
+        with tornado.stack_context.StackContext(functools.partial(RequestContext,
+                                                                  {'data': data, 'opts': opts})):
+            with tornado.stack_context.StackContext(minion_instance.ctx):
+                run_func(minion_instance, opts, data)
+
+    @classmethod
+    def _thread_return(cls, minion_instance, opts, data):
+        '''
+        This method should be used as a threading target, start the actual
+        minion side execution.
+        '''
+        fn_ = os.path.join(minion_instance.proc_dir, data['jid'])
+
+        if opts['multiprocessing'] and not salt.utils.platform.is_windows():
+            # Shutdown the multiprocessing before daemonizing
+            salt.log.setup.shutdown_multiprocessing_logging()
+
+            salt.utils.process.daemonize_if(opts)
+
+            # Reconfigure multiprocessing logging after daemonizing
+            salt.log.setup.setup_multiprocessing_logging()
+
+        salt.utils.process.appendproctitle('{0}._thread_return {1}'.format(cls.__name__, data['jid']))
+
+        sdata = {'pid': os.getpid()}
+        sdata.update(data)
+        log.info('Starting a new job %s with PID %s', data['jid'], sdata['pid'])
+        with salt.utils.files.fopen(fn_, 'w+b') as fp_:
+            fp_.write(minion_instance.serial.dumps(sdata))
+        ret = {'success': False}
+        function_name = data['fun']
+        executors = data.get('module_executors') or \
+                    getattr(minion_instance, 'module_executors', []) or \
+                    opts.get('module_executors', ['direct_call'])
+        allow_missing_funcs = any([
+            minion_instance.executors['{0}.allow_missing_func'.format(executor)](function_name)
+            for executor in executors
+            if '{0}.allow_missing_func' in minion_instance.executors
+        ])
+        if function_name in minion_instance.functions or allow_missing_funcs is True:
+            try:
+                minion_blackout_violation = False
+                if minion_instance.connected and minion_instance.opts['pillar'].get('minion_blackout', False):
+                    whitelist = minion_instance.opts['pillar'].get('minion_blackout_whitelist', [])
+                    # this minion is blacked out. Only allow saltutil.refresh_pillar and the whitelist
+                    if function_name != 'saltutil.refresh_pillar' and function_name not in whitelist:
+                        minion_blackout_violation = True
+                # use minion_blackout_whitelist from grains if it exists
+                if minion_instance.opts['grains'].get('minion_blackout', False):
+                    whitelist = minion_instance.opts['grains'].get('minion_blackout_whitelist', [])
+                    if function_name != 'saltutil.refresh_pillar' and function_name not in whitelist:
+                        minion_blackout_violation = True
+                if minion_blackout_violation:
+                    raise SaltInvocationError('Minion in blackout mode. Set \'minion_blackout\' '
+                                             'to False in pillar or grains to resume operations. Only '
+                                             'saltutil.refresh_pillar allowed in blackout mode.')
+
+                if function_name in minion_instance.functions:
+                    func = minion_instance.functions[function_name]
+                    args, kwargs = load_args_and_kwargs(
+                        func,
+                        data['arg'],
+                        data)
+                else:
+                    # only run if function_name is not in minion_instance.functions and allow_missing_funcs is True
+                    func = function_name
+                    args, kwargs = data['arg'], data
+                minion_instance.functions.pack['__context__']['retcode'] = 0
+                if isinstance(executors, six.string_types):
+                    executors = [executors]
+                elif not isinstance(executors, list) or not executors:
+                    raise SaltInvocationError("Wrong executors specification: {0}. String or non-empty list expected".
+                        format(executors))
+                if opts.get('sudo_user', '') and executors[-1] != 'sudo':
+                    executors[-1] = 'sudo'  # replace the last one with sudo
+                log.trace('Executors list %s', executors)  # pylint: disable=no-member
+
+                for name in executors:
+                    fname = '{0}.execute'.format(name)
+                    if fname not in minion_instance.executors:
+                        raise SaltInvocationError("Executor '{0}' is not available".format(name))
+                    return_data = minion_instance.executors[fname](opts, data, func, args, kwargs)
+                    if return_data is not None:
+                        break
+
+                if isinstance(return_data, types.GeneratorType):
+                    ind = 0
+                    iret = {}
+                    for single in return_data:
+                        if isinstance(single, dict) and isinstance(iret, dict):
+                            iret.update(single)
+                        else:
+                            if not iret:
+                                iret = []
+                            iret.append(single)
+                        tag = tagify([data['jid'], 'prog', opts['id'], six.text_type(ind)], 'job')
+                        event_data = {'return': single}
+                        minion_instance._fire_master(event_data, tag)
+                        ind += 1
+                    ret['return'] = iret
+                else:
+                    ret['return'] = return_data
+
+                retcode = minion_instance.functions.pack['__context__'].get(
+                    'retcode',
+                    salt.defaults.exitcodes.EX_OK
+                )
+                if retcode == salt.defaults.exitcodes.EX_OK:
+                    # No nonzero retcode in __context__ dunder. Check if return
+                    # is a dictionary with a "result" or "success" key.
+                    try:
+                        func_result = all(return_data.get(x, True)
+                                          for x in ('result', 'success'))
+                    except Exception:
+                        # return data is not a dict
+                        func_result = True
+                    if not func_result:
+                        retcode = salt.defaults.exitcodes.EX_GENERIC
+
+                ret['retcode'] = retcode
+                ret['success'] = retcode == salt.defaults.exitcodes.EX_OK
+            except CommandNotFoundError as exc:
+                msg = 'Command required for \'{0}\' not found'.format(
+                    function_name
+                )
+                log.debug(msg, exc_info=True)
+                ret['return'] = '{0}: {1}'.format(msg, exc)
+                ret['out'] = 'nested'
+                ret['retcode'] = salt.defaults.exitcodes.EX_GENERIC
+            except CommandExecutionError as exc:
+                log.error(
+                    'A command in \'%s\' had a problem: %s',
+                    function_name, exc,
+                    exc_info_on_loglevel=logging.DEBUG
+                )
+                ret['return'] = 'ERROR: {0}'.format(exc)
+                ret['out'] = 'nested'
+                ret['retcode'] = salt.defaults.exitcodes.EX_GENERIC
+            except SaltInvocationError as exc:
+                log.error(
+                    'Problem executing \'%s\': %s',
+                    function_name, exc,
+                    exc_info_on_loglevel=logging.DEBUG
+                )
+                ret['return'] = 'ERROR executing \'{0}\': {1}'.format(
+                    function_name, exc
+                )
+                ret['out'] = 'nested'
+                ret['retcode'] = salt.defaults.exitcodes.EX_GENERIC
+            except TypeError as exc:
+                msg = 'Passed invalid arguments to {0}: {1}\n{2}'.format(
+                    function_name, exc, func.__doc__ or ''
+                )
+                log.warning(msg, exc_info_on_loglevel=logging.DEBUG)
+                ret['return'] = msg
+                ret['out'] = 'nested'
+                ret['retcode'] = salt.defaults.exitcodes.EX_GENERIC
+            except Exception:
+                msg = 'The minion function caused an exception'
+                log.warning(msg, exc_info_on_loglevel=True)
+                salt.utils.error.fire_exception(salt.exceptions.MinionError(msg), opts, job=data)
+                ret['return'] = '{0}: {1}'.format(msg, traceback.format_exc())
+                ret['out'] = 'nested'
+                ret['retcode'] = salt.defaults.exitcodes.EX_GENERIC
+        else:
+            docs = minion_instance.functions['sys.doc']('{0}*'.format(function_name))
+            if docs:
+                docs[function_name] = minion_instance.functions.missing_fun_string(function_name)
+                ret['return'] = docs
+            else:
+                ret['return'] = minion_instance.functions.missing_fun_string(function_name)
+                mod_name = function_name.split('.')[0]
+                if mod_name in minion_instance.function_errors:
+                    ret['return'] += ' Possible reasons: \'{0}\''.format(
+                        minion_instance.function_errors[mod_name]
+                    )
+            ret['success'] = False
+            ret['retcode'] = salt.defaults.exitcodes.EX_GENERIC
+            ret['out'] = 'nested'
+
+        ret['jid'] = data['jid']
+        ret['fun'] = data['fun']
+        ret['fun_args'] = data['arg']
+        if 'master_id' in data:
+            ret['master_id'] = data['master_id']
+        if 'metadata' in data:
+            if isinstance(data['metadata'], dict):
+                ret['metadata'] = data['metadata']
+            else:
+                log.warning('The metadata parameter must be a dictionary. Ignoring.')
+        if minion_instance.connected:
+            minion_instance._return_pub(
+                ret,
+                timeout=minion_instance._return_retry_timer()
+            )
+
+        # Add default returners from minion config
+        # Should have been coverted to comma-delimited string already
+        if isinstance(opts.get('return'), six.string_types):
+            if data['ret']:
+                data['ret'] = ','.join((data['ret'], opts['return']))
+            else:
+                data['ret'] = opts['return']
+
+        log.debug('minion return: %s', ret)
+        # TODO: make a list? Seems odd to split it this late :/
+        if data['ret'] and isinstance(data['ret'], six.string_types):
+            if 'ret_config' in data:
+                ret['ret_config'] = data['ret_config']
+            if 'ret_kwargs' in data:
+                ret['ret_kwargs'] = data['ret_kwargs']
+            ret['id'] = opts['id']
+            for returner in set(data['ret'].split(',')):
+                try:
+                    returner_str = '{0}.returner'.format(returner)
+                    if returner_str in minion_instance.returners:
+                        minion_instance.returners[returner_str](ret)
+                    else:
+                        returner_err = minion_instance.returners.missing_fun_string(returner_str)
+                        log.error(
+                            'Returner %s could not be loaded: %s',
+                            returner_str, returner_err
+                        )
+                except Exception as exc:
+                    log.exception(
+                        'The return failed for job %s: %s', data['jid'], exc
+                    )
+
+    @classmethod
+    def _thread_multi_return(cls, minion_instance, opts, data):
+        '''
+        This method should be used as a threading target, start the actual
+        minion side execution.
+        '''
+        fn_ = os.path.join(minion_instance.proc_dir, data['jid'])
+
+        if opts['multiprocessing'] and not salt.utils.platform.is_windows():
+            # Shutdown the multiprocessing before daemonizing
+            salt.log.setup.shutdown_multiprocessing_logging()
+
+            salt.utils.process.daemonize_if(opts)
+
+            # Reconfigure multiprocessing logging after daemonizing
+            salt.log.setup.setup_multiprocessing_logging()
+
+        salt.utils.process.appendproctitle('{0}._thread_multi_return {1}'.format(cls.__name__, data['jid']))
+
+        sdata = {'pid': os.getpid()}
+        sdata.update(data)
+        log.info('Starting a new job with PID %s', sdata['pid'])
+        with salt.utils.files.fopen(fn_, 'w+b') as fp_:
+            fp_.write(minion_instance.serial.dumps(sdata))
+
+        multifunc_ordered = opts.get('multifunc_ordered', False)
+        num_funcs = len(data['fun'])
+        if multifunc_ordered:
+            ret = {
+                'return': [None] * num_funcs,
+                'retcode': [None] * num_funcs,
+                'success': [False] * num_funcs
+            }
+        else:
+            ret = {
+                'return': {},
+                'retcode': {},
+                'success': {}
+            }
+
+        for ind in range(0, num_funcs):
+            if not multifunc_ordered:
+                ret['success'][data['fun'][ind]] = False
+            try:
+                minion_blackout_violation = False
+                if minion_instance.connected and minion_instance.opts['pillar'].get('minion_blackout', False):
+                    whitelist = minion_instance.opts['pillar'].get('minion_blackout_whitelist', [])
+                    # this minion is blacked out. Only allow saltutil.refresh_pillar and the whitelist
+                    if data['fun'][ind] != 'saltutil.refresh_pillar' and data['fun'][ind] not in whitelist:
+                        minion_blackout_violation = True
+                elif minion_instance.opts['grains'].get('minion_blackout', False):
+                    whitelist = minion_instance.opts['grains'].get('minion_blackout_whitelist', [])
+                    if data['fun'][ind] != 'saltutil.refresh_pillar' and data['fun'][ind] not in whitelist:
+                        minion_blackout_violation = True
+                if minion_blackout_violation:
+                    raise SaltInvocationError('Minion in blackout mode. Set \'minion_blackout\' '
+                                             'to False in pillar or grains to resume operations. Only '
+                                             'saltutil.refresh_pillar allowed in blackout mode.')
+
+                func = minion_instance.functions[data['fun'][ind]]
+
+                args, kwargs = load_args_and_kwargs(
+                    func,
+                    data['arg'][ind],
+                    data)
+                minion_instance.functions.pack['__context__']['retcode'] = 0
+                key = ind if multifunc_ordered else data['fun'][ind]
+                ret['return'][key] = func(*args, **kwargs)
+                retcode = minion_instance.functions.pack['__context__'].get(
+                    'retcode',
+                    0
+                )
+                if retcode == 0:
+                    # No nonzero retcode in __context__ dunder. Check if return
+                    # is a dictionary with a "result" or "success" key.
+                    try:
+                        func_result = all(ret['return'][key].get(x, True)
+                                          for x in ('result', 'success'))
+                    except Exception:
+                        # return data is not a dict
+                        func_result = True
+                    if not func_result:
+                        retcode = 1
+
+                ret['retcode'][key] = retcode
+                ret['success'][key] = retcode == 0
+            except Exception as exc:
+                trb = traceback.format_exc()
+                log.warning('The minion function caused an exception: %s', exc)
+                if multifunc_ordered:
+                    ret['return'][ind] = trb
+                else:
+                    ret['return'][data['fun'][ind]] = trb
+            ret['jid'] = data['jid']
+            ret['fun'] = data['fun']
+            ret['fun_args'] = data['arg']
+        if 'metadata' in data:
+            ret['metadata'] = data['metadata']
+        if minion_instance.connected:
+            minion_instance._return_pub(
+                ret,
+                timeout=minion_instance._return_retry_timer()
+            )
+        if data['ret']:
+            if 'ret_config' in data:
+                ret['ret_config'] = data['ret_config']
+            if 'ret_kwargs' in data:
+                ret['ret_kwargs'] = data['ret_kwargs']
+            for returner in set(data['ret'].split(',')):
+                ret['id'] = opts['id']
+                try:
+                    minion_instance.returners['{0}.returner'.format(
+                        returner
+                    )](ret)
+                except Exception as exc:
+                    log.error(
+                        'The return failed for job %s: %s',
+                        data['jid'], exc
+                    )
+
+    def _return_pub(self, ret, ret_cmd='_return', timeout=60, sync=True):
+        '''
+        Return the data from the executed command to the master server
+        '''
+        jid = ret.get('jid', ret.get('__jid__'))
+        fun = ret.get('fun', ret.get('__fun__'))
+        if self.opts['multiprocessing']:
+            fn_ = os.path.join(self.proc_dir, jid)
+            if os.path.isfile(fn_):
+                try:
+                    os.remove(fn_)
+                except (OSError, IOError):
+                    # The file is gone already
+                    pass
+        log.info('Returning information for job: %s', jid)
+        log.trace('Return data: %s', ret)
+        if ret_cmd == '_syndic_return':
+            load = {'cmd': ret_cmd,
+                    'id': self.opts['uid'],
+                    'jid': jid,
+                    'fun': fun,
+                    'arg': ret.get('arg'),
+                    'tgt': ret.get('tgt'),
+                    'tgt_type': ret.get('tgt_type'),
+                    'load': ret.get('__load__')}
+            if '__master_id__' in ret:
+                load['master_id'] = ret['__master_id__']
+            load['return'] = {}
+            for key, value in six.iteritems(ret):
+                if key.startswith('__'):
+                    continue
+                load['return'][key] = value
+        else:
+            load = {'cmd': ret_cmd,
+                    'id': self.opts['id']}
+            for key, value in six.iteritems(ret):
+                load[key] = value
+
+        if 'out' in ret:
+            if isinstance(ret['out'], six.string_types):
+                load['out'] = ret['out']
+            else:
+                log.error(
+                    'Invalid outputter %s. This is likely a bug.',
+                    ret['out']
+                )
+        else:
+            try:
+                oput = self.functions[fun].__outputter__
+            except (KeyError, AttributeError, TypeError):
+                pass
+            else:
+                if isinstance(oput, six.string_types):
+                    load['out'] = oput
+        if self.opts['cache_jobs']:
+            # Local job cache has been enabled
+            if ret['jid'] == 'req':
+                ret['jid'] = salt.utils.jid.gen_jid(self.opts)
+            salt.utils.minion.cache_jobs(self.opts, ret['jid'], ret)
+
+        if not self.opts['pub_ret']:
+            return ''
+
+        def timeout_handler(*_):
+            log.warning(
+               'The minion failed to return the job information for job %s. '
+               'This is often due to the master being shut down or '
+               'overloaded. If the master is running, consider increasing '
+               'the worker_threads value.', jid
+            )
+            return True
+
+        if sync:
+            try:
+                ret_val = self._send_req_sync(load, timeout=timeout)
+            except SaltReqTimeoutError:
+                timeout_handler()
+                return ''
+        else:
+            with tornado.stack_context.ExceptionStackContext(timeout_handler):
+                ret_val = self._send_req_async(load, timeout=timeout, callback=lambda f: None)  # pylint: disable=unexpected-keyword-arg
+
+        log.trace('ret_val = %s', ret_val)  # pylint: disable=no-member
+        return ret_val
+
+    def _return_pub_multi(self, rets, ret_cmd='_return', timeout=60, sync=True):
+        '''
+        Return the data from the executed command to the master server
+        '''
+        if not isinstance(rets, list):
+            rets = [rets]
+        jids = {}
+        for ret in rets:
+            jid = ret.get('jid', ret.get('__jid__'))
+            fun = ret.get('fun', ret.get('__fun__'))
+            if self.opts['multiprocessing']:
+                fn_ = os.path.join(self.proc_dir, jid)
+                if os.path.isfile(fn_):
+                    try:
+                        os.remove(fn_)
+                    except (OSError, IOError):
+                        # The file is gone already
+                        pass
+            log.info('Returning information for job: %s', jid)
+            load = jids.setdefault(jid, {})
+            if ret_cmd == '_syndic_return':
+                if not load:
+                    load.update({'id': self.opts['id'],
+                                 'jid': jid,
+                                 'fun': fun,
+                                 'arg': ret.get('arg'),
+                                 'tgt': ret.get('tgt'),
+                                 'tgt_type': ret.get('tgt_type'),
+                                 'load': ret.get('__load__'),
+                                 'return': {}})
+                if '__master_id__' in ret:
+                    load['master_id'] = ret['__master_id__']
+                for key, value in six.iteritems(ret):
+                    if key.startswith('__'):
+                        continue
+                    load['return'][key] = value
+            else:
+                load.update({'id': self.opts['id']})
+                for key, value in six.iteritems(ret):
+                    load[key] = value
+
+            if 'out' in ret:
+                if isinstance(ret['out'], six.string_types):
+                    load['out'] = ret['out']
+                else:
+                    log.error(
+                        'Invalid outputter %s. This is likely a bug.',
+                        ret['out']
+                    )
+            else:
+                try:
+                    oput = self.functions[fun].__outputter__
+                except (KeyError, AttributeError, TypeError):
+                    pass
+                else:
+                    if isinstance(oput, six.string_types):
+                        load['out'] = oput
+            if self.opts['cache_jobs']:
+                # Local job cache has been enabled
+                salt.utils.minion.cache_jobs(self.opts, load['jid'], ret)
+
+        load = {'cmd': ret_cmd,
+                'load': list(six.itervalues(jids))}
+
+        def timeout_handler(*_):
+            log.warning(
+               'The minion failed to return the job information for job %s. '
+               'This is often due to the master being shut down or '
+               'overloaded. If the master is running, consider increasing '
+               'the worker_threads value.', jid
+            )
+            return True
+
+        if sync:
+            try:
+                ret_val = self._send_req_sync(load, timeout=timeout)
+            except SaltReqTimeoutError:
+                timeout_handler()
+                return ''
+        else:
+            with tornado.stack_context.ExceptionStackContext(timeout_handler):
+                ret_val = self._send_req_async(load, timeout=timeout, callback=lambda f: None)  # pylint: disable=unexpected-keyword-arg
+
+        log.trace('ret_val = %s', ret_val)  # pylint: disable=no-member
+        return ret_val
+
+    def _state_run(self):
+        '''
+        Execute a state run based on information set in the minion config file
+        '''
+        if self.opts['startup_states']:
+            if self.opts.get('master_type', 'str') == 'disable' and \
+                        self.opts.get('file_client', 'remote') == 'remote':
+                log.warning(
+                    'Cannot run startup_states when \'master_type\' is set '
+                    'to \'disable\' and \'file_client\' is set to '
+                    '\'remote\'. Skipping.'
+                )
+            else:
+                data = {'jid': 'req', 'ret': self.opts.get('ext_job_cache', '')}
+                if self.opts['startup_states'] == 'sls':
+                    data['fun'] = 'state.sls'
+                    data['arg'] = [self.opts['sls_list']]
+                elif self.opts['startup_states'] == 'top':
+                    data['fun'] = 'state.top'
+                    data['arg'] = [self.opts['top_file']]
+                else:
+                    data['fun'] = 'state.highstate'
+                    data['arg'] = []
+                self._handle_decoded_payload(data)
+
+    def _refresh_grains_watcher(self, refresh_interval_in_minutes):
+        '''
+        Create a loop that will fire a pillar refresh to inform a master about a change in the grains of this minion
+        :param refresh_interval_in_minutes:
+        :return: None
+        '''
+        if '__update_grains' not in self.opts.get('schedule', {}):
+            if 'schedule' not in self.opts:
+                self.opts['schedule'] = {}
+            self.opts['schedule'].update({
+                '__update_grains':
+                    {
+                        'function': 'event.fire',
+                        'args': [{}, 'grains_refresh'],
+                        'minutes': refresh_interval_in_minutes
+                    }
+            })
+
+    def _fire_master_minion_start(self):
+        # Send an event to the master that the minion is live
+        if self.opts['enable_legacy_startup_events']:
+            # Old style event. Defaults to False in Sodium release.
+            self._fire_master(
+                'Minion {0} started at {1}'.format(
+                self.opts['id'],
+                time.asctime()
+                ),
+                'minion_start'
+            )
+        # send name spaced event
+        self._fire_master(
+            'Minion {0} started at {1}'.format(
+            self.opts['id'],
+            time.asctime()
+            ),
+            tagify([self.opts['id'], 'start'], 'minion'),
+        )
+
+    def module_refresh(self, force_refresh=False, notify=False):
+        '''
+        Refresh the functions and returners.
+        '''
+        log.debug('Refreshing modules. Notify=%s', notify)
+        self.functions, self.returners, _, self.executors = self._load_modules(force_refresh, notify=notify)
+
+        self.schedule.functions = self.functions
+        self.schedule.returners = self.returners
+
+    def beacons_refresh(self):
+        '''
+        Refresh the functions and returners.
+        '''
+        log.debug('Refreshing beacons.')
+        self.beacons = salt.beacons.Beacon(self.opts, self.functions)
+
+    def matchers_refresh(self):
+        '''
+        Refresh the matchers
+        '''
+        log.debug('Refreshing matchers.')
+        self.matchers = salt.loader.matchers(self.opts)
+
+    # TODO: only allow one future in flight at a time?
+    @tornado.gen.coroutine
+    def pillar_refresh(self, force_refresh=False):
+        '''
+        Refresh the pillar
+        '''
+        if self.connected:
+            log.debug('Refreshing pillar')
+            async_pillar = salt.pillar.get_async_pillar(
+                self.opts,
+                self.opts['grains'],
+                self.opts['id'],
+                self.opts['saltenv'],
+                pillarenv=self.opts.get('pillarenv'),
+            )
+            try:
+                self.opts['pillar'] = yield async_pillar.compile_pillar()
+            except SaltClientError:
+                # Do not exit if a pillar refresh fails.
+                log.error('Pillar data could not be refreshed. '
+                          'One or more masters may be down!')
+            finally:
+                async_pillar.destroy()
+        self.module_refresh(force_refresh)
+        self.matchers_refresh()
+        self.beacons_refresh()
+
+    def manage_schedule(self, tag, data):
+        '''
+        Refresh the functions and returners.
+        '''
+        func = data.get('func', None)
+        name = data.get('name', None)
+        schedule = data.get('schedule', None)
+        where = data.get('where', None)
+        persist = data.get('persist', None)
+
+        if func == 'delete':
+            self.schedule.delete_job(name, persist)
+        elif func == 'add':
+            self.schedule.add_job(schedule, persist)
+        elif func == 'modify':
+            self.schedule.modify_job(name, schedule, persist)
+        elif func == 'enable':
+            self.schedule.enable_schedule()
+        elif func == 'disable':
+            self.schedule.disable_schedule()
+        elif func == 'enable_job':
+            self.schedule.enable_job(name, persist)
+        elif func == 'run_job':
+            self.schedule.run_job(name)
+        elif func == 'disable_job':
+            self.schedule.disable_job(name, persist)
+        elif func == 'postpone_job':
+            self.schedule.postpone_job(name, data)
+        elif func == 'skip_job':
+            self.schedule.skip_job(name, data)
+        elif func == 'reload':
+            self.schedule.reload(schedule)
+        elif func == 'list':
+            self.schedule.list(where)
+        elif func == 'save_schedule':
+            self.schedule.save_schedule()
+        elif func == 'get_next_fire_time':
+            self.schedule.get_next_fire_time(name)
+
+    def manage_beacons(self, tag, data):
+        '''
+        Manage Beacons
+        '''
+        func = data.get('func', None)
+        name = data.get('name', None)
+        beacon_data = data.get('beacon_data', None)
+        include_pillar = data.get('include_pillar', None)
+        include_opts = data.get('include_opts', None)
+
+        if func == 'add':
+            self.beacons.add_beacon(name, beacon_data)
+        elif func == 'modify':
+            self.beacons.modify_beacon(name, beacon_data)
+        elif func == 'delete':
+            self.beacons.delete_beacon(name)
+        elif func == 'enable':
+            self.beacons.enable_beacons()
+        elif func == 'disable':
+            self.beacons.disable_beacons()
+        elif func == 'enable_beacon':
+            self.beacons.enable_beacon(name)
+        elif func == 'disable_beacon':
+            self.beacons.disable_beacon(name)
+        elif func == 'list':
+            self.beacons.list_beacons(include_opts, include_pillar)
+        elif func == 'list_available':
+            self.beacons.list_available_beacons()
+        elif func == 'validate_beacon':
+            self.beacons.validate_beacon(name, beacon_data)
+        elif func == 'reset':
+            self.beacons.reset()
+
+    def environ_setenv(self, tag, data):
+        '''
+        Set the salt-minion main process environment according to
+        the data contained in the minion event data
+        '''
+        environ = data.get('environ', None)
+        if environ is None:
+            return False
+        false_unsets = data.get('false_unsets', False)
+        clear_all = data.get('clear_all', False)
+        import salt.modules.environ as mod_environ
+        return mod_environ.setenv(environ, false_unsets, clear_all)
+
+    def _pre_tune(self):
+        '''
+        Set the minion running flag and issue the appropriate warnings if
+        the minion cannot be started or is already running
+        '''
+        if self._running is None:
+            self._running = True
+        elif self._running is False:
+            log.error(
+                'This %s was scheduled to stop. Not running %s.tune_in()',
+                self.__class__.__name__, self.__class__.__name__
+            )
+            return
+        elif self._running is True:
+            log.error(
+                'This %s is already running. Not running %s.tune_in()',
+                self.__class__.__name__, self.__class__.__name__
+            )
+            return
+
+        try:
+            log.info(
+                '%s is starting as user \'%s\'',
+                self.__class__.__name__, salt.utils.user.get_user()
+            )
+        except Exception as err:
+            # Only windows is allowed to fail here. See #3189. Log as debug in
+            # that case. Else, error.
+            log.log(
+                salt.utils.platform.is_windows() and logging.DEBUG or logging.ERROR,
+                'Failed to get the user who is starting %s',
+                self.__class__.__name__,
+                exc_info=err
+            )
+
+    def _mine_send(self, tag, data):
+        '''
+        Send mine data to the master
+        '''
+        channel = salt.transport.client.ReqChannel.factory(self.opts)
+        data['tok'] = self.tok
+        try:
+            ret = channel.send(data)
+            return ret
+        except SaltReqTimeoutError:
+            log.warning('Unable to send mine data to master.')
+            return None
+        finally:
+            channel.close()
+
+    @tornado.gen.coroutine
+    def handle_event(self, package):
+        '''
+        Handle an event from the epull_sock (all local minion events)
+        '''
+        if not self.ready:
+            raise tornado.gen.Return()
+        tag, data = salt.utils.event.SaltEvent.unpack(package)
+        log.debug(
+            'Minion of \'%s\' is handling event tag \'%s\'',
+            self.opts['master'], tag
+        )
+        if tag.startswith('module_refresh'):
+            self.module_refresh(
+                force_refresh=data.get('force_refresh', False),
+                notify=data.get('notify', False)
+            )
+        elif tag.startswith('pillar_refresh'):
+            yield self.pillar_refresh(
+                force_refresh=data.get('force_refresh', False)
+            )
+        elif tag.startswith('beacons_refresh'):
+            self.beacons_refresh()
+        elif tag.startswith('matchers_refresh'):
+            self.matchers_refresh()
+        elif tag.startswith('manage_schedule'):
+            self.manage_schedule(tag, data)
+        elif tag.startswith('manage_beacons'):
+            self.manage_beacons(tag, data)
+        elif tag.startswith('grains_refresh'):
+            if (data.get('force_refresh', False) or
+                    self.grains_cache != self.opts['grains']):
+                self.pillar_refresh(force_refresh=True)
+                self.grains_cache = self.opts['grains']
+        elif tag.startswith('environ_setenv'):
+            self.environ_setenv(tag, data)
+        elif tag.startswith('_minion_mine'):
+            self._mine_send(tag, data)
+        elif tag.startswith('fire_master'):
+            if self.connected:
+                log.debug('Forwarding master event tag=%s', data['tag'])
+                self._fire_master(data['data'], data['tag'], data['events'], data['pretag'])
+        elif tag.startswith(master_event(type='disconnected')) or tag.startswith(master_event(type='failback')):
+            # if the master disconnect event is for a different master, raise an exception
+            if tag.startswith(master_event(type='disconnected')) and data['master'] != self.opts['master']:
+                # not mine master, ignore
+                return
+            if tag.startswith(master_event(type='failback')):
+                # if the master failback event is not for the top master, raise an exception
+                if data['master'] != self.opts['master_list'][0]:
+                    raise SaltException('Bad master \'{0}\' when mine failback is \'{1}\''.format(
+                        data['master'], self.opts['master']))
+                # if the master failback event is for the current master, raise an exception
+                elif data['master'] == self.opts['master'][0]:
+                    raise SaltException('Already connected to \'{0}\''.format(data['master']))
+
+            if self.connected:
+                # we are not connected anymore
+                self.connected = False
+                log.info('Connection to master %s lost', self.opts['master'])
+
+                if self.opts['master_type'] != 'failover':
+                    # modify the scheduled job to fire on reconnect
+                    if self.opts['transport'] != 'tcp':
+                        schedule = {
+                           'function': 'status.master',
+                           'seconds': self.opts['master_alive_interval'],
+                           'jid_include': True,
+                           'maxrunning': 1,
+                           'return_job': False,
+                           'kwargs': {'master': self.opts['master'],
+                                       'connected': False}
+                        }
+                        self.schedule.modify_job(name=master_event(type='alive', master=self.opts['master']),
+                                                 schedule=schedule)
+                else:
+                    # delete the scheduled job to don't interfere with the failover process
+                    if self.opts['transport'] != 'tcp':
+                        self.schedule.delete_job(name=master_event(type='alive'))
+
+                    log.info('Trying to tune in to next master from master-list')
+
+                    if hasattr(self, 'pub_channel'):
+                        self.pub_channel.on_recv(None)
+                        if hasattr(self.pub_channel, 'auth'):
+                            self.pub_channel.auth.invalidate()
+                        if hasattr(self.pub_channel, 'close'):
+                            self.pub_channel.close()
+                        del self.pub_channel
+
+                    # if eval_master finds a new master for us, self.connected
+                    # will be True again on successful master authentication
+                    try:
+                        master, self.pub_channel = yield self.eval_master(
+                                                            opts=self.opts,
+                                                            failed=True,
+                                                            failback=tag.startswith(master_event(type='failback')))
+                    except SaltClientError:
+                        pass
+
+                    if self.connected:
+                        self.opts['master'] = master
+
+                        # re-init the subsystems to work with the new master
+                        log.info(
+                            'Re-initialising subsystems for new master %s',
+                            self.opts['master']
+                        )
+                        # put the current schedule into the new loaders
+                        self.opts['schedule'] = self.schedule.option('schedule')
+                        self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+                        # make the schedule to use the new 'functions' loader
+                        self.schedule.functions = self.functions
+                        self.pub_channel.on_recv(self._handle_payload)
+                        self._fire_master_minion_start()
+                        log.info('Minion is ready to receive requests!')
+
+                        # update scheduled job to run with the new master addr
+                        if self.opts['transport'] != 'tcp':
+                            schedule = {
+                               'function': 'status.master',
+                               'seconds': self.opts['master_alive_interval'],
+                               'jid_include': True,
+                               'maxrunning': 1,
+                               'return_job': False,
+                               'kwargs': {'master': self.opts['master'],
+                                           'connected': True}
+                            }
+                            self.schedule.modify_job(name=master_event(type='alive', master=self.opts['master']),
+                                                     schedule=schedule)
+
+                            if self.opts['master_failback'] and 'master_list' in self.opts:
+                                if self.opts['master'] != self.opts['master_list'][0]:
+                                    schedule = {
+                                       'function': 'status.ping_master',
+                                       'seconds': self.opts['master_failback_interval'],
+                                       'jid_include': True,
+                                       'maxrunning': 1,
+                                       'return_job': False,
+                                       'kwargs': {'master': self.opts['master_list'][0]}
+                                    }
+                                    self.schedule.modify_job(name=master_event(type='failback'),
+                                                             schedule=schedule)
+                                else:
+                                    self.schedule.delete_job(name=master_event(type='failback'), persist=True)
+                    else:
+                        self.restart = True
+                        self.io_loop.stop()
+
+        elif tag.startswith(master_event(type='connected')):
+            # handle this event only once. otherwise it will pollute the log
+            # also if master type is failover all the reconnection work is done
+            # by `disconnected` event handler and this event must never happen,
+            # anyway check it to be sure
+            if not self.connected and self.opts['master_type'] != 'failover':
+                log.info('Connection to master %s re-established', self.opts['master'])
+                self.connected = True
+                # modify the __master_alive job to only fire,
+                # if the connection is lost again
+                if self.opts['transport'] != 'tcp':
+                    schedule = {
+                       'function': 'status.master',
+                       'seconds': self.opts['master_alive_interval'],
+                       'jid_include': True,
+                       'maxrunning': 1,
+                       'return_job': False,
+                       'kwargs': {'master': self.opts['master'],
+                                   'connected': True}
+                    }
+
+                    self.schedule.modify_job(name=master_event(type='alive', master=self.opts['master']),
+                                             schedule=schedule)
+        elif tag.startswith('__schedule_return'):
+            # reporting current connection with master
+            if data['schedule'].startswith(master_event(type='alive', master='')):
+                if data['return']:
+                    log.debug(
+                        'Connected to master %s',
+                        data['schedule'].split(master_event(type='alive', master=''))[1]
+                    )
+            self._return_pub(data, ret_cmd='_return', sync=False)
+        elif tag.startswith('_salt_error'):
+            if self.connected:
+                log.debug('Forwarding salt error event tag=%s', tag)
+                self._fire_master(data, tag)
+        elif tag.startswith('salt/auth/creds'):
+            key = tuple(data['key'])
+            log.debug(
+                'Updating auth data for %s: %s -> %s',
+                key, salt.crypt.AsyncAuth.creds_map.get(key), data['creds']
+            )
+            salt.crypt.AsyncAuth.creds_map[tuple(data['key'])] = data['creds']
+
+    def _fallback_cleanups(self):
+        '''
+        Fallback cleanup routines, attempting to fix leaked processes, threads, etc.
+        '''
+        # Add an extra fallback in case a forked process leaks through
+        multiprocessing.active_children()
+
+        # Cleanup Windows threads
+        if not salt.utils.platform.is_windows():
+            return
+        for thread in self.win_proc:
+            if not thread.is_alive():
+                thread.join()
+                try:
+                    self.win_proc.remove(thread)
+                    del thread
+                except (ValueError, NameError):
+                    pass
+
+    def _setup_core(self):
+        '''
+        Set up the core minion attributes.
+        This is safe to call multiple times.
+        '''
+        if not self.ready:
+            # First call. Initialize.
+            self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+            self.serial = salt.payload.Serial(self.opts)
+            self.mod_opts = self._prep_mod_opts()
+#            self.matcher = Matcher(self.opts, self.functions)
+            self.matchers = salt.loader.matchers(self.opts)
+            self.beacons = salt.beacons.Beacon(self.opts, self.functions)
+            uid = salt.utils.user.get_uid(user=self.opts.get('user', None))
+            self.proc_dir = get_proc_dir(self.opts['cachedir'], uid=uid)
+            self.grains_cache = self.opts['grains']
+            self.ready = True
+
+    def setup_beacons(self, before_connect=False):
+        '''
+        Set up the beacons.
+        This is safe to call multiple times.
+        '''
+        self._setup_core()
+
+        loop_interval = self.opts['loop_interval']
+        new_periodic_callbacks = {}
+
+        if 'beacons' not in self.periodic_callbacks:
+            self.beacons = salt.beacons.Beacon(self.opts, self.functions)
+
+            def handle_beacons():
+                # Process Beacons
+                beacons = None
+                try:
+                    beacons = self.process_beacons(self.functions)
+                except Exception:
+                    log.critical('The beacon errored: ', exc_info=True)
+                if beacons and self.connected:
+                    self._fire_master(events=beacons)
+
+            new_periodic_callbacks['beacons'] = tornado.ioloop.PeriodicCallback(
+                    handle_beacons, loop_interval * 1000)
+            if before_connect:
+                # Make sure there is a chance for one iteration to occur before connect
+                handle_beacons()
+
+        if 'cleanup' not in self.periodic_callbacks:
+            new_periodic_callbacks['cleanup'] = tornado.ioloop.PeriodicCallback(
+                    self._fallback_cleanups, loop_interval * 1000)
+
+        # start all the other callbacks
+        for periodic_cb in six.itervalues(new_periodic_callbacks):
+            periodic_cb.start()
+
+        self.periodic_callbacks.update(new_periodic_callbacks)
+
+    def setup_scheduler(self, before_connect=False):
+        '''
+        Set up the scheduler.
+        This is safe to call multiple times.
+        '''
+        self._setup_core()
+
+        loop_interval = self.opts['loop_interval']
+        new_periodic_callbacks = {}
+
+        if 'schedule' not in self.periodic_callbacks:
+            if 'schedule' not in self.opts:
+                self.opts['schedule'] = {}
+            if not hasattr(self, 'schedule'):
+                self.schedule = salt.utils.schedule.Schedule(
+                    self.opts,
+                    self.functions,
+                    self.returners,
+                    utils=self.utils,
+                    cleanup=[master_event(type='alive')])
+
+            try:
+                if self.opts['grains_refresh_every']:  # In minutes, not seconds!
+                    log.debug(
+                        'Enabling the grains refresher. Will run every %d minute(s).',
+                        self.opts['grains_refresh_every']
+                    )
+                    self._refresh_grains_watcher(abs(self.opts['grains_refresh_every']))
+            except Exception as exc:
+                log.error(
+                    'Exception occurred in attempt to initialize grain refresh '
+                    'routine during minion tune-in: %s', exc
+                )
+
+            # TODO: actually listen to the return and change period
+            def handle_schedule():
+                self.process_schedule(self, loop_interval)
+            new_periodic_callbacks['schedule'] = tornado.ioloop.PeriodicCallback(handle_schedule, 1000)
+
+            if before_connect:
+                # Make sure there is a chance for one iteration to occur before connect
+                handle_schedule()
+
+        if 'cleanup' not in self.periodic_callbacks:
+            new_periodic_callbacks['cleanup'] = tornado.ioloop.PeriodicCallback(
+                    self._fallback_cleanups, loop_interval * 1000)
+
+        # start all the other callbacks
+        for periodic_cb in six.itervalues(new_periodic_callbacks):
+            periodic_cb.start()
+
+        self.periodic_callbacks.update(new_periodic_callbacks)
+
+    # Main Minion Tune In
+    def tune_in(self, start=True):
+        '''
+        Lock onto the publisher. This is the main event loop for the minion
+        :rtype : None
+        '''
+        self._pre_tune()
+
+        log.debug('Minion \'%s\' trying to tune in', self.opts['id'])
+
+        if start:
+            if self.opts.get('beacons_before_connect', False):
+                self.setup_beacons(before_connect=True)
+            if self.opts.get('scheduler_before_connect', False):
+                self.setup_scheduler(before_connect=True)
+            self.sync_connect_master()
+        if self.connected:
+            self._fire_master_minion_start()
+            log.info('Minion is ready to receive requests!')
+
+        # Make sure to gracefully handle SIGUSR1
+        enable_sigusr1_handler()
+
+        # Make sure to gracefully handle CTRL_LOGOFF_EVENT
+        if HAS_WIN_FUNCTIONS:
+            salt.utils.win_functions.enable_ctrl_logoff_handler()
+
+        # On first startup execute a state run if configured to do so
+        self._state_run()
+
+        self.setup_beacons()
+        self.setup_scheduler()
+
+        # schedule the stuff that runs every interval
+        ping_interval = self.opts.get('ping_interval', 0) * 60
+        if ping_interval > 0 and self.connected:
+            def ping_master():
+                try:
+                    def ping_timeout_handler(*_):
+                        if self.opts.get('auth_safemode', False):
+                            log.error('** Master Ping failed. Attempting to restart minion**')
+                            delay = self.opts.get('random_reauth_delay', 5)
+                            log.info('delaying random_reauth_delay %ss', delay)
+                            try:
+                                self.functions['service.restart'](service_name())
+                            except KeyError:
+                                # Probably no init system (running in docker?)
+                                log.warning(
+                                    'ping_interval reached without response '
+                                    'from the master, but service.restart '
+                                    'could not be run to restart the minion '
+                                    'daemon. ping_interval requires that the '
+                                    'minion is running under an init system.'
+                                )
+
+                    self._fire_master('ping', 'minion_ping', sync=False, timeout_handler=ping_timeout_handler)
+                except Exception:
+                    log.warning('Attempt to ping master failed.', exc_on_loglevel=logging.DEBUG)
+            self.periodic_callbacks['ping'] = tornado.ioloop.PeriodicCallback(ping_master, ping_interval * 1000)
+            self.periodic_callbacks['ping'].start()
+
+        # add handler to subscriber
+        if hasattr(self, 'pub_channel') and self.pub_channel is not None:
+            self.pub_channel.on_recv(self._handle_payload)
+        elif self.opts.get('master_type') != 'disable':
+            log.error('No connection to master found. Scheduled jobs will not run.')
+
+        if start:
+            try:
+                self.io_loop.start()
+                if self.restart:
+                    self.destroy()
+            except (KeyboardInterrupt, RuntimeError):  # A RuntimeError can be re-raised by Tornado on shutdown
+                self.destroy()
+
+    def _handle_payload(self, payload):
+        if payload is not None and payload['enc'] == 'aes':
+            if self._target_load(payload['load']):
+                self._handle_decoded_payload(payload['load'])
+            elif self.opts['zmq_filtering']:
+                # In the filtering enabled case, we'd like to know when minion sees something it shouldnt
+                log.trace(
+                    'Broadcast message received not for this minion, Load: %s',
+                    payload['load']
+                )
+        # If it's not AES, and thus has not been verified, we do nothing.
+        # In the future, we could add support for some clearfuncs, but
+        # the minion currently has no need.
+
+    def _target_load(self, load):
+        # Verify that the publication is valid
+        if 'tgt' not in load or 'jid' not in load or 'fun' not in load \
+           or 'arg' not in load:
+            return False
+        # Verify that the publication applies to this minion
+
+        # It's important to note that the master does some pre-processing
+        # to determine which minions to send a request to. So for example,
+        # a "salt -G 'grain_key:grain_val' test.ping" will invoke some
+        # pre-processing on the master and this minion should not see the
+        # publication if the master does not determine that it should.
+
+        if 'tgt_type' in load:
+            match_func = self.matchers.get('{0}_match.match'.format(load['tgt_type']), None)
+            if match_func is None:
+                return False
+            if load['tgt_type'] in ('grain', 'grain_pcre', 'pillar'):
+                delimiter = load.get('delimiter', DEFAULT_TARGET_DELIM)
+                if not match_func(load['tgt'], delimiter=delimiter):
+                    return False
+            elif not match_func(load['tgt']):
+                return False
+        else:
+            if not self.matchers['glob_match.match'](load['tgt']):
+                return False
+
+        return True
+
+    def destroy(self):
+        '''
+        Tear down the minion
+        '''
+        if self._running is False:
+            return
+
+        self._running = False
+        if hasattr(self, 'schedule'):
+            del self.schedule
+        if hasattr(self, 'pub_channel') and self.pub_channel is not None:
+            self.pub_channel.on_recv(None)
+            if hasattr(self.pub_channel, 'close'):
+                self.pub_channel.close()
+            del self.pub_channel
+        if hasattr(self, 'periodic_callbacks'):
+            for cb in six.itervalues(self.periodic_callbacks):
+                cb.stop()
+
+    def __del__(self):
+        self.destroy()
+
+
+class Syndic(Minion):
+    '''
+    Make a Syndic minion, this minion will use the minion keys on the
+    master to authenticate with a higher level master.
+    '''
+    def __init__(self, opts, **kwargs):
+        self._syndic_interface = opts.get('interface')
+        self._syndic = True
+        # force auth_safemode True because Syndic don't support autorestart
+        opts['auth_safemode'] = True
+        opts['loop_interval'] = 1
+        super(Syndic, self).__init__(opts, **kwargs)
+        self.mminion = salt.minion.MasterMinion(opts)
+        self.jid_forward_cache = set()
+        self.jids = {}
+        self.raw_events = []
+        self.pub_future = None
+
+    def _handle_decoded_payload(self, data):
+        '''
+        Override this method if you wish to handle the decoded data
+        differently.
+        '''
+        # TODO: even do this??
+        data['to'] = int(data.get('to', self.opts['timeout'])) - 1
+        # Only forward the command if it didn't originate from ourselves
+        if data.get('master_id', 0) != self.opts.get('master_id', 1):
+            self.syndic_cmd(data)
+
+    def syndic_cmd(self, data):
+        '''
+        Take the now clear load and forward it on to the client cmd
+        '''
+        # Set up default tgt_type
+        if 'tgt_type' not in data:
+            data['tgt_type'] = 'glob'
+        kwargs = {}
+
+        # optionally add a few fields to the publish data
+        for field in ('master_id',  # which master the job came from
+                      'user',  # which user ran the job
+                      ):
+            if field in data:
+                kwargs[field] = data[field]
+
+        def timeout_handler(*args):
+            log.warning('Unable to forward pub data: %s', args[1])
+            return True
+
+        with tornado.stack_context.ExceptionStackContext(timeout_handler):
+            self.local.pub_async(data['tgt'],
+                                 data['fun'],
+                                 data['arg'],
+                                 data['tgt_type'],
+                                 data['ret'],
+                                 data['jid'],
+                                 data['to'],
+                                 io_loop=self.io_loop,
+                                 callback=lambda _: None,
+                                 **kwargs)
+
+    def fire_master_syndic_start(self):
+        # Send an event to the master that the minion is live
+        if self.opts['enable_legacy_startup_events']:
+            # Old style event. Defaults to false in Sodium release.
+            self._fire_master(
+                'Syndic {0} started at {1}'.format(
+                    self.opts['id'],
+                    time.asctime()
+                ),
+                'syndic_start',
+                sync=False,
+            )
+        self._fire_master(
+            'Syndic {0} started at {1}'.format(
+                self.opts['id'],
+                time.asctime()
+            ),
+            tagify([self.opts['id'], 'start'], 'syndic'),
+            sync=False,
+        )
+
+    # TODO: clean up docs
+    def tune_in_no_block(self):
+        '''
+        Executes the tune_in sequence but omits extra logging and the
+        management of the event bus assuming that these are handled outside
+        the tune_in sequence
+        '''
+        # Instantiate the local client
+        self.local = salt.client.get_local_client(
+                self.opts['_minion_conf_file'], io_loop=self.io_loop)
+
+        # add handler to subscriber
+        self.pub_channel.on_recv(self._process_cmd_socket)
+
+    def _process_cmd_socket(self, payload):
+        if payload is not None and payload['enc'] == 'aes':
+            log.trace('Handling payload')
+            self._handle_decoded_payload(payload['load'])
+        # If it's not AES, and thus has not been verified, we do nothing.
+        # In the future, we could add support for some clearfuncs, but
+        # the syndic currently has no need.
+
+    @tornado.gen.coroutine
+    def reconnect(self):
+        if hasattr(self, 'pub_channel'):
+            self.pub_channel.on_recv(None)
+            if hasattr(self.pub_channel, 'close'):
+                self.pub_channel.close()
+            del self.pub_channel
+
+        # if eval_master finds a new master for us, self.connected
+        # will be True again on successful master authentication
+        master, self.pub_channel = yield self.eval_master(opts=self.opts)
+
+        if self.connected:
+            self.opts['master'] = master
+            self.pub_channel.on_recv(self._process_cmd_socket)
+            log.info('Minion is ready to receive requests!')
+
+        raise tornado.gen.Return(self)
+
+    def destroy(self):
+        '''
+        Tear down the syndic minion
+        '''
+        # We borrowed the local clients poller so give it back before
+        # it's destroyed. Reset the local poller reference.
+        super(Syndic, self).destroy()
+        if hasattr(self, 'local'):
+            del self.local
+
+        if hasattr(self, 'forward_events'):
+            self.forward_events.stop()
+
+
+# TODO: need a way of knowing if the syndic connection is busted
+class SyndicManager(MinionBase):
+    '''
+    Make a MultiMaster syndic minion, this minion will handle relaying jobs and returns from
+    all minions connected to it to the list of masters it is connected to.
+
+    Modes (controlled by `syndic_mode`:
+        sync: This mode will synchronize all events and publishes from higher level masters
+        cluster: This mode will only sync job publishes and returns
+
+    Note: jobs will be returned best-effort to the requesting master. This also means
+    (since we are using zmq) that if a job was fired and the master disconnects
+    between the publish and return, that the return will end up in a zmq buffer
+    in this Syndic headed to that original master.
+
+    In addition, since these classes all seem to use a mix of blocking and non-blocking
+    calls (with varying timeouts along the way) this daemon does not handle failure well,
+    it will (under most circumstances) stall the daemon for ~15s trying to forward events
+    to the down master
+    '''
+    # time to connect to upstream master
+    SYNDIC_CONNECT_TIMEOUT = 5
+    SYNDIC_EVENT_TIMEOUT = 5
+
+    def __init__(self, opts, io_loop=None):
+        opts['loop_interval'] = 1
+        super(SyndicManager, self).__init__(opts)
+        self.mminion = salt.minion.MasterMinion(opts)
+        # sync (old behavior), cluster (only returns and publishes)
+        self.syndic_mode = self.opts.get('syndic_mode', 'sync')
+        self.syndic_failover = self.opts.get('syndic_failover', 'random')
+
+        self.auth_wait = self.opts['acceptance_wait_time']
+        self.max_auth_wait = self.opts['acceptance_wait_time_max']
+
+        self._has_master = threading.Event()
+        self.jid_forward_cache = set()
+
+        if io_loop is None:
+            install_zmq()
+            self.io_loop = ZMQDefaultLoop.current()
+        else:
+            self.io_loop = io_loop
+
+        # List of events
+        self.raw_events = []
+        # Dict of rets: {master_id: {event_tag: job_ret, ...}, ...}
+        self.job_rets = {}
+        # List of delayed job_rets which was unable to send for some reason and will be resend to
+        # any available master
+        self.delayed = []
+        # Active pub futures: {master_id: (future, [job_ret, ...]), ...}
+        self.pub_futures = {}
+
+    def _spawn_syndics(self):
+        '''
+        Spawn all the coroutines which will sign in the syndics
+        '''
+        self._syndics = OrderedDict()  # mapping of opts['master'] -> syndic
+        masters = self.opts['master']
+        if not isinstance(masters, list):
+            masters = [masters]
+        for master in masters:
+            s_opts = copy.copy(self.opts)
+            s_opts['master'] = master
+            self._syndics[master] = self._connect_syndic(s_opts)
+
+    @tornado.gen.coroutine
+    def _connect_syndic(self, opts):
+        '''
+        Create a syndic, and asynchronously connect it to a master
+        '''
+        last = 0  # never have we signed in
+        auth_wait = opts['acceptance_wait_time']
+        failed = False
+        while True:
+            log.debug(
+                'Syndic attempting to connect to %s',
+                opts['master']
+            )
+            try:
+                syndic = Syndic(opts,
+                                timeout=self.SYNDIC_CONNECT_TIMEOUT,
+                                safe=False,
+                                io_loop=self.io_loop,
+                                )
+                yield syndic.connect_master(failed=failed)
+                # set up the syndic to handle publishes (specifically not event forwarding)
+                syndic.tune_in_no_block()
+
+                # Send an event to the master that the minion is live
+                syndic.fire_master_syndic_start()
+
+                log.info(
+                    'Syndic successfully connected to %s',
+                    opts['master']
+                )
+                break
+            except SaltClientError as exc:
+                failed = True
+                log.error(
+                    'Error while bringing up syndic for multi-syndic. Is the '
+                    'master at %s responding?', opts['master']
+                )
+                last = time.time()
+                if auth_wait < self.max_auth_wait:
+                    auth_wait += self.auth_wait
+                yield tornado.gen.sleep(auth_wait)  # TODO: log?
+            except (KeyboardInterrupt, SystemExit):
+                raise
+            except Exception:
+                failed = True
+                log.critical(
+                    'Unexpected error while connecting to %s',
+                    opts['master'], exc_info=True
+                )
+
+        raise tornado.gen.Return(syndic)
+
+    def _mark_master_dead(self, master):
+        '''
+        Mark a master as dead. This will start the sign-in routine
+        '''
+        # if its connected, mark it dead
+        if self._syndics[master].done():
+            syndic = self._syndics[master].result()  # pylint: disable=no-member
+            self._syndics[master] = syndic.reconnect()
+        else:
+            # TODO: debug?
+            log.info(
+                'Attempting to mark %s as dead, although it is already '
+                'marked dead', master
+            )
+
+    def _call_syndic(self, func, args=(), kwargs=None, master_id=None):
+        '''
+        Wrapper to call a given func on a syndic, best effort to get the one you asked for
+        '''
+        if kwargs is None:
+            kwargs = {}
+        successful = False
+        # Call for each master
+        for master, syndic_future in self.iter_master_options(master_id):
+            if not syndic_future.done() or syndic_future.exception():
+                log.error(
+                    'Unable to call %s on %s, that syndic is not connected',
+                    func, master
+                )
+                continue
+
+            try:
+                getattr(syndic_future.result(), func)(*args, **kwargs)
+                successful = True
+            except SaltClientError:
+                log.error(
+                    'Unable to call %s on %s, trying another...',
+                    func, master
+                )
+                self._mark_master_dead(master)
+        if not successful:
+            log.critical('Unable to call %s on any masters!', func)
+
+    def _return_pub_syndic(self, values, master_id=None):
+        '''
+        Wrapper to call the '_return_pub_multi' a syndic, best effort to get the one you asked for
+        '''
+        func = '_return_pub_multi'
+        for master, syndic_future in self.iter_master_options(master_id):
+            if not syndic_future.done() or syndic_future.exception():
+                log.error(
+                    'Unable to call %s on %s, that syndic is not connected',
+                    func, master
+                )
+                continue
+
+            future, data = self.pub_futures.get(master, (None, None))
+            if future is not None:
+                if not future.done():
+                    if master == master_id:
+                        # Targeted master previous send not done yet, call again later
+                        return False
+                    else:
+                        # Fallback master is busy, try the next one
+                        continue
+                elif future.exception():
+                    # Previous execution on this master returned an error
+                    log.error(
+                        'Unable to call %s on %s, trying another...',
+                        func, master
+                    )
+                    self._mark_master_dead(master)
+                    del self.pub_futures[master]
+                    # Add not sent data to the delayed list and try the next master
+                    self.delayed.extend(data)
+                    continue
+            future = getattr(syndic_future.result(), func)(values,
+                                                           '_syndic_return',
+                                                           timeout=self._return_retry_timer(),
+                                                           sync=False)
+            self.pub_futures[master] = (future, values)
+            return True
+        # Loop done and didn't exit: wasn't sent, try again later
+        return False
+
+    def iter_master_options(self, master_id=None):
+        '''
+        Iterate (in order) over your options for master
+        '''
+        masters = list(self._syndics.keys())
+        if self.opts['syndic_failover'] == 'random':
+            shuffle(masters)
+        if master_id not in self._syndics:
+            master_id = masters.pop(0)
+        else:
+            masters.remove(master_id)
+
+        while True:
+            yield master_id, self._syndics[master_id]
+            if not masters:
+                break
+            master_id = masters.pop(0)
+
+    def _reset_event_aggregation(self):
+        self.job_rets = {}
+        self.raw_events = []
+
+    def reconnect_event_bus(self, something):
+        future = self.local.event.set_event_handler(self._process_event)
+        self.io_loop.add_future(future, self.reconnect_event_bus)
+
+    # Syndic Tune In
+    def tune_in(self):
+        '''
+        Lock onto the publisher. This is the main event loop for the syndic
+        '''
+        self._spawn_syndics()
+        # Instantiate the local client
+        self.local = salt.client.get_local_client(
+            self.opts['_minion_conf_file'], io_loop=self.io_loop)
+        self.local.event.subscribe('')
+
+        log.debug('SyndicManager \'%s\' trying to tune in', self.opts['id'])
+
+        # register the event sub to the poller
+        self.job_rets = {}
+        self.raw_events = []
+        self._reset_event_aggregation()
+        future = self.local.event.set_event_handler(self._process_event)
+        self.io_loop.add_future(future, self.reconnect_event_bus)
+
+        # forward events every syndic_event_forward_timeout
+        self.forward_events = tornado.ioloop.PeriodicCallback(self._forward_events,
+                                                              self.opts['syndic_event_forward_timeout'] * 1000,
+                                                              )
+        self.forward_events.start()
+
+        # Make sure to gracefully handle SIGUSR1
+        enable_sigusr1_handler()
+
+        self.io_loop.start()
+
+    def _process_event(self, raw):
+        # TODO: cleanup: Move down into event class
+        mtag, data = self.local.event.unpack(raw, self.local.event.serial)
+        log.trace('Got event %s', mtag)  # pylint: disable=no-member
+
+        tag_parts = mtag.split('/')
+        if len(tag_parts) >= 4 and tag_parts[1] == 'job' and \
+            salt.utils.jid.is_jid(tag_parts[2]) and tag_parts[3] == 'ret' and \
+            'return' in data:
+            if 'jid' not in data:
+                # Not a job return
+                return
+            if self.syndic_mode == 'cluster' and data.get('master_id', 0) == self.opts.get('master_id', 1):
+                log.debug('Return received with matching master_id, not forwarding')
+                return
+
+            master = data.get('master_id')
+            jdict = self.job_rets.setdefault(master, {}).setdefault(mtag, {})
+            if not jdict:
+                jdict['__fun__'] = data.get('fun')
+                jdict['__jid__'] = data['jid']
+                jdict['__load__'] = {}
+                fstr = '{0}.get_load'.format(self.opts['master_job_cache'])
+                # Only need to forward each load once. Don't hit the disk
+                # for every minion return!
+                if data['jid'] not in self.jid_forward_cache:
+                    jdict['__load__'].update(
+                        self.mminion.returners[fstr](data['jid'])
+                        )
+                    self.jid_forward_cache.add(data['jid'])
+                    if len(self.jid_forward_cache) > self.opts['syndic_jid_forward_cache_hwm']:
+                        # Pop the oldest jid from the cache
+                        tmp = sorted(list(self.jid_forward_cache))
+                        tmp.pop(0)
+                        self.jid_forward_cache = set(tmp)
+            if master is not None:
+                # __'s to make sure it doesn't print out on the master cli
+                jdict['__master_id__'] = master
+            ret = {}
+            for key in 'return', 'retcode', 'success':
+                if key in data:
+                    ret[key] = data[key]
+            jdict[data['id']] = ret
+        else:
+            # TODO: config to forward these? If so we'll have to keep track of who
+            # has seen them
+            # if we are the top level masters-- don't forward all the minion events
+            if self.syndic_mode == 'sync':
+                # Add generic event aggregation here
+                if 'retcode' not in data:
+                    self.raw_events.append({'data': data, 'tag': mtag})
+
+    def _forward_events(self):
+        log.trace('Forwarding events')  # pylint: disable=no-member
+        if self.raw_events:
+            events = self.raw_events
+            self.raw_events = []
+            self._call_syndic('_fire_master',
+                              kwargs={'events': events,
+                                      'pretag': tagify(self.opts['id'], base='syndic'),
+                                      'timeout': self._return_retry_timer(),
+                                      'sync': False,
+                                      },
+                              )
+        if self.delayed:
+            res = self._return_pub_syndic(self.delayed)
+            if res:
+                self.delayed = []
+        for master in list(six.iterkeys(self.job_rets)):
+            values = list(six.itervalues(self.job_rets[master]))
+            res = self._return_pub_syndic(values, master_id=master)
+            if res:
+                del self.job_rets[master]
+
+
+class ProxyMinionManager(MinionManager):
+    '''
+    Create the multi-minion interface but for proxy minions
+    '''
+    def _create_minion_object(self, opts, timeout, safe,
+                              io_loop=None, loaded_base_name=None,
+                              jid_queue=None):
+        '''
+        Helper function to return the correct type of object
+        '''
+        return ProxyMinion(opts,
+                           timeout,
+                           safe,
+                           io_loop=io_loop,
+                           loaded_base_name=loaded_base_name,
+                           jid_queue=jid_queue)
+
+
+class ProxyMinion(Minion):
+    '''
+    This class instantiates a 'proxy' minion--a minion that does not manipulate
+    the host it runs on, but instead manipulates a device that cannot run a minion.
+    '''
+
+    # TODO: better name...
+    @tornado.gen.coroutine
+    def _post_master_init(self, master):
+        '''
+        Function to finish init after connecting to a master
+
+        This is primarily loading modules, pillars, etc. (since they need
+        to know which master they connected to)
+
+        If this function is changed, please check Minion._post_master_init
+        to see if those changes need to be propagated.
+
+        ProxyMinions need a significantly different post master setup,
+        which is why the differences are not factored out into separate helper
+        functions.
+        '''
+        log.debug("subclassed _post_master_init")
+
+        if self.connected:
+            self.opts['master'] = master
+
+            async_pillar = salt.pillar.get_async_pillar(
+                self.opts,
+                self.opts['grains'],
+                self.opts['id'],
+                saltenv=self.opts['saltenv'],
+                pillarenv=self.opts.get('pillarenv'),
+            )
+            self.opts['pillar'] = yield async_pillar.compile_pillar()
+            async_pillar.destroy()
+
+        if 'proxy' not in self.opts['pillar'] and 'proxy' not in self.opts:
+            errmsg = 'No proxy key found in pillar or opts for id ' + self.opts['id'] + '. ' + \
+                     'Check your pillar/opts configuration and contents.  Salt-proxy aborted.'
+            log.error(errmsg)
+            self._running = False
+            raise SaltSystemExit(code=-1, msg=errmsg)
+
+        if 'proxy' not in self.opts:
+            self.opts['proxy'] = self.opts['pillar']['proxy']
+
+        if self.opts.get('proxy_merge_pillar_in_opts'):
+            # Override proxy opts with pillar data when the user required.
+            self.opts = salt.utils.dictupdate.merge(self.opts,
+                                                    self.opts['pillar'],
+                                                    strategy=self.opts.get('proxy_merge_pillar_in_opts_strategy'),
+                                                    merge_lists=self.opts.get('proxy_deep_merge_pillar_in_opts', False))
+        elif self.opts.get('proxy_mines_pillar'):
+            # Even when not required, some details such as mine configuration
+            # should be merged anyway whenever possible.
+            if 'mine_interval' in self.opts['pillar']:
+                self.opts['mine_interval'] = self.opts['pillar']['mine_interval']
+            if 'mine_functions' in self.opts['pillar']:
+                general_proxy_mines = self.opts.get('mine_functions', [])
+                specific_proxy_mines = self.opts['pillar']['mine_functions']
+                try:
+                    self.opts['mine_functions'] = general_proxy_mines + specific_proxy_mines
+                except TypeError as terr:
+                    log.error('Unable to merge mine functions from the pillar in the opts, for proxy {}'.format(
+                        self.opts['id']))
+
+        fq_proxyname = self.opts['proxy']['proxytype']
+
+        # Need to load the modules so they get all the dunder variables
+        self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+
+        # we can then sync any proxymodules down from the master
+        # we do a sync_all here in case proxy code was installed by
+        # SPM or was manually placed in /srv/salt/_modules etc.
+        self.functions['saltutil.sync_all'](saltenv=self.opts['saltenv'])
+
+        # Pull in the utils
+        self.utils = salt.loader.utils(self.opts)
+
+        # Then load the proxy module
+        self.proxy = salt.loader.proxy(self.opts, utils=self.utils)
+
+        # And re-load the modules so the __proxy__ variable gets injected
+        self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+        self.functions.pack['__proxy__'] = self.proxy
+        self.proxy.pack['__salt__'] = self.functions
+        self.proxy.pack['__ret__'] = self.returners
+        self.proxy.pack['__pillar__'] = self.opts['pillar']
+
+        # Reload utils as well (chicken and egg, __utils__ needs __proxy__ and __proxy__ needs __utils__
+        self.utils = salt.loader.utils(self.opts, proxy=self.proxy)
+        self.proxy.pack['__utils__'] = self.utils
+
+        # Reload all modules so all dunder variables are injected
+        self.proxy.reload_modules()
+
+        # Start engines here instead of in the Minion superclass __init__
+        # This is because we need to inject the __proxy__ variable but
+        # it is not setup until now.
+        self.io_loop.spawn_callback(salt.engines.start_engines, self.opts,
+                                    self.process_manager, proxy=self.proxy)
+
+        if ('{0}.init'.format(fq_proxyname) not in self.proxy
+                or '{0}.shutdown'.format(fq_proxyname) not in self.proxy):
+            errmsg = 'Proxymodule {0} is missing an init() or a shutdown() or both. '.format(fq_proxyname) + \
+                     'Check your proxymodule.  Salt-proxy aborted.'
+            log.error(errmsg)
+            self._running = False
+            raise SaltSystemExit(code=-1, msg=errmsg)
+
+        self.module_executors = self.proxy.get('{0}.module_executors'.format(fq_proxyname), lambda: [])()
+        proxy_init_fn = self.proxy[fq_proxyname + '.init']
+        proxy_init_fn(self.opts)
+
+        self.opts['grains'] = salt.loader.grains(self.opts, proxy=self.proxy)
+
+        self.serial = salt.payload.Serial(self.opts)
+        self.mod_opts = self._prep_mod_opts()
+        self.matchers = salt.loader.matchers(self.opts)
+        self.beacons = salt.beacons.Beacon(self.opts, self.functions)
+        uid = salt.utils.user.get_uid(user=self.opts.get('user', None))
+        self.proc_dir = get_proc_dir(self.opts['cachedir'], uid=uid)
+
+        if self.connected and self.opts['pillar']:
+            # The pillar has changed due to the connection to the master.
+            # Reload the functions so that they can use the new pillar data.
+            self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
+            if hasattr(self, 'schedule'):
+                self.schedule.functions = self.functions
+                self.schedule.returners = self.returners
+
+        if not hasattr(self, 'schedule'):
+            self.schedule = salt.utils.schedule.Schedule(
+                self.opts,
+                self.functions,
+                self.returners,
+                cleanup=[master_event(type='alive')],
+                proxy=self.proxy)
+
+        # add default scheduling jobs to the minions scheduler
+        if self.opts['mine_enabled'] and 'mine.update' in self.functions:
+            self.schedule.add_job({
+                '__mine_interval':
+                    {
+                        'function': 'mine.update',
+                        'minutes': self.opts['mine_interval'],
+                        'jid_include': True,
+                        'maxrunning': 2,
+                        'run_on_start': True,
+                        'return_job': self.opts.get('mine_return_job', False)
+                    }
+            }, persist=True)
+            log.info('Added mine.update to scheduler')
+        else:
+            self.schedule.delete_job('__mine_interval', persist=True)
+
+        # add master_alive job if enabled
+        if (self.opts['transport'] != 'tcp' and
+                self.opts['master_alive_interval'] > 0):
+            self.schedule.add_job({
+                master_event(type='alive', master=self.opts['master']):
+                    {
+                        'function': 'status.master',
+                        'seconds': self.opts['master_alive_interval'],
+                        'jid_include': True,
+                        'maxrunning': 1,
+                        'return_job': False,
+                        'kwargs': {'master': self.opts['master'],
+                                    'connected': True}
+                    }
+            }, persist=True)
+            if self.opts['master_failback'] and \
+                    'master_list' in self.opts and \
+                    self.opts['master'] != self.opts['master_list'][0]:
+                self.schedule.add_job({
+                    master_event(type='failback'):
+                    {
+                        'function': 'status.ping_master',
+                        'seconds': self.opts['master_failback_interval'],
+                        'jid_include': True,
+                        'maxrunning': 1,
+                        'return_job': False,
+                        'kwargs': {'master': self.opts['master_list'][0]}
+                    }
+                }, persist=True)
+            else:
+                self.schedule.delete_job(master_event(type='failback'), persist=True)
+        else:
+            self.schedule.delete_job(master_event(type='alive', master=self.opts['master']), persist=True)
+            self.schedule.delete_job(master_event(type='failback'), persist=True)
+
+        # proxy keepalive
+        proxy_alive_fn = fq_proxyname+'.alive'
+        if (proxy_alive_fn in self.proxy
+            and 'status.proxy_reconnect' in self.functions
+            and self.opts.get('proxy_keep_alive', True)):
+            # if `proxy_keep_alive` is either not specified, either set to False does not retry reconnecting
+            self.schedule.add_job({
+                '__proxy_keepalive':
+                {
+                    'function': 'status.proxy_reconnect',
+                    'minutes': self.opts.get('proxy_keep_alive_interval', 1),  # by default, check once per minute
+                    'jid_include': True,
+                    'maxrunning': 1,
+                    'return_job': False,
+                    'kwargs': {
+                        'proxy_name': fq_proxyname
+                    }
+                }
+            }, persist=True)
+            self.schedule.enable_schedule()
+        else:
+            self.schedule.delete_job('__proxy_keepalive', persist=True)
+
+        #  Sync the grains here so the proxy can communicate them to the master
+        self.functions['saltutil.sync_grains'](saltenv='base')
+        self.grains_cache = self.opts['grains']
+        self.ready = True
+
+    @classmethod
+    def _target(cls, minion_instance, opts, data, connected):
+        if not minion_instance:
+            minion_instance = cls(opts)
+            minion_instance.connected = connected
+            if not hasattr(minion_instance, 'functions'):
+                # Need to load the modules so they get all the dunder variables
+                functions, returners, function_errors, executors = (
+                    minion_instance._load_modules(grains=opts['grains'])
+                    )
+                minion_instance.functions = functions
+                minion_instance.returners = returners
+                minion_instance.function_errors = function_errors
+                minion_instance.executors = executors
+
+                # Pull in the utils
+                minion_instance.utils = salt.loader.utils(minion_instance.opts)
+
+                # Then load the proxy module
+                minion_instance.proxy = salt.loader.proxy(minion_instance.opts, utils=minion_instance.utils)
+
+                # And re-load the modules so the __proxy__ variable gets injected
+                functions, returners, function_errors, executors = (
+                    minion_instance._load_modules(grains=opts['grains'])
+                    )
+                minion_instance.functions = functions
+                minion_instance.returners = returners
+                minion_instance.function_errors = function_errors
+                minion_instance.executors = executors
+
+                minion_instance.functions.pack['__proxy__'] = minion_instance.proxy
+                minion_instance.proxy.pack['__salt__'] = minion_instance.functions
+                minion_instance.proxy.pack['__ret__'] = minion_instance.returners
+                minion_instance.proxy.pack['__pillar__'] = minion_instance.opts['pillar']
+
+                # Reload utils as well (chicken and egg, __utils__ needs __proxy__ and __proxy__ needs __utils__
+                minion_instance.utils = salt.loader.utils(minion_instance.opts, proxy=minion_instance.proxy)
+                minion_instance.proxy.pack['__utils__'] = minion_instance.utils
+
+                # Reload all modules so all dunder variables are injected
+                minion_instance.proxy.reload_modules()
+
+                fq_proxyname = opts['proxy']['proxytype']
+
+                minion_instance.module_executors = minion_instance.proxy.get('{0}.module_executors'.format(fq_proxyname), lambda: [])()
+
+                proxy_init_fn = minion_instance.proxy[fq_proxyname + '.init']
+                proxy_init_fn(opts)
+            if not hasattr(minion_instance, 'serial'):
+                minion_instance.serial = salt.payload.Serial(opts)
+            if not hasattr(minion_instance, 'proc_dir'):
+                uid = salt.utils.user.get_uid(user=opts.get('user', None))
+                minion_instance.proc_dir = (
+                    get_proc_dir(opts['cachedir'], uid=uid)
+                    )
+
+        with tornado.stack_context.StackContext(minion_instance.ctx):
+            if isinstance(data['fun'], tuple) or isinstance(data['fun'], list):
+                Minion._thread_multi_return(minion_instance, opts, data)
+            else:
+                Minion._thread_return(minion_instance, opts, data)
+
+
+class SProxyMinion(SMinion):
+    '''
+    Create an object that has loaded all of the minion module functions,
+    grains, modules, returners etc.  The SProxyMinion allows developers to
+    generate all of the salt minion functions and present them with these
+    functions for general use.
+    '''
+    def gen_modules(self, initial_load=False):
+        '''
+        Tell the minion to reload the execution modules
+
+        CLI Example:
+
+        .. code-block:: bash
+
+            salt '*' sys.reload_modules
+        '''
+        self.opts['grains'] = salt.loader.grains(self.opts)
+        self.opts['pillar'] = salt.pillar.get_pillar(
+            self.opts,
+            self.opts['grains'],
+            self.opts['id'],
+            saltenv=self.opts['saltenv'],
+            pillarenv=self.opts.get('pillarenv'),
+        ).compile_pillar()
+
+        if 'proxy' not in self.opts['pillar'] and 'proxy' not in self.opts:
+            errmsg = (
+                'No "proxy" configuration key found in pillar or opts '
+                'dictionaries for id {id}. Check your pillar/options '
+                'configuration and contents. Salt-proxy aborted.'
+            ).format(id=self.opts['id'])
+            log.error(errmsg)
+            self._running = False
+            raise SaltSystemExit(code=salt.defaults.exitcodes.EX_GENERIC, msg=errmsg)
+
+        if 'proxy' not in self.opts:
+            self.opts['proxy'] = self.opts['pillar']['proxy']
+
+        # Then load the proxy module
+        self.proxy = salt.loader.proxy(self.opts)
+
+        self.utils = salt.loader.utils(self.opts, proxy=self.proxy)
+
+        self.functions = salt.loader.minion_mods(self.opts, utils=self.utils, notify=False, proxy=self.proxy)
+        self.returners = salt.loader.returners(self.opts, self.functions, proxy=self.proxy)
+        self.matchers = salt.loader.matchers(self.opts)
+        self.functions['sys.reload_modules'] = self.gen_modules
+        self.executors = salt.loader.executors(self.opts, self.functions, proxy=self.proxy)
+
+        fq_proxyname = self.opts['proxy']['proxytype']
+
+        # we can then sync any proxymodules down from the master
+        # we do a sync_all here in case proxy code was installed by
+        # SPM or was manually placed in /srv/salt/_modules etc.
+        self.functions['saltutil.sync_all'](saltenv=self.opts['saltenv'])
+
+        self.functions.pack['__proxy__'] = self.proxy
+        self.proxy.pack['__salt__'] = self.functions
+        self.proxy.pack['__ret__'] = self.returners
+        self.proxy.pack['__pillar__'] = self.opts['pillar']
+
+        # Reload utils as well (chicken and egg, __utils__ needs __proxy__ and __proxy__ needs __utils__
+        self.utils = salt.loader.utils(self.opts, proxy=self.proxy)
+        self.proxy.pack['__utils__'] = self.utils
+
+        # Reload all modules so all dunder variables are injected
+        self.proxy.reload_modules()
+
+        if ('{0}.init'.format(fq_proxyname) not in self.proxy
+                or '{0}.shutdown'.format(fq_proxyname) not in self.proxy):
+            errmsg = 'Proxymodule {0} is missing an init() or a shutdown() or both. '.format(fq_proxyname) + \
+                     'Check your proxymodule.  Salt-proxy aborted.'
+            log.error(errmsg)
+            self._running = False
+            raise SaltSystemExit(code=salt.defaults.exitcodes.EX_GENERIC, msg=errmsg)
+
+        self.module_executors = self.proxy.get('{0}.module_executors'.format(fq_proxyname), lambda: [])()
+        proxy_init_fn = self.proxy[fq_proxyname + '.init']
+        proxy_init_fn(self.opts)
+
+        self.opts['grains'] = salt.loader.grains(self.opts, proxy=self.proxy)
+
+        #  Sync the grains here so the proxy can communicate them to the master
+        self.functions['saltutil.sync_grains'](saltenv='base')
+        self.grains_cache = self.opts['grains']
+        self.ready = True
diff -Naur a/salt/netapi/rest_tornado/__init__.py c/salt/netapi/rest_tornado/__init__.py
--- a/salt/netapi/rest_tornado/__init__.py	2019-07-02 10:15:07.055874718 -0600
+++ c/salt/netapi/rest_tornado/__init__.py	2019-07-02 10:58:03.171938594 -0600
@@ -18,11 +18,20 @@
 min_tornado_version = '4.0'
 has_tornado = False
 try:
-    import tornado
-    if _StrictVersion(tornado.version) >= _StrictVersion(min_tornado_version):
+    try:
+        from tornado4 import version as tornado_version
+    except ImportError:
+        from tornado import version as tornado_version
+    if _StrictVersion(tornado_version) >= _StrictVersion(min_tornado_version):
         has_tornado = True
     else:
         log.error('rest_tornado requires at least tornado %s', min_tornado_version)
+    try:
+        from tornado4.ioloop import IOLoop
+        from tornado4.web import Application
+    except ImportError:
+        from tornado.ioloop import IOLoop
+        from tornado.web import Application
 except (ImportError, TypeError) as err:
     has_tornado = False
     log.error('ImportError! %s', err)
@@ -76,7 +85,7 @@
             (formatted_events_pattern, saltnado_websockets.FormattedEventsHandler),
         ]
 
-    application = tornado.web.Application(paths, debug=mod_opts.get('debug', False))
+    application = Application(paths, debug=mod_opts.get('debug', False))
 
     application.opts = opts
     application.mod_opts = mod_opts
@@ -115,8 +124,8 @@
             ssl_opts.update({'keyfile': mod_opts['ssl_key']})
         kwargs['ssl_options'] = ssl_opts
 
-    import tornado.httpserver
-    http_server = tornado.httpserver.HTTPServer(get_application(__opts__), **kwargs)
+    from tornado.httpserver import HTTPServer
+    http_server = HTTPServer(get_application(__opts__), **kwargs)
     try:
         http_server.bind(mod_opts['port'],
                          address=mod_opts.get('address'),
@@ -128,6 +137,6 @@
         raise SystemExit(1)
 
     try:
-        tornado.ioloop.IOLoop.current().start()
+        IOLoop.current().start()
     except KeyboardInterrupt:
         raise SystemExit(0)
diff -Naur a/salt/netapi/rest_tornado/saltnado.py c/salt/netapi/rest_tornado/saltnado.py
--- a/salt/netapi/rest_tornado/saltnado.py	2019-07-02 10:15:07.055874718 -0600
+++ c/salt/netapi/rest_tornado/saltnado.py	2019-07-02 10:58:03.171938594 -0600
@@ -195,12 +195,21 @@
 
 # pylint: disable=import-error
 import cgi
-import tornado.escape
-import tornado.httpserver
-import tornado.ioloop
-import tornado.web
-import tornado.gen
-from tornado.concurrent import Future
+
+try:
+    import tornado4.gen as tornado_gen
+    import tornado4.httpserver
+    from tornado4.escape import native_str
+    from tornado4.ioloop import IOLoop
+    from tornado4.web import RequestHandler, asynchronous
+    from tornado4.concurrent import Future
+except ImportError:
+    import tornado.gen as tornado_gen
+    import tornado.httpserver
+    from tornado.escape import native_str
+    from tornado.ioloop import IOLoop
+    from tornado.web import RequestHandler, asynchronous
+    from tornado.concurrent import Future
 # pylint: enable=import-error
 
 # salt imports
@@ -285,7 +294,7 @@
             opts['transport'],
             opts=opts,
             listen=True,
-            io_loop=tornado.ioloop.IOLoop.current()
+            io_loop = IOLoop.current()
         )
 
         # tag -> list of futures
@@ -310,7 +319,7 @@
             self._timeout_future(tag, matcher, future)
             # remove the timeout
             if future in self.timeout_map:
-                tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future])
+                IOLoop.current().remove_timeout(self.timeout_map[future])
                 del self.timeout_map[future]
 
         del self.request_map[request]
@@ -347,14 +356,14 @@
         future = Future()
         if callback is not None:
             def handle_future(future):
-                tornado.ioloop.IOLoop.current().add_callback(callback, future)
+                IOLoop.current().add_callback(callback, future)
             future.add_done_callback(handle_future)
         # add this tag and future to the callbacks
         self.tag_map[(tag, matcher)].append(future)
         self.request_map[request].append((tag, matcher, future))
 
         if timeout:
-            timeout_future = tornado.ioloop.IOLoop.current().call_later(timeout, self._timeout_future, tag, matcher, future)
+            timeout_future = IOLoop.current().call_later(timeout, self._timeout_future, tag, matcher, future)
             self.timeout_map[future] = timeout_future
 
         return future
@@ -394,11 +403,11 @@
                 future.set_result({'data': data, 'tag': mtag})
                 self.tag_map[(tag, matcher)].remove(future)
                 if future in self.timeout_map:
-                    tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future])
+                    IOLoop.current().remove_timeout(self.timeout_map[future])
                     del self.timeout_map[future]
 
 
-class BaseSaltAPIHandler(tornado.web.RequestHandler):  # pylint: disable=W0223
+class BaseSaltAPIHandler(RequestHandler):  # pylint: disable=W0223
     ct_out_map = (
         ('application/json', _json_dumps),
         ('application/x-yaml', salt.utils.yaml.safe_dump),
@@ -549,7 +558,7 @@
         try:
             # Use cgi.parse_header to correctly separate parameters from value
             value, parameters = cgi.parse_header(self.request.headers['Content-Type'])
-            return ct_in_map[value](tornado.escape.native_str(data))
+            return ct_in_map[value](native_str(data))
         except KeyError:
             self.send_error(406)
         except ValueError:
@@ -825,7 +834,7 @@
                "return": "Welcome"}
         self.write(self.serialize(ret))
 
-    @tornado.web.asynchronous
+    @asynchronous
     def post(self):
         '''
         Send one or more Salt commands (lowstates) in the request body
@@ -903,7 +912,7 @@
 
         self.disbatch()
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def disbatch(self):
         '''
         Disbatch all lowstates to the appropriate clients
@@ -939,7 +948,7 @@
         self.write(self.serialize({'return': ret}))
         self.finish()
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _disbatch_local(self, chunk):
         '''
         Dispatch local client commands
@@ -978,7 +987,7 @@
                     future.set_result(None)
                 except Exception:
                     pass
-            raise tornado.gen.Return('No minions matched the target. No command was sent, no jid was assigned.')
+            raise tornado_gen.Return('No minions matched the target. No command was sent, no jid was assigned.')
 
         # get_event for missing minion
         for minion in list(set(pub_data['minions']) - set(minions)):
@@ -996,18 +1005,18 @@
 
         # wait syndic a while to avoid missing published events
         if self.application.opts['order_masters']:
-            min_wait_time = tornado.gen.sleep(self.application.opts['syndic_wait'])
+            min_wait_time = tornado_gen.sleep(self.application.opts['syndic_wait'])
 
         # To ensure job_not_running and all_return are terminated by each other, communicate using a future
-        is_finished = tornado.gen.sleep(self.application.opts['gather_job_timeout'])
+        is_finished = tornado_gen.sleep(self.application.opts['gather_job_timeout'])
 
         # ping until the job is not running, while doing so, if we see new minions returning
         # that they are running the job, add them to the list
-        tornado.ioloop.IOLoop.current().spawn_callback(self.job_not_running, pub_data['jid'],
-                                                      chunk['tgt'],
-                                                      f_call['kwargs']['tgt_type'],
-                                                      minions,
-                                                      is_finished)
+        IOLoop.current().spawn_callback(self.job_not_running, pub_data['jid'],
+                                        chunk['tgt'],
+                                        f_call['kwargs']['tgt_type'],
+                                        minions,
+                                        is_finished)
 
         def more_todo():
             '''
@@ -1034,11 +1043,11 @@
                 # When finished entire routine, cleanup other futures and return result
                 if f is is_finished:
                     cancel_inflight_futures()
-                    raise tornado.gen.Return(chunk_ret)
+                    raise tornado_gen.Return(chunk_ret)
                 elif f is min_wait_time:
                     if not more_todo():
                         cancel_inflight_futures()
-                        raise tornado.gen.Return(chunk_ret)
+                        raise tornado_gen.Return(chunk_ret)
                     continue
 
                 f_result = f.result()
@@ -1056,12 +1065,12 @@
                     # if there are no more minions to wait for, then we are done
                     if not more_todo() and min_wait_time.done():
                         cancel_inflight_futures()
-                        raise tornado.gen.Return(chunk_ret)
+                        raise tornado_gen.Return(chunk_ret)
 
             except TimeoutException:
                 pass
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def job_not_running(self, jid, tgt, tgt_type, minions, is_finished):
         '''
         Return a future which will complete once jid (passed in) is no longer
@@ -1084,11 +1093,11 @@
                 if f is is_finished:
                     if not event.done():
                         event.set_result(None)
-                    raise tornado.gen.Return(True)
+                    raise tornado_gen.Return(True)
                 event = f.result()
             except TimeoutException:
                 if not minion_running:
-                    raise tornado.gen.Return(True)
+                    raise tornado_gen.Return(True)
                 else:
                     ping_pub_data = yield self.saltclients['local'](tgt,
                                                                     'saltutil.find_job',
@@ -1105,7 +1114,7 @@
                 minions[event['data']['id']] = False
             minion_running = True
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _disbatch_local_async(self, chunk):
         '''
         Disbatch local client_async commands
@@ -1114,9 +1123,9 @@
         # fire a job off
         pub_data = yield self.saltclients['local_async'](*f_call.get('args', ()), **f_call.get('kwargs', {}))
 
-        raise tornado.gen.Return(pub_data)
+        raise tornado_gen.Return(pub_data)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _disbatch_runner(self, chunk):
         '''
         Disbatch runner client commands
@@ -1129,26 +1138,26 @@
 
             # only return the return data
             ret = event if full_return else event['data']['return']
-            raise tornado.gen.Return(ret)
+            raise tornado_gen.Return(ret)
         except TimeoutException:
-            raise tornado.gen.Return('Timeout waiting for runner to execute')
+            raise tornado_gen.Return('Timeout waiting for runner to execute')
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _disbatch_runner_async(self, chunk):
         '''
         Disbatch runner client_async commands
         '''
         pub_data = self.saltclients['runner'](chunk)
-        raise tornado.gen.Return(pub_data)
+        raise tornado_gen.Return(pub_data)
 
     # salt.utils.args.format_call doesn't work for functions having the
-    # annotation tornado.gen.coroutine
+    # annotation tornado_gen.coroutine
     def _format_call_run_job_async(self, chunk):
         f_call = salt.utils.args.format_call(
             salt.client.LocalClient.run_job,
             chunk,
             is_class_method=True)
-        f_call.get('kwargs', {})['io_loop'] = tornado.ioloop.IOLoop.current()
+        f_call.get('kwargs', {})['io_loop'] = IOLoop.current()
         return f_call
 
 
@@ -1156,7 +1165,7 @@
     '''
     A convenience endpoint for minion related functions
     '''
-    @tornado.web.asynchronous
+    @asynchronous
     def get(self, mid=None):  # pylint: disable=W0221
         '''
         A convenience URL for getting lists of minions or getting minion
@@ -1208,7 +1217,7 @@
         }]
         self.disbatch()
 
-    @tornado.web.asynchronous
+    @asynchronous
     def post(self):
         '''
         Start an execution command and immediately return the job id
@@ -1285,7 +1294,7 @@
     '''
     A convenience endpoint for job cache data
     '''
-    @tornado.web.asynchronous
+    @asynchronous
     def get(self, jid=None):  # pylint: disable=W0221
         '''
         A convenience URL for getting lists of previously run jobs or getting
@@ -1391,7 +1400,7 @@
     '''
     Endpoint to run commands without normal session handling
     '''
-    @tornado.web.asynchronous
+    @asynchronous
     def post(self):
         '''
         Run commands bypassing the :ref:`normal session handling
@@ -1462,7 +1471,7 @@
 
     .. seealso:: :ref:`events`
     '''
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def get(self):
         r'''
         An HTTP stream of the Salt master event bus
diff -Naur a/salt/netapi/rest_tornado/saltnado_websockets.py c/salt/netapi/rest_tornado/saltnado_websockets.py
--- a/salt/netapi/rest_tornado/saltnado_websockets.py	2019-07-02 10:15:07.055874718 -0600
+++ c/salt/netapi/rest_tornado/saltnado_websockets.py	2019-07-02 10:58:03.175938594 -0600
@@ -291,11 +291,16 @@
 '''
 from __future__ import absolute_import, print_function, unicode_literals
 
-import tornado.websocket
+from tornado.websocket import WebSocketHandler
 from . import event_processor
 from .saltnado import _check_cors_origin
 
-import tornado.gen
+try:
+    from tornado4.websocket import WebSocketHandler
+    import tornado4.gen as tornado_gen
+except ImportError:
+    from tornado.websocket import WebSocketHandler
+    import tornado.gen as tornado_gen
 
 import salt.utils.json
 import salt.netapi
@@ -306,7 +311,7 @@
 log = logging.getLogger(__name__)
 
 
-class AllEventsHandler(tornado.websocket.WebSocketHandler):  # pylint: disable=W0223,W0232
+class AllEventsHandler(WebSocketHandler):  # pylint: disable=W0223,W0232
     '''
     Server side websocket handler.
     '''
@@ -334,7 +339,7 @@
         '''
         self.connected = False
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def on_message(self, message):
         """Listens for a "websocket client ready" message.
         Once that message is received an asynchronous job
@@ -387,7 +392,7 @@
 
 class FormattedEventsHandler(AllEventsHandler):  # pylint: disable=W0223,W0232
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def on_message(self, message):
         """Listens for a "websocket client ready" message.
         Once that message is received an asynchronous job
diff -Naur a/salt/pillar/__init__.py c/salt/pillar/__init__.py
--- a/salt/pillar/__init__.py	2019-07-02 10:15:07.059874718 -0600
+++ c/salt/pillar/__init__.py	2019-07-02 10:58:03.175938594 -0600
@@ -10,7 +10,10 @@
 import os
 import collections
 import logging
-import tornado.gen
+try:
+    import tornado4.gen as tornado_gen
+except ImportError:
+    import tornado.gen as tornado_gen
 import sys
 import traceback
 import inspect
@@ -158,7 +161,7 @@
                                      merge_lists=True)
         self._closing = False
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def compile_pillar(self):
         '''
         Return a future which will contain the pillar data from the master
@@ -188,7 +191,7 @@
             log.error(msg)
             # raise an exception! Pillar isn't empty, we can't sync it!
             raise SaltClientError(msg)
-        raise tornado.gen.Return(ret_pillar)
+        raise tornado_gen.Return(ret_pillar)
 
     def destroy(self):
         if self._closing:
@@ -1129,7 +1132,7 @@
 # TODO: actually migrate from Pillar to AsyncPillar to allow for futures in
 # ext_pillar etc.
 class AsyncPillar(Pillar):
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def compile_pillar(self, ext=True):
         ret = super(AsyncPillar, self).compile_pillar(ext=ext)
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
diff -Naur a/salt/pillar/__init__.py.orig c/salt/pillar/__init__.py.orig
--- a/salt/pillar/__init__.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/pillar/__init__.py.orig	2019-07-02 10:57:25.423937658 -0600
@@ -0,0 +1,1135 @@
+# -*- coding: utf-8 -*-
+'''
+Render the pillar data
+'''
+
+# Import python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import copy
+import fnmatch
+import os
+import collections
+import logging
+import tornado.gen
+import sys
+import traceback
+import inspect
+
+# Import salt libs
+import salt.loader
+import salt.fileclient
+import salt.minion
+import salt.transport.client
+import salt.utils.args
+import salt.utils.cache
+import salt.utils.crypt
+import salt.utils.data
+import salt.utils.dictupdate
+import salt.utils.url
+from salt.exceptions import SaltClientError
+from salt.template import compile_template
+from salt.utils.odict import OrderedDict
+from salt.version import __version__
+# Even though dictupdate is imported, invoking salt.utils.dictupdate.merge here
+# causes an UnboundLocalError. This should be investigated and fixed, but until
+# then, leave the import directly below this comment intact.
+from salt.utils.dictupdate import merge
+
+# Import 3rd-party libs
+from salt.ext import six
+
+log = logging.getLogger(__name__)
+
+
+def get_pillar(opts, grains, minion_id, saltenv=None, ext=None, funcs=None,
+               pillar_override=None, pillarenv=None, extra_minion_data=None):
+    '''
+    Return the correct pillar driver based on the file_client option
+    '''
+    file_client = opts['file_client']
+    if opts.get('master_type') == 'disable' and file_client == 'remote':
+        file_client = 'local'
+    ptype = {
+        'remote': RemotePillar,
+        'local': Pillar
+    }.get(file_client, Pillar)
+    # If local pillar and we're caching, run through the cache system first
+    log.debug('Determining pillar cache')
+    if opts['pillar_cache']:
+        log.info('Compiling pillar from cache')
+        log.debug('get_pillar using pillar cache with ext: %s', ext)
+        return PillarCache(opts, grains, minion_id, saltenv, ext=ext, functions=funcs,
+                pillar_override=pillar_override, pillarenv=pillarenv)
+    return ptype(opts, grains, minion_id, saltenv, ext, functions=funcs,
+                 pillar_override=pillar_override, pillarenv=pillarenv,
+                 extra_minion_data=extra_minion_data)
+
+
+# TODO: migrate everyone to this one!
+def get_async_pillar(opts, grains, minion_id, saltenv=None, ext=None, funcs=None,
+                     pillar_override=None, pillarenv=None,
+                     extra_minion_data=None):
+    '''
+    Return the correct pillar driver based on the file_client option
+    '''
+    file_client = opts['file_client']
+    if opts.get('master_type') == 'disable' and file_client == 'remote':
+        file_client = 'local'
+    ptype = {
+        'remote': AsyncRemotePillar,
+        'local': AsyncPillar,
+    }.get(file_client, AsyncPillar)
+    return ptype(opts, grains, minion_id, saltenv, ext, functions=funcs,
+                 pillar_override=pillar_override, pillarenv=pillarenv,
+                 extra_minion_data=extra_minion_data)
+
+
+class RemotePillarMixin(object):
+    '''
+    Common remote pillar functionality
+    '''
+    def get_ext_pillar_extra_minion_data(self, opts):
+        '''
+        Returns the extra data from the minion's opts dict (the config file).
+
+        This data will be passed to external pillar functions.
+        '''
+        def get_subconfig(opts_key):
+            '''
+            Returns a dict containing the opts key subtree, while maintaining
+            the opts structure
+            '''
+            ret_dict = aux_dict = {}
+            config_val = opts
+            subkeys = opts_key.split(':')
+            # Build an empty dict with the opts path
+            for subkey in subkeys[:-1]:
+                aux_dict[subkey] = {}
+                aux_dict = aux_dict[subkey]
+                if not config_val.get(subkey):
+                    # The subkey is not in the config
+                    return {}
+                config_val = config_val[subkey]
+            if subkeys[-1] not in config_val:
+                return {}
+            aux_dict[subkeys[-1]] = config_val[subkeys[-1]]
+            return ret_dict
+
+        extra_data = {}
+        if 'pass_to_ext_pillars' in opts:
+            if not isinstance(opts['pass_to_ext_pillars'], list):
+                log.exception('\'pass_to_ext_pillars\' config is malformed.')
+                raise SaltClientError('\'pass_to_ext_pillars\' config is '
+                                      'malformed.')
+            for key in opts['pass_to_ext_pillars']:
+                salt.utils.dictupdate.update(extra_data,
+                                             get_subconfig(key),
+                                             recursive_update=True,
+                                             merge_lists=True)
+        log.trace('ext_pillar_extra_data = %s', extra_data)
+        return extra_data
+
+
+class AsyncRemotePillar(RemotePillarMixin):
+    '''
+    Get the pillar from the master
+    '''
+    def __init__(self, opts, grains, minion_id, saltenv, ext=None, functions=None,
+                 pillar_override=None, pillarenv=None, extra_minion_data=None):
+        self.opts = opts
+        self.opts['saltenv'] = saltenv
+        self.ext = ext
+        self.grains = grains
+        self.minion_id = minion_id
+        self.channel = salt.transport.client.AsyncReqChannel.factory(opts)
+        if pillarenv is not None:
+            self.opts['pillarenv'] = pillarenv
+        self.pillar_override = pillar_override or {}
+        if not isinstance(self.pillar_override, dict):
+            self.pillar_override = {}
+            log.error('Pillar data must be a dictionary')
+        self.extra_minion_data = extra_minion_data or {}
+        if not isinstance(self.extra_minion_data, dict):
+            self.extra_minion_data = {}
+            log.error('Extra minion data must be a dictionary')
+        salt.utils.dictupdate.update(self.extra_minion_data,
+                                     self.get_ext_pillar_extra_minion_data(opts),
+                                     recursive_update=True,
+                                     merge_lists=True)
+        self._closing = False
+
+    @tornado.gen.coroutine
+    def compile_pillar(self):
+        '''
+        Return a future which will contain the pillar data from the master
+        '''
+        load = {'id': self.minion_id,
+                'grains': self.grains,
+                'saltenv': self.opts['saltenv'],
+                'pillarenv': self.opts['pillarenv'],
+                'pillar_override': self.pillar_override,
+                'extra_minion_data': self.extra_minion_data,
+                'ver': '2',
+                'cmd': '_pillar'}
+        if self.ext:
+            load['ext'] = self.ext
+        try:
+            ret_pillar = yield self.channel.crypted_transfer_decode_dictentry(
+                load,
+                dictkey='pillar',
+            )
+        except Exception:
+            log.exception('Exception getting pillar:')
+            raise SaltClientError('Exception getting pillar.')
+
+        if not isinstance(ret_pillar, dict):
+            msg = ('Got a bad pillar from master, type {0}, expecting dict: '
+                   '{1}').format(type(ret_pillar).__name__, ret_pillar)
+            log.error(msg)
+            # raise an exception! Pillar isn't empty, we can't sync it!
+            raise SaltClientError(msg)
+        raise tornado.gen.Return(ret_pillar)
+
+    def destroy(self):
+        if self._closing:
+            return
+
+        self._closing = True
+        self.channel.close()
+
+    def __del__(self):
+        self.destroy()
+
+
+class RemotePillar(RemotePillarMixin):
+    '''
+    Get the pillar from the master
+    '''
+    def __init__(self, opts, grains, minion_id, saltenv, ext=None, functions=None,
+                 pillar_override=None, pillarenv=None, extra_minion_data=None):
+        self.opts = opts
+        self.opts['saltenv'] = saltenv
+        self.ext = ext
+        self.grains = grains
+        self.minion_id = minion_id
+        self.channel = salt.transport.client.ReqChannel.factory(opts)
+        if pillarenv is not None:
+            self.opts['pillarenv'] = pillarenv
+        self.pillar_override = pillar_override or {}
+        if not isinstance(self.pillar_override, dict):
+            self.pillar_override = {}
+            log.error('Pillar data must be a dictionary')
+        self.extra_minion_data = extra_minion_data or {}
+        if not isinstance(self.extra_minion_data, dict):
+            self.extra_minion_data = {}
+            log.error('Extra minion data must be a dictionary')
+        salt.utils.dictupdate.update(self.extra_minion_data,
+                                     self.get_ext_pillar_extra_minion_data(opts),
+                                     recursive_update=True,
+                                     merge_lists=True)
+        self._closing = False
+
+    def compile_pillar(self):
+        '''
+        Return the pillar data from the master
+        '''
+        load = {'id': self.minion_id,
+                'grains': self.grains,
+                'saltenv': self.opts['saltenv'],
+                'pillarenv': self.opts['pillarenv'],
+                'pillar_override': self.pillar_override,
+                'extra_minion_data': self.extra_minion_data,
+                'ver': '2',
+                'cmd': '_pillar'}
+        if self.ext:
+            load['ext'] = self.ext
+        ret_pillar = self.channel.crypted_transfer_decode_dictentry(load,
+                                                                    dictkey='pillar',
+                                                                    )
+
+        if not isinstance(ret_pillar, dict):
+            log.error(
+                'Got a bad pillar from master, type %s, expecting dict: %s',
+                type(ret_pillar).__name__, ret_pillar
+            )
+            return {}
+        return ret_pillar
+
+    def destroy(self):
+        if hasattr(self, '_closing') and self._closing:
+            return
+
+        self._closing = True
+        self.channel.close()
+
+    def __del__(self):
+        self.destroy()
+
+
+class PillarCache(object):
+    '''
+    Return a cached pillar if it exists, otherwise cache it.
+
+    Pillar caches are structed in two diminensions: minion_id with a dict of
+    saltenvs. Each saltenv contains a pillar dict
+
+    Example data structure:
+
+    ```
+    {'minion_1':
+        {'base': {'pilar_key_1' 'pillar_val_1'}
+    }
+    '''
+    # TODO ABC?
+    def __init__(self, opts, grains, minion_id, saltenv, ext=None, functions=None,
+                 pillar_override=None, pillarenv=None, extra_minion_data=None):
+        # Yes, we need all of these because we need to route to the Pillar object
+        # if we have no cache. This is another refactor target.
+
+        # Go ahead and assign these because they may be needed later
+        self.opts = opts
+        self.grains = grains
+        self.minion_id = minion_id
+        self.ext = ext
+        self.functions = functions
+        self.pillar_override = pillar_override
+        self.pillarenv = pillarenv
+
+        if saltenv is None:
+            self.saltenv = 'base'
+        else:
+            self.saltenv = saltenv
+
+        # Determine caching backend
+        self.cache = salt.utils.cache.CacheFactory.factory(
+                self.opts['pillar_cache_backend'],
+                self.opts['pillar_cache_ttl'],
+                minion_cache_path=self._minion_cache_path(minion_id))
+
+    def _minion_cache_path(self, minion_id):
+        '''
+        Return the path to the cache file for the minion.
+
+        Used only for disk-based backends
+        '''
+        return os.path.join(self.opts['cachedir'], 'pillar_cache', minion_id)
+
+    def fetch_pillar(self):
+        '''
+        In the event of a cache miss, we need to incur the overhead of caching
+        a new pillar.
+        '''
+        log.debug('Pillar cache getting external pillar with ext: %s', self.ext)
+        fresh_pillar = Pillar(self.opts,
+                              self.grains,
+                              self.minion_id,
+                              self.saltenv,
+                              ext=self.ext,
+                              functions=self.functions,
+                              pillar_override=self.pillar_override,
+                              pillarenv=self.pillarenv)
+        return fresh_pillar.compile_pillar()
+
+    def compile_pillar(self, *args, **kwargs):  # Will likely just be pillar_dirs
+        log.debug('Scanning pillar cache for information about minion %s and pillarenv %s', self.minion_id, self.pillarenv)
+        log.debug('Scanning cache: %s', self.cache._dict)
+        # Check the cache!
+        if self.minion_id in self.cache:  # Keyed by minion_id
+            # TODO Compare grains, etc?
+            if self.pillarenv in self.cache[self.minion_id]:
+                # We have a cache hit! Send it back.
+                log.debug('Pillar cache hit for minion %s and pillarenv %s', self.minion_id, self.pillarenv)
+                return self.cache[self.minion_id][self.pillarenv]
+            else:
+                # We found the minion but not the env. Store it.
+                fresh_pillar = self.fetch_pillar()
+                self.cache[self.minion_id][self.pillarenv] = fresh_pillar
+                log.debug('Pillar cache miss for pillarenv %s for minion %s', self.pillarenv, self.minion_id)
+                return fresh_pillar
+        else:
+            # We haven't seen this minion yet in the cache. Store it.
+            fresh_pillar = self.fetch_pillar()
+            self.cache[self.minion_id] = {self.pillarenv: fresh_pillar}
+            log.debug('Pillar cache miss for minion %s', self.minion_id)
+            log.debug('Current pillar cache: %s', self.cache._dict)  # FIXME hack!
+            return fresh_pillar
+
+
+class Pillar(object):
+    '''
+    Read over the pillar top files and render the pillar data
+    '''
+    def __init__(self, opts, grains, minion_id, saltenv, ext=None, functions=None,
+                 pillar_override=None, pillarenv=None, extra_minion_data=None):
+        self.minion_id = minion_id
+        self.ext = ext
+        if pillarenv is None:
+            if opts.get('pillarenv_from_saltenv', False):
+                opts['pillarenv'] = saltenv
+        # use the local file client
+        self.opts = self.__gen_opts(opts, grains, saltenv=saltenv, pillarenv=pillarenv)
+        self.saltenv = saltenv
+        self.client = salt.fileclient.get_file_client(self.opts, True)
+        self.avail = self.__gather_avail()
+
+        if opts.get('file_client', '') == 'local':
+            opts['grains'] = grains
+
+        # if we didn't pass in functions, lets load them
+        if functions is None:
+            utils = salt.loader.utils(opts)
+            if opts.get('file_client', '') == 'local':
+                self.functions = salt.loader.minion_mods(opts, utils=utils)
+            else:
+                self.functions = salt.loader.minion_mods(self.opts, utils=utils)
+        else:
+            self.functions = functions
+
+        self.opts['minion_id'] = minion_id
+        self.matchers = salt.loader.matchers(self.opts)
+        self.rend = salt.loader.render(self.opts, self.functions)
+        ext_pillar_opts = copy.deepcopy(self.opts)
+        # Keep the incoming opts ID intact, ie, the master id
+        if 'id' in opts:
+            ext_pillar_opts['id'] = opts['id']
+        self.merge_strategy = 'smart'
+        if opts.get('pillar_source_merging_strategy'):
+            self.merge_strategy = opts['pillar_source_merging_strategy']
+
+        self.ext_pillars = salt.loader.pillars(ext_pillar_opts, self.functions)
+        self.ignored_pillars = {}
+        self.pillar_override = pillar_override or {}
+        if not isinstance(self.pillar_override, dict):
+            self.pillar_override = {}
+            log.error('Pillar data must be a dictionary')
+        self.extra_minion_data = extra_minion_data or {}
+        if not isinstance(self.extra_minion_data, dict):
+            self.extra_minion_data = {}
+            log.error('Extra minion data must be a dictionary')
+        self._closing = False
+
+    def __valid_on_demand_ext_pillar(self, opts):
+        '''
+        Check to see if the on demand external pillar is allowed
+        '''
+        if not isinstance(self.ext, dict):
+            log.error(
+                'On-demand pillar %s is not formatted as a dictionary',
+                self.ext
+            )
+            return False
+
+        on_demand = opts.get('on_demand_ext_pillar', [])
+        try:
+            invalid_on_demand = set([x for x in self.ext if x not in on_demand])
+        except TypeError:
+            # Prevent traceback when on_demand_ext_pillar option is malformed
+            log.error(
+                'The \'on_demand_ext_pillar\' configuration option is '
+                'malformed, it should be a list of ext_pillar module names'
+            )
+            return False
+
+        if invalid_on_demand:
+            log.error(
+                'The following ext_pillar modules are not allowed for '
+                'on-demand pillar data: %s. Valid on-demand ext_pillar '
+                'modules are: %s. The valid modules can be adjusted by '
+                'setting the \'on_demand_ext_pillar\' config option.',
+                ', '.join(sorted(invalid_on_demand)),
+                ', '.join(on_demand),
+            )
+            return False
+        return True
+
+    def __gather_avail(self):
+        '''
+        Gather the lists of available sls data from the master
+        '''
+        avail = {}
+        for saltenv in self._get_envs():
+            avail[saltenv] = self.client.list_states(saltenv)
+        return avail
+
+    def __gen_opts(self, opts_in, grains, saltenv=None, ext=None, pillarenv=None):
+        '''
+        The options need to be altered to conform to the file client
+        '''
+        opts = copy.deepcopy(opts_in)
+        opts['file_client'] = 'local'
+        if not grains:
+            opts['grains'] = {}
+        else:
+            opts['grains'] = grains
+        # Allow minion/CLI saltenv/pillarenv to take precedence over master
+        opts['saltenv'] = saltenv \
+            if saltenv is not None \
+            else opts.get('saltenv')
+        opts['pillarenv'] = pillarenv \
+            if pillarenv is not None \
+            else opts.get('pillarenv')
+        opts['id'] = self.minion_id
+        if opts['state_top'].startswith('salt://'):
+            opts['state_top'] = opts['state_top']
+        elif opts['state_top'].startswith('/'):
+            opts['state_top'] = salt.utils.url.create(opts['state_top'][1:])
+        else:
+            opts['state_top'] = salt.utils.url.create(opts['state_top'])
+        if self.ext and self.__valid_on_demand_ext_pillar(opts):
+            if 'ext_pillar' in opts:
+                opts['ext_pillar'].append(self.ext)
+            else:
+                opts['ext_pillar'] = [self.ext]
+        if '__env__' in opts['pillar_roots']:
+            env = opts.get('pillarenv') or opts.get('saltenv') or 'base'
+            if env not in opts['pillar_roots']:
+                log.debug("pillar environment '%s' maps to __env__ pillar_roots directory", env)
+                opts['pillar_roots'][env] = opts['pillar_roots'].pop('__env__')
+            else:
+                log.debug("pillar_roots __env__ ignored (environment '%s' found in pillar_roots)",
+                          env)
+                opts['pillar_roots'].pop('__env__')
+        return opts
+
+    def _get_envs(self):
+        '''
+        Pull the file server environments out of the master options
+        '''
+        envs = set(['base'])
+        if 'pillar_roots' in self.opts:
+            envs.update(list(self.opts['pillar_roots']))
+        return envs
+
+    def get_tops(self):
+        '''
+        Gather the top files
+        '''
+        tops = collections.defaultdict(list)
+        include = collections.defaultdict(list)
+        done = collections.defaultdict(list)
+        errors = []
+        # Gather initial top files
+        try:
+            saltenvs = set()
+            if self.opts['pillarenv']:
+                # If the specified pillarenv is not present in the available
+                # pillar environments, do not cache the pillar top file.
+                if self.opts['pillarenv'] not in self.opts['pillar_roots']:
+                    log.debug(
+                        'pillarenv \'%s\' not found in the configured pillar '
+                        'environments (%s)',
+                        self.opts['pillarenv'], ', '.join(self.opts['pillar_roots'])
+                    )
+                else:
+                    saltenvs.add(self.opts['pillarenv'])
+            else:
+                saltenvs = self._get_envs()
+                if self.opts.get('pillar_source_merging_strategy', None) == "none":
+                    saltenvs &= set([self.saltenv or 'base'])
+
+            for saltenv in saltenvs:
+                top = self.client.cache_file(self.opts['state_top'], saltenv)
+                if top:
+                    tops[saltenv].append(compile_template(
+                        top,
+                        self.rend,
+                        self.opts['renderer'],
+                        self.opts['renderer_blacklist'],
+                        self.opts['renderer_whitelist'],
+                        saltenv=saltenv,
+                        _pillar_rend=True,
+                    ))
+        except Exception as exc:
+            errors.append(
+                    ('Rendering Primary Top file failed, render error:\n{0}'
+                        .format(exc)))
+            log.exception('Pillar rendering failed for minion %s', self.minion_id)
+
+        # Search initial top files for includes
+        for saltenv, ctops in six.iteritems(tops):
+            for ctop in ctops:
+                if 'include' not in ctop:
+                    continue
+                for sls in ctop['include']:
+                    include[saltenv].append(sls)
+                ctop.pop('include')
+        # Go through the includes and pull out the extra tops and add them
+        while include:
+            pops = []
+            for saltenv, states in six.iteritems(include):
+                pops.append(saltenv)
+                if not states:
+                    continue
+                for sls in states:
+                    if sls in done[saltenv]:
+                        continue
+                    try:
+                        tops[saltenv].append(
+                                compile_template(
+                                    self.client.get_state(
+                                        sls,
+                                        saltenv
+                                        ).get('dest', False),
+                                    self.rend,
+                                    self.opts['renderer'],
+                                    self.opts['renderer_blacklist'],
+                                    self.opts['renderer_whitelist'],
+                                    saltenv=saltenv,
+                                    _pillar_rend=True,
+                                    )
+                                )
+                    except Exception as exc:
+                        errors.append(
+                                ('Rendering Top file {0} failed, render error'
+                                 ':\n{1}').format(sls, exc))
+                    done[saltenv].append(sls)
+            for saltenv in pops:
+                if saltenv in include:
+                    include.pop(saltenv)
+
+        return tops, errors
+
+    def merge_tops(self, tops):
+        '''
+        Cleanly merge the top files
+        '''
+        top = collections.defaultdict(OrderedDict)
+        orders = collections.defaultdict(OrderedDict)
+        for ctops in six.itervalues(tops):
+            for ctop in ctops:
+                for saltenv, targets in six.iteritems(ctop):
+                    if saltenv == 'include':
+                        continue
+                    for tgt in targets:
+                        matches = []
+                        states = OrderedDict()
+                        orders[saltenv][tgt] = 0
+                        ignore_missing = False
+                        for comp in ctop[saltenv][tgt]:
+                            if isinstance(comp, dict):
+                                if 'match' in comp:
+                                    matches.append(comp)
+                                if 'order' in comp:
+                                    order = comp['order']
+                                    if not isinstance(order, int):
+                                        try:
+                                            order = int(order)
+                                        except ValueError:
+                                            order = 0
+                                    orders[saltenv][tgt] = order
+                                if comp.get('ignore_missing', False):
+                                    ignore_missing = True
+                            if isinstance(comp, six.string_types):
+                                states[comp] = True
+                        if ignore_missing:
+                            if saltenv not in self.ignored_pillars:
+                                self.ignored_pillars[saltenv] = []
+                            self.ignored_pillars[saltenv].extend(states.keys())
+                        top[saltenv][tgt] = matches
+                        top[saltenv][tgt].extend(states)
+        return self.sort_top_targets(top, orders)
+
+    def sort_top_targets(self, top, orders):
+        '''
+        Returns the sorted high data from the merged top files
+        '''
+        sorted_top = collections.defaultdict(OrderedDict)
+        # pylint: disable=cell-var-from-loop
+        for saltenv, targets in six.iteritems(top):
+            sorted_targets = sorted(targets,
+                    key=lambda target: orders[saltenv][target])
+            for target in sorted_targets:
+                sorted_top[saltenv][target] = targets[target]
+        # pylint: enable=cell-var-from-loop
+        return sorted_top
+
+    def get_top(self):
+        '''
+        Returns the high data derived from the top file
+        '''
+        tops, errors = self.get_tops()
+        try:
+            merged_tops = self.merge_tops(tops)
+        except TypeError as err:
+            merged_tops = OrderedDict()
+            errors.append('Error encountered while rendering pillar top file.')
+        return merged_tops, errors
+
+    def top_matches(self, top, reload=False):
+        '''
+        Search through the top high data for matches and return the states
+        that this minion needs to execute.
+
+        Returns:
+        {'saltenv': ['state1', 'state2', ...]}
+
+        reload
+            Reload the matcher loader
+        '''
+        matches = {}
+        if reload:
+            self.matchers = salt.loader.matchers(self.opts)
+        for saltenv, body in six.iteritems(top):
+            if self.opts['pillarenv']:
+                if saltenv != self.opts['pillarenv']:
+                    continue
+            for match, data in six.iteritems(body):
+                if self.matchers['confirm_top.confirm_top'](
+                        match,
+                        data,
+                        self.opts.get('nodegroups', {}),
+                        ):
+                    if saltenv not in matches:
+                        matches[saltenv] = env_matches = []
+                    else:
+                        env_matches = matches[saltenv]
+                    for item in data:
+                        if isinstance(item, six.string_types) and item not in env_matches:
+                            env_matches.append(item)
+        return matches
+
+    def render_pstate(self, sls, saltenv, mods, defaults=None):
+        '''
+        Collect a single pillar sls file and render it
+        '''
+        if defaults is None:
+            defaults = {}
+        err = ''
+        errors = []
+        fn_ = self.client.get_state(sls, saltenv).get('dest', False)
+        if not fn_:
+            if sls in self.ignored_pillars.get(saltenv, []):
+                log.debug('Skipping ignored and missing SLS \'%s\' in '
+                          'environment \'%s\'', sls, saltenv)
+                return None, mods, errors
+            elif self.opts['pillar_roots'].get(saltenv):
+                msg = ('Specified SLS \'{0}\' in environment \'{1}\' is not'
+                       ' available on the salt master').format(sls, saltenv)
+                log.error(msg)
+                errors.append(msg)
+            else:
+                msg = ('Specified SLS \'{0}\' in environment \'{1}\' was not '
+                       'found. '.format(sls, saltenv))
+                if self.opts.get('__git_pillar', False) is True:
+                    msg += (
+                        'This is likely caused by a git_pillar top file '
+                        'containing an environment other than the one for the '
+                        'branch in which it resides. Each git_pillar '
+                        'branch/tag must have its own top file.'
+                    )
+                else:
+                    msg += (
+                        'This could be because SLS \'{0}\' is in an '
+                        'environment other than \'{1}\', but \'{1}\' is '
+                        'included in that environment\'s Pillar top file. It '
+                        'could also be due to environment \'{1}\' not being '
+                        'defined in \'pillar_roots\'.'.format(sls, saltenv)
+                    )
+                log.debug(msg)
+                # return state, mods, errors
+                return None, mods, errors
+        state = None
+        try:
+            state = compile_template(fn_,
+                                     self.rend,
+                                     self.opts['renderer'],
+                                     self.opts['renderer_blacklist'],
+                                     self.opts['renderer_whitelist'],
+                                     saltenv,
+                                     sls,
+                                     _pillar_rend=True,
+                                     **defaults)
+        except Exception as exc:
+            msg = 'Rendering SLS \'{0}\' failed, render error:\n{1}'.format(
+                sls, exc
+            )
+            log.critical(msg, exc_info=True)
+            if self.opts.get('pillar_safe_render_error', True):
+                errors.append(
+                    'Rendering SLS \'{0}\' failed. Please see master log for '
+                    'details.'.format(sls)
+                )
+            else:
+                errors.append(msg)
+        mods.add(sls)
+        nstate = None
+        if state:
+            if not isinstance(state, dict):
+                msg = 'SLS \'{0}\' does not render to a dictionary'.format(sls)
+                log.error(msg)
+                errors.append(msg)
+            else:
+                if 'include' in state:
+                    if not isinstance(state['include'], list):
+                        msg = ('Include Declaration in SLS \'{0}\' is not '
+                               'formed as a list'.format(sls))
+                        log.error(msg)
+                        errors.append(msg)
+                    else:
+                        # render included state(s)
+                        include_states = []
+                        for sub_sls in state.pop('include'):
+                            if isinstance(sub_sls, dict):
+                                sub_sls, v = next(six.iteritems(sub_sls))
+                                defaults = v.get('defaults', {})
+                                key = v.get('key', None)
+                            else:
+                                key = None
+                            try:
+                                matched_pstates = fnmatch.filter(
+                                    self.avail[saltenv],
+                                    sub_sls.lstrip('.').replace('/', '.'),
+                                )
+                            except KeyError:
+                                errors.extend(
+                                    ['No matching pillar environment for environment '
+                                     '\'{0}\' found'.format(saltenv)]
+                                )
+                                matched_pstates = [sub_sls]
+                            for m_sub_sls in matched_pstates:
+                                if m_sub_sls not in mods:
+                                    nstate, mods, err = self.render_pstate(
+                                            m_sub_sls,
+                                            saltenv,
+                                            mods,
+                                            defaults
+                                            )
+                                    if nstate:
+                                        if key:
+                                            # If key is x:y, convert it to {x: {y: nstate}}
+                                            for key_fragment in reversed(key.split(":")):
+                                                nstate = {
+                                                    key_fragment: nstate
+                                                }
+                                        if not self.opts.get('pillar_includes_override_sls', False):
+                                            include_states.append(nstate)
+                                        else:
+                                            state = merge(
+                                                state,
+                                                nstate,
+                                                self.merge_strategy,
+                                                self.opts.get('renderer', 'yaml'),
+                                                self.opts.get('pillar_merge_lists', False))
+                                    if err:
+                                        errors += err
+                        if not self.opts.get('pillar_includes_override_sls', False):
+                            # merge included state(s) with the current state
+                            # merged last to ensure that its values are
+                            # authoritative.
+                            include_states.append(state)
+                            state = None
+                            for s in include_states:
+                                if state is None:
+                                    state = s
+                                else:
+                                    state = merge(
+                                        state,
+                                        s,
+                                        self.merge_strategy,
+                                        self.opts.get('renderer', 'yaml'),
+                                        self.opts.get('pillar_merge_lists', False))
+        return state, mods, errors
+
+    def render_pillar(self, matches, errors=None):
+        '''
+        Extract the sls pillar files from the matches and render them into the
+        pillar
+        '''
+        pillar = copy.copy(self.pillar_override)
+        if errors is None:
+            errors = []
+        for saltenv, pstates in six.iteritems(matches):
+            pstatefiles = []
+            mods = set()
+            for sls_match in pstates:
+                matched_pstates = []
+                try:
+                    matched_pstates = fnmatch.filter(self.avail[saltenv], sls_match)
+                except KeyError:
+                    errors.extend(
+                        ['No matching pillar environment for environment '
+                         '\'{0}\' found'.format(saltenv)]
+                    )
+                if matched_pstates:
+                    pstatefiles.extend(matched_pstates)
+                else:
+                    pstatefiles.append(sls_match)
+
+            for sls in pstatefiles:
+                pstate, mods, err = self.render_pstate(sls, saltenv, mods)
+
+                if err:
+                    errors += err
+
+                if pstate is not None:
+                    if not isinstance(pstate, dict):
+                        log.error(
+                            'The rendered pillar sls file, \'%s\' state did '
+                            'not return the expected data format. This is '
+                            'a sign of a malformed pillar sls file. Returned '
+                            'errors: %s',
+                            sls,
+                            ', '.join(["'{0}'".format(e) for e in errors])
+                        )
+                        continue
+                    pillar = merge(
+                        pillar,
+                        pstate,
+                        self.merge_strategy,
+                        self.opts.get('renderer', 'yaml'),
+                        self.opts.get('pillar_merge_lists', False))
+
+        return pillar, errors
+
+    def _external_pillar_data(self, pillar, val, key):
+        '''
+        Builds actual pillar data structure and updates the ``pillar`` variable
+        '''
+        ext = None
+        args = salt.utils.args.get_function_argspec(self.ext_pillars[key]).args
+
+        if isinstance(val, dict):
+            if ('extra_minion_data' in args) and self.extra_minion_data:
+                ext = self.ext_pillars[key](
+                    self.minion_id, pillar,
+                    extra_minion_data=self.extra_minion_data, **val)
+            else:
+                ext = self.ext_pillars[key](self.minion_id, pillar, **val)
+        elif isinstance(val, list):
+            if ('extra_minion_data' in args) and self.extra_minion_data:
+                ext = self.ext_pillars[key](
+                    self.minion_id, pillar, *val,
+                    extra_minion_data=self.extra_minion_data)
+            else:
+                ext = self.ext_pillars[key](self.minion_id,
+                                            pillar,
+                                            *val)
+        else:
+            if ('extra_minion_data' in args) and self.extra_minion_data:
+                ext = self.ext_pillars[key](
+                    self.minion_id,
+                    pillar,
+                    val,
+                    extra_minion_data=self.extra_minion_data)
+            else:
+                ext = self.ext_pillars[key](self.minion_id,
+                                            pillar,
+                                            val)
+        return ext
+
+    def ext_pillar(self, pillar, errors=None):
+        '''
+        Render the external pillar data
+        '''
+        if errors is None:
+            errors = []
+        try:
+            # Make sure that on-demand git_pillar is fetched before we try to
+            # compile the pillar data. git_pillar will fetch a remote when
+            # the git ext_pillar() func is run, but only for masterless.
+            if self.ext and 'git' in self.ext \
+                    and self.opts.get('__role') != 'minion':
+                # Avoid circular import
+                import salt.utils.gitfs
+                import salt.pillar.git_pillar
+                git_pillar = salt.utils.gitfs.GitPillar(
+                    self.opts,
+                    self.ext['git'],
+                    per_remote_overrides=salt.pillar.git_pillar.PER_REMOTE_OVERRIDES,
+                    per_remote_only=salt.pillar.git_pillar.PER_REMOTE_ONLY,
+                    global_only=salt.pillar.git_pillar.GLOBAL_ONLY)
+                git_pillar.fetch_remotes()
+        except TypeError:
+            # Handle malformed ext_pillar
+            pass
+        if 'ext_pillar' not in self.opts:
+            return pillar, errors
+        if not isinstance(self.opts['ext_pillar'], list):
+            errors.append('The "ext_pillar" option is malformed')
+            log.critical(errors[-1])
+            return pillar, errors
+        ext = None
+        # Bring in CLI pillar data
+        if self.pillar_override:
+            pillar = merge(
+                pillar,
+                self.pillar_override,
+                self.merge_strategy,
+                self.opts.get('renderer', 'yaml'),
+                self.opts.get('pillar_merge_lists', False))
+
+        for run in self.opts['ext_pillar']:
+            if not isinstance(run, dict):
+                errors.append('The "ext_pillar" option is malformed')
+                log.critical(errors[-1])
+                return {}, errors
+            if next(six.iterkeys(run)) in self.opts.get('exclude_ext_pillar', []):
+                continue
+            for key, val in six.iteritems(run):
+                if key not in self.ext_pillars:
+                    log.critical(
+                        'Specified ext_pillar interface %s is unavailable',
+                        key
+                    )
+                    continue
+                try:
+                    ext = self._external_pillar_data(pillar,
+                                                     val,
+                                                     key)
+                except Exception as exc:
+                    errors.append(
+                        'Failed to load ext_pillar {0}: {1}'.format(
+                            key,
+                            exc.__str__(),
+                        )
+                    )
+                    log.error(
+                        'Exception caught loading ext_pillar \'%s\':\n%s',
+                        key, ''.join(traceback.format_tb(sys.exc_info()[2]))
+                    )
+            if ext:
+                pillar = merge(
+                    pillar,
+                    ext,
+                    self.merge_strategy,
+                    self.opts.get('renderer', 'yaml'),
+                    self.opts.get('pillar_merge_lists', False))
+                ext = None
+        return pillar, errors
+
+    def compile_pillar(self, ext=True):
+        '''
+        Render the pillar data and return
+        '''
+        top, top_errors = self.get_top()
+        if ext:
+            if self.opts.get('ext_pillar_first', False):
+                self.opts['pillar'], errors = self.ext_pillar(self.pillar_override)
+                self.rend = salt.loader.render(self.opts, self.functions)
+                matches = self.top_matches(top, reload=True)
+                pillar, errors = self.render_pillar(matches, errors=errors)
+                pillar = merge(
+                    self.opts['pillar'],
+                    pillar,
+                    self.merge_strategy,
+                    self.opts.get('renderer', 'yaml'),
+                    self.opts.get('pillar_merge_lists', False))
+            else:
+                matches = self.top_matches(top)
+                pillar, errors = self.render_pillar(matches)
+                pillar, errors = self.ext_pillar(pillar, errors=errors)
+        else:
+            matches = self.top_matches(top)
+            pillar, errors = self.render_pillar(matches)
+        errors.extend(top_errors)
+        if self.opts.get('pillar_opts', False):
+            mopts = dict(self.opts)
+            if 'grains' in mopts:
+                mopts.pop('grains')
+            mopts['saltversion'] = __version__
+            pillar['master'] = mopts
+        if 'pillar' in self.opts and self.opts.get('ssh_merge_pillar', False):
+            pillar = merge(
+                self.opts['pillar'],
+                pillar,
+                self.merge_strategy,
+                self.opts.get('renderer', 'yaml'),
+                self.opts.get('pillar_merge_lists', False))
+        if errors:
+            for error in errors:
+                log.critical('Pillar render error: %s', error)
+            pillar['_errors'] = errors
+
+        if self.pillar_override:
+            pillar = merge(
+                pillar,
+                self.pillar_override,
+                self.merge_strategy,
+                self.opts.get('renderer', 'yaml'),
+                self.opts.get('pillar_merge_lists', False))
+
+        decrypt_errors = self.decrypt_pillar(pillar)
+        if decrypt_errors:
+            pillar.setdefault('_errors', []).extend(decrypt_errors)
+
+        return pillar
+
+    def decrypt_pillar(self, pillar):
+        '''
+        Decrypt the specified pillar dictionary items, if configured to do so
+        '''
+        errors = []
+        if self.opts.get('decrypt_pillar'):
+            decrypt_pillar = self.opts['decrypt_pillar']
+            if not isinstance(decrypt_pillar, dict):
+                decrypt_pillar = \
+                    salt.utils.data.repack_dictlist(self.opts['decrypt_pillar'])
+            if not decrypt_pillar:
+                errors.append('decrypt_pillar config option is malformed')
+            for key, rend in six.iteritems(decrypt_pillar):
+                ptr = salt.utils.data.traverse_dict(
+                    pillar,
+                    key,
+                    default=None,
+                    delimiter=self.opts['decrypt_pillar_delimiter'])
+                if ptr is None:
+                    log.debug('Pillar key %s not present', key)
+                    continue
+                try:
+                    hash(ptr)
+                    immutable = True
+                except TypeError:
+                    immutable = False
+                try:
+                    ret = salt.utils.crypt.decrypt(
+                        ptr,
+                        rend or self.opts['decrypt_pillar_default'],
+                        renderers=self.rend,
+                        opts=self.opts,
+                        valid_rend=self.opts['decrypt_pillar_renderers'])
+                    if immutable:
+                        # Since the key pointed to an immutable type, we need
+                        # to replace it in the pillar dict. First we will find
+                        # the parent, and then we will replace the child key
+                        # with the return data from the renderer.
+                        parent, _, child = key.rpartition(
+                            self.opts['decrypt_pillar_delimiter'])
+                        if not parent:
+                            # key is a top-level key, so the pointer to the
+                            # parent is the pillar dict itself.
+                            ptr = pillar
+                        else:
+                            ptr = salt.utils.data.traverse_dict(
+                                pillar,
+                                parent,
+                                default=None,
+                                delimiter=self.opts['decrypt_pillar_delimiter'])
+                        if ptr is not None:
+                            ptr[child] = ret
+                except Exception as exc:
+                    msg = 'Failed to decrypt pillar key \'{0}\': {1}'.format(
+                        key, exc
+                    )
+                    errors.append(msg)
+                    log.error(msg, exc_info=True)
+        return errors
+
+    def destroy(self):
+        '''
+        This method exist in order to be API compatible with RemotePillar
+        '''
+        if self._closing:
+            return
+        self._closing = True
+
+    def __del__(self):
+        self.destroy()
+
+
+# TODO: actually migrate from Pillar to AsyncPillar to allow for futures in
+# ext_pillar etc.
+class AsyncPillar(Pillar):
+    @tornado.gen.coroutine
+    def compile_pillar(self, ext=True):
+        ret = super(AsyncPillar, self).compile_pillar(ext=ext)
+        raise tornado.gen.Return(ret)
diff -Naur a/salt/transport/client.py c/salt/transport/client.py
--- a/salt/transport/client.py	2019-07-02 10:15:07.051874718 -0600
+++ c/salt/transport/client.py	2019-07-02 10:58:03.175938594 -0600
@@ -79,9 +79,14 @@
 
     @classmethod
     def _config_resolver(cls, num_threads=10):
-        from tornado.netutil import Resolver
+        try:
+            from tornado4.netutil import Resolver
+            tornado_name = 'tornado4'
+        except ImportError:
+            from tornado.netutil import Resolver
+            tornado_name = 'tornado'
         Resolver.configure(
-                'tornado.netutil.ThreadedResolver',
+                tornado_name + '.netutil.ThreadedResolver',
                 num_threads=num_threads)
         cls._resolver_configured = True
 
diff -Naur a/salt/transport/ipc.py c/salt/transport/ipc.py
--- a/salt/transport/ipc.py	2019-07-02 10:15:07.051874718 -0600
+++ c/salt/transport/ipc.py	2019-07-02 11:18:51.083969537 -0600
@@ -14,13 +14,23 @@
 import msgpack
 
 # Import Tornado libs
-import tornado
-import tornado.gen
-import tornado.netutil
-import tornado.concurrent
-from tornado.locks import Lock
-from tornado.ioloop import IOLoop, TimeoutError as TornadoTimeoutError
-from tornado.iostream import IOStream, StreamClosedError
+try:
+    import tornado4
+    import tornado4.gen as tornado_gen
+    from tornado4.netutil import add_accept_handler, bind_unix_socket
+    from tornado4.concurrent import Future as TornadoFuture
+    from tornado4.locks import Semaphore
+    from tornado4.ioloop import IOLoop, TimeoutError as TornadoTimeoutError
+    from tornado4.iostream import IOStream, StreamClosedError
+except ImportError:
+    import tornado
+    import tornado.gen as tornado_gen
+    from tornado.netutil import add_accept_handler, bind_unix_socket
+    from tornado.concurrent import Future as TornadoFuture
+    from tornado.locks import Semaphore
+    from tornado.ioloop import IOLoop, TimeoutError as TornadoTimeoutError
+    from tornado.iostream import IOStream, StreamClosedError
+
 # Import Salt libs
 import salt.transport.client
 import salt.transport.frame
@@ -38,7 +48,7 @@
         future._future_with_timeout._done_callback(future)
 
 
-class FutureWithTimeout(tornado.concurrent.Future):
+class FutureWithTimeout(TornadoFuture):
     def __init__(self, io_loop, future, timeout):
         super(FutureWithTimeout, self).__init__()
         self.io_loop = io_loop
@@ -128,16 +138,16 @@
             # Based on default used in tornado.netutil.bind_sockets()
             self.sock.listen(128)
         else:
-            self.sock = tornado.netutil.bind_unix_socket(self.socket_path)
+            self.sock = bind_unix_socket(self.socket_path)
 
         with salt.utils.asynchronous.current_ioloop(self.io_loop):
-            tornado.netutil.add_accept_handler(
+            add_accept_handler(
                 self.sock,
                 self.handle_connection,
             )
         self._started = True
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def handle_stream(self, stream):
         '''
         Override this to handle the streams as they arrive
@@ -147,13 +157,13 @@
         See https://tornado.readthedocs.io/en/latest/iostream.html#tornado.iostream.IOStream
         for additional details.
         '''
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def _null(msg):
-            raise tornado.gen.Return(None)
+            raise tornado_gen.Return(None)
 
         def write_callback(stream, header):
             if header.get('mid'):
-                @tornado.gen.coroutine
+                @tornado_gen.coroutine
                 def return_message(msg):
                     pack = salt.transport.frame.frame_msg_ipc(
                         msg,
@@ -255,7 +265,7 @@
         to the server.
 
         '''
-        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        self.io_loop = io_loop or IOLoop.current()
         self.socket_path = socket_path
         self._closing = False
         self.stream = None
@@ -283,7 +293,7 @@
             if hasattr(self, '_connecting_future'):
                 # read previous future result to prevent the "unhandled future exception" error
                 self._connecting_future.exception()  # pylint: disable=E0203
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             self._connecting_future = future
             self._connect(timeout=timeout)
 
@@ -295,7 +305,7 @@
 
         return future
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _connect(self, timeout=None):
         '''
         Connect to a running IPCServer
@@ -336,7 +346,7 @@
                     self._connecting_future.set_exception(e)
                     break
 
-                yield tornado.gen.sleep(1)
+                yield tornado_gen.sleep(1)
 
     def __del__(self):
         try:
@@ -379,13 +389,13 @@
     IMPORTANT: The below example also assumes a running IOLoop process.
 
     # Import Tornado libs
-    import tornado.ioloop
+    from tornado.ioloop import IOLoop
 
     # Import Salt libs
     import salt.config
     import salt.transport.ipc
 
-    io_loop = tornado.ioloop.IOLoop.current()
+    io_loop = IOLoop.current()
 
     ipc_server_socket_path = '/var/run/ipc_server.ipc'
 
@@ -399,7 +409,7 @@
     '''
     # FIXME timeout unimplemented
     # FIXME tries unimplemented
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def send(self, msg, timeout=None, tries=None):
         '''
         Send a message to an IPC socket
@@ -426,7 +436,7 @@
     a console:
 
         # Import Tornado libs
-        import tornado.ioloop
+        from tornado.ioloop import IOLoop
 
         # Import Salt libs
         import salt.transport.ipc
@@ -434,7 +444,7 @@
 
         opts = salt.config.master_opts()
 
-        io_loop = tornado.ioloop.IOLoop.current()
+        io_loop = IOLoop.current()
         ipc_server_socket_path = '/var/run/ipc_server.ipc'
         ipc_server = salt.transport.ipc.IPCMessageServer(opts, io_loop=io_loop
                                                          stream_handler=print_to_console)
@@ -497,16 +507,16 @@
             # Based on default used in tornado.netutil.bind_sockets()
             self.sock.listen(128)
         else:
-            self.sock = tornado.netutil.bind_unix_socket(self.socket_path)
+            self.sock = bind_unix_socket(self.socket_path)
 
         with salt.utils.asynchronous.current_ioloop(self.io_loop):
-            tornado.netutil.add_accept_handler(
+            add_accept_handler(
                 self.sock,
                 self.handle_connection,
             )
         self._started = True
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _write(self, stream, pack):
         try:
             yield stream.write(pack)
@@ -588,7 +598,7 @@
     IMPORTANT: The below example also assumes the IOLoop is NOT running.
 
     # Import Tornado libs
-    import tornado.ioloop
+    from tornado.ioloop import IOLoop
 
     # Import Salt libs
     import salt.config
@@ -596,7 +606,7 @@
 
     # Create a new IO Loop.
     # We know that this new IO Loop is not currently running.
-    io_loop = tornado.ioloop.IOLoop()
+    io_loop = IOLoop()
 
     ipc_publisher_socket_path = '/var/run/ipc_publisher.ipc'
 
@@ -616,12 +626,12 @@
         self._saved_data = []
         self._read_in_progress = Lock()
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _read(self, timeout, callback=None):
         try:
             yield self._read_in_progress.acquire(timeout=0.00000001)
-        except tornado.gen.TimeoutError:
-            raise tornado.gen.Return(None)
+        except tornado_gen.TimeoutError:
+            raise tornado_gen.Return(None)
 
         log.debug('IPC Subscriber is starting reading')
         exc_to_raise = None
@@ -674,7 +684,7 @@
 
         if exc_to_raise is not None:
             raise exc_to_raise  # pylint: disable=E0702
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
     def read_sync(self, timeout=None):
         '''
@@ -690,7 +700,7 @@
             return self._saved_data.pop(0)
         return self.io_loop.run_sync(lambda: self._read(timeout))
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def read_async(self, callback):
         '''
         Asynchronously read messages and invoke a callback when they are ready.
@@ -702,10 +712,10 @@
                 yield self.connect(timeout=5)
             except StreamClosedError:
                 log.trace('Subscriber closed stream on IPC %s before connect', self.socket_path)
-                yield tornado.gen.sleep(1)
+                yield tornado_gen.sleep(1)
             except Exception as exc:
                 log.error('Exception occurred while Subscriber connecting: %s', exc)
-                yield tornado.gen.sleep(1)
+                yield tornado_gen.sleep(1)
         yield self._read(None, callback)
 
     def close(self):
diff -Naur a/salt/transport/ipc.py.orig c/salt/transport/ipc.py.orig
--- a/salt/transport/ipc.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/transport/ipc.py.orig	2019-07-02 10:57:25.415937658 -0600
@@ -0,0 +1,730 @@
+# -*- coding: utf-8 -*-
+'''
+IPC transport classes
+'''
+
+# Import Python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import errno
+import logging
+import socket
+import time
+
+# Import 3rd-party libs
+import msgpack
+
+# Import Tornado libs
+import tornado
+import tornado.gen
+import tornado.netutil
+import tornado.concurrent
+from tornado.locks import Lock
+from tornado.ioloop import IOLoop, TimeoutError as TornadoTimeoutError
+from tornado.iostream import IOStream, StreamClosedError
+# Import Salt libs
+import salt.transport.client
+import salt.transport.frame
+from salt.ext import six
+
+log = logging.getLogger(__name__)
+
+
+# 'tornado.concurrent.Future' doesn't support
+# remove_done_callback() which we would have called
+# in the timeout case. Due to this, we have this
+# callback function outside of FutureWithTimeout.
+def future_with_timeout_callback(future):
+    if future._future_with_timeout is not None:
+        future._future_with_timeout._done_callback(future)
+
+
+class FutureWithTimeout(tornado.concurrent.Future):
+    def __init__(self, io_loop, future, timeout):
+        super(FutureWithTimeout, self).__init__()
+        self.io_loop = io_loop
+        self._future = future
+        if timeout is not None:
+            if timeout < 0.1:
+                timeout = 0.1
+            self._timeout_handle = self.io_loop.add_timeout(
+                self.io_loop.time() + timeout, self._timeout_callback)
+        else:
+            self._timeout_handle = None
+
+        if hasattr(self._future, '_future_with_timeout'):
+            # Reusing a future that has previously been used.
+            # Due to this, no need to call add_done_callback()
+            # because we did that before.
+            self._future._future_with_timeout = self
+            if self._future.done():
+                future_with_timeout_callback(self._future)
+        else:
+            self._future._future_with_timeout = self
+            self._future.add_done_callback(future_with_timeout_callback)
+
+    def _timeout_callback(self):
+        self._timeout_handle = None
+        # 'tornado.concurrent.Future' doesn't support
+        # remove_done_callback(). So we set an attribute
+        # inside the future itself to track what happens
+        # when it completes.
+        self._future._future_with_timeout = None
+        self.set_exception(TornadoTimeoutError())
+
+    def _done_callback(self, future):
+        try:
+            if self._timeout_handle is not None:
+                self.io_loop.remove_timeout(self._timeout_handle)
+                self._timeout_handle = None
+
+            self.set_result(future.result())
+        except Exception as exc:
+            self.set_exception(exc)
+
+
+class IPCServer(object):
+    '''
+    A Tornado IPC server very similar to Tornado's TCPServer class
+    but using either UNIX domain sockets or TCP sockets
+    '''
+    def __init__(self, socket_path, io_loop=None, payload_handler=None):
+        '''
+        Create a new Tornado IPC server
+
+        :param str/int socket_path: Path on the filesystem for the
+                                    socket to bind to. This socket does
+                                    not need to exist prior to calling
+                                    this method, but parent directories
+                                    should.
+                                    It may also be of type 'int', in
+                                    which case it is used as the port
+                                    for a tcp localhost connection.
+        :param IOLoop io_loop: A Tornado ioloop to handle scheduling
+        :param func payload_handler: A function to customize handling of
+                                     incoming data.
+        '''
+        self.socket_path = socket_path
+        self._started = False
+        self.payload_handler = payload_handler
+
+        # Placeholders for attributes to be populated by method calls
+        self.sock = None
+        self.io_loop = io_loop or IOLoop.current()
+        self._closing = False
+
+    def start(self):
+        '''
+        Perform the work necessary to start up a Tornado IPC server
+
+        Blocks until socket is established
+        '''
+        # Start up the ioloop
+        log.trace('IPCServer: binding to socket: %s', self.socket_path)
+        if isinstance(self.socket_path, int):
+            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            self.sock.setblocking(0)
+            self.sock.bind(('127.0.0.1', self.socket_path))
+            # Based on default used in tornado.netutil.bind_sockets()
+            self.sock.listen(128)
+        else:
+            self.sock = tornado.netutil.bind_unix_socket(self.socket_path)
+
+        with salt.utils.asynchronous.current_ioloop(self.io_loop):
+            tornado.netutil.add_accept_handler(
+                self.sock,
+                self.handle_connection,
+            )
+        self._started = True
+
+    @tornado.gen.coroutine
+    def handle_stream(self, stream):
+        '''
+        Override this to handle the streams as they arrive
+
+        :param IOStream stream: An IOStream for processing
+
+        See https://tornado.readthedocs.io/en/latest/iostream.html#tornado.iostream.IOStream
+        for additional details.
+        '''
+        @tornado.gen.coroutine
+        def _null(msg):
+            raise tornado.gen.Return(None)
+
+        def write_callback(stream, header):
+            if header.get('mid'):
+                @tornado.gen.coroutine
+                def return_message(msg):
+                    pack = salt.transport.frame.frame_msg_ipc(
+                        msg,
+                        header={'mid': header['mid']},
+                        raw_body=True,
+                    )
+                    yield stream.write(pack)
+                return return_message
+            else:
+                return _null
+        # msgpack deprecated `encoding` starting with version 0.5.2
+        if msgpack.version >= (0, 5, 2):
+            # Under Py2 we still want raw to be set to True
+            msgpack_kwargs = {'raw': six.PY2}
+        else:
+            if six.PY2:
+                msgpack_kwargs = {'encoding': None}
+            else:
+                msgpack_kwargs = {'encoding': 'utf-8'}
+        unpacker = msgpack.Unpacker(**msgpack_kwargs)
+        while not stream.closed():
+            try:
+                wire_bytes = yield stream.read_bytes(4096, partial=True)
+                unpacker.feed(wire_bytes)
+                for framed_msg in unpacker:
+                    body = framed_msg['body']
+                    self.io_loop.spawn_callback(self.payload_handler, body, write_callback(stream, framed_msg['head']))
+            except StreamClosedError:
+                log.trace('Client disconnected from IPC %s', self.socket_path)
+                break
+            except socket.error as exc:
+                # On occasion an exception will occur with
+                # an error code of 0, it's a spurious exception.
+                if exc.errno == 0:
+                    log.trace('Exception occured with error number 0, '
+                              'spurious exception: %s', exc)
+                else:
+                    log.error('Exception occurred while '
+                              'handling stream: %s', exc)
+            except Exception as exc:
+                log.error('Exception occurred while '
+                          'handling stream: %s', exc)
+
+    def handle_connection(self, connection, address):
+        log.trace('IPCServer: Handling connection '
+                  'to address: %s', address)
+        try:
+            with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                stream = IOStream(
+                    connection,
+                )
+            self.io_loop.spawn_callback(self.handle_stream, stream)
+        except Exception as exc:
+            log.error('IPC streaming error: %s', exc)
+
+    def close(self):
+        '''
+        Routines to handle any cleanup before the instance shuts down.
+        Sockets and filehandles should be closed explicitly, to prevent
+        leaks.
+        '''
+        if self._closing:
+            return
+        self._closing = True
+        if hasattr(self.sock, 'close'):
+            self.sock.close()
+
+    def __del__(self):
+        try:
+            self.close()
+        except TypeError:
+            # This is raised when Python's GC has collected objects which
+            # would be needed when calling self.close()
+            pass
+
+
+class IPCClient(object):
+    '''
+    A Tornado IPC client very similar to Tornado's TCPClient class
+    but using either UNIX domain sockets or TCP sockets
+
+    This was written because Tornado does not have its own IPC
+    server/client implementation.
+
+    :param IOLoop io_loop: A Tornado ioloop to handle scheduling
+    :param str/int socket_path: A path on the filesystem where a socket
+                                belonging to a running IPCServer can be
+                                found.
+                                It may also be of type 'int', in which
+                                case it is used as the port for a tcp
+                                localhost connection.
+    '''
+    def __init__(self, socket_path, io_loop=None):
+        '''
+        Create a new IPC client
+
+        IPC clients cannot bind to ports, but must connect to
+        existing IPC servers. Clients can then send messages
+        to the server.
+
+        '''
+        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        self.socket_path = socket_path
+        self._closing = False
+        self.stream = None
+        # msgpack deprecated `encoding` starting with version 0.5.2
+        if msgpack.version >= (0, 5, 2):
+            # Under Py2 we still want raw to be set to True
+            msgpack_kwargs = {'raw': six.PY2}
+        else:
+            if six.PY2:
+                msgpack_kwargs = {'encoding': None}
+            else:
+                msgpack_kwargs = {'encoding': 'utf-8'}
+        self.unpacker = msgpack.Unpacker(**msgpack_kwargs)
+
+    def connected(self):
+        return self.stream is not None and not self.stream.closed()
+
+    def connect(self, callback=None, timeout=None):
+        '''
+        Connect to the IPC socket
+        '''
+        if hasattr(self, '_connecting_future') and not self._connecting_future.done():  # pylint: disable=E0203
+            future = self._connecting_future  # pylint: disable=E0203
+        else:
+            if hasattr(self, '_connecting_future'):
+                # read previous future result to prevent the "unhandled future exception" error
+                self._connecting_future.exception()  # pylint: disable=E0203
+            future = tornado.concurrent.Future()
+            self._connecting_future = future
+            self._connect(timeout=timeout)
+
+        if callback is not None:
+            def handle_future(future):
+                response = future.result()
+                self.io_loop.add_callback(callback, response)
+            future.add_done_callback(handle_future)
+
+        return future
+
+    @tornado.gen.coroutine
+    def _connect(self, timeout=None):
+        '''
+        Connect to a running IPCServer
+        '''
+        if isinstance(self.socket_path, int):
+            sock_type = socket.AF_INET
+            sock_addr = ('127.0.0.1', self.socket_path)
+        else:
+            sock_type = socket.AF_UNIX
+            sock_addr = self.socket_path
+
+        self.stream = None
+        if timeout is not None:
+            timeout_at = time.time() + timeout
+
+        while True:
+            if self._closing:
+                break
+
+            if self.stream is None:
+                with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                    self.stream = IOStream(
+                        socket.socket(sock_type, socket.SOCK_STREAM)
+                    )
+            try:
+                log.trace('IPCClient: Connecting to socket: %s', self.socket_path)
+                yield self.stream.connect(sock_addr)
+                self._connecting_future.set_result(True)
+                break
+            except Exception as e:
+                if self.stream.closed():
+                    self.stream = None
+
+                if timeout is None or time.time() > timeout_at:
+                    if self.stream is not None:
+                        self.stream.close()
+                        self.stream = None
+                    self._connecting_future.set_exception(e)
+                    break
+
+                yield tornado.gen.sleep(1)
+
+    def __del__(self):
+        try:
+            self.close()
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+        except TypeError:
+            # This is raised when Python's GC has collected objects which
+            # would be needed when calling self.close()
+            pass
+
+    def close(self):
+        '''
+        Routines to handle any cleanup before the instance shuts down.
+        Sockets and filehandles should be closed explicitly, to prevent
+        leaks.
+        '''
+        if self._closing:
+            return
+
+        self._closing = True
+
+        log.debug('Closing %s instance', self.__class__.__name__)
+
+        if self.stream is not None and not self.stream.closed():
+            self.stream.close()
+
+
+class IPCMessageClient(IPCClient):
+    '''
+    Salt IPC message client
+
+    Create an IPC client to send messages to an IPC server
+
+    An example of a very simple IPCMessageClient connecting to an IPCServer. This
+    example assumes an already running IPCMessage server.
+
+    IMPORTANT: The below example also assumes a running IOLoop process.
+
+    # Import Tornado libs
+    import tornado.ioloop
+
+    # Import Salt libs
+    import salt.config
+    import salt.transport.ipc
+
+    io_loop = tornado.ioloop.IOLoop.current()
+
+    ipc_server_socket_path = '/var/run/ipc_server.ipc'
+
+    ipc_client = salt.transport.ipc.IPCMessageClient(ipc_server_socket_path, io_loop=io_loop)
+
+    # Connect to the server
+    ipc_client.connect()
+
+    # Send some data
+    ipc_client.send('Hello world')
+    '''
+    # FIXME timeout unimplemented
+    # FIXME tries unimplemented
+    @tornado.gen.coroutine
+    def send(self, msg, timeout=None, tries=None):
+        '''
+        Send a message to an IPC socket
+
+        If the socket is not currently connected, a connection will be established.
+
+        :param dict msg: The message to be sent
+        :param int timeout: Timeout when sending message (Currently unimplemented)
+        '''
+        if not self.connected():
+            yield self.connect()
+        pack = salt.transport.frame.frame_msg_ipc(msg, raw_body=True)
+        yield self.stream.write(pack)
+
+
+class IPCMessageServer(IPCServer):
+    '''
+    Salt IPC message server
+
+    Creates a message server which can create and bind to a socket on a given
+    path and then respond to messages asynchronously.
+
+    An example of a very simple IPCServer which prints received messages to
+    a console:
+
+        # Import Tornado libs
+        import tornado.ioloop
+
+        # Import Salt libs
+        import salt.transport.ipc
+        import salt.config
+
+        opts = salt.config.master_opts()
+
+        io_loop = tornado.ioloop.IOLoop.current()
+        ipc_server_socket_path = '/var/run/ipc_server.ipc'
+        ipc_server = salt.transport.ipc.IPCMessageServer(opts, io_loop=io_loop
+                                                         stream_handler=print_to_console)
+        # Bind to the socket and prepare to run
+        ipc_server.start(ipc_server_socket_path)
+
+        # Start the server
+        io_loop.start()
+
+        # This callback is run whenever a message is received
+        def print_to_console(payload):
+            print(payload)
+
+    See IPCMessageClient() for an example of sending messages to an IPCMessageServer instance
+    '''
+
+
+class IPCMessagePublisher(object):
+    '''
+    A Tornado IPC Publisher similar to Tornado's TCPServer class
+    but using either UNIX domain sockets or TCP sockets
+    '''
+    def __init__(self, opts, socket_path, io_loop=None):
+        '''
+        Create a new Tornado IPC server
+        :param dict opts: Salt options
+        :param str/int socket_path: Path on the filesystem for the
+                                    socket to bind to. This socket does
+                                    not need to exist prior to calling
+                                    this method, but parent directories
+                                    should.
+                                    It may also be of type 'int', in
+                                    which case it is used as the port
+                                    for a tcp localhost connection.
+        :param IOLoop io_loop: A Tornado ioloop to handle scheduling
+        '''
+        self.opts = opts
+        self.socket_path = socket_path
+        self._started = False
+
+        # Placeholders for attributes to be populated by method calls
+        self.sock = None
+        self.io_loop = io_loop or IOLoop.current()
+        self._closing = False
+        self.streams = set()
+
+    def start(self):
+        '''
+        Perform the work necessary to start up a Tornado IPC server
+
+        Blocks until socket is established
+        '''
+        # Start up the ioloop
+        log.trace('IPCMessagePublisher: binding to socket: %s', self.socket_path)
+        if isinstance(self.socket_path, int):
+            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            self.sock.setblocking(0)
+            self.sock.bind(('127.0.0.1', self.socket_path))
+            # Based on default used in tornado.netutil.bind_sockets()
+            self.sock.listen(128)
+        else:
+            self.sock = tornado.netutil.bind_unix_socket(self.socket_path)
+
+        with salt.utils.asynchronous.current_ioloop(self.io_loop):
+            tornado.netutil.add_accept_handler(
+                self.sock,
+                self.handle_connection,
+            )
+        self._started = True
+
+    @tornado.gen.coroutine
+    def _write(self, stream, pack):
+        try:
+            yield stream.write(pack)
+        except StreamClosedError:
+            log.trace('Client disconnected from IPC %s', self.socket_path)
+            self.streams.discard(stream)
+        except Exception as exc:
+            log.error('Exception occurred while handling stream: %s', exc)
+            if not stream.closed():
+                stream.close()
+            self.streams.discard(stream)
+
+    def publish(self, msg):
+        '''
+        Send message to all connected sockets
+        '''
+        if not len(self.streams):
+            return
+
+        pack = salt.transport.frame.frame_msg_ipc(msg, raw_body=True)
+
+        for stream in self.streams:
+            self.io_loop.spawn_callback(self._write, stream, pack)
+
+    def handle_connection(self, connection, address):
+        log.trace('IPCServer: Handling connection to address: %s', address)
+        try:
+            kwargs = {}
+            if self.opts['ipc_write_buffer'] > 0:
+                kwargs['max_write_buffer_size'] = self.opts['ipc_write_buffer']
+                log.trace('Setting IPC connection write buffer: %s', (self.opts['ipc_write_buffer']))
+            with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                stream = IOStream(
+                    connection,
+                    **kwargs
+                )
+            self.streams.add(stream)
+
+            def discard_after_closed():
+                self.streams.discard(stream)
+
+            stream.set_close_callback(discard_after_closed)
+        except Exception as exc:
+            log.error('IPC streaming error: %s', exc)
+
+    def close(self):
+        '''
+        Routines to handle any cleanup before the instance shuts down.
+        Sockets and filehandles should be closed explicitly, to prevent
+        leaks.
+        '''
+        if self._closing:
+            return
+        self._closing = True
+        for stream in self.streams:
+            stream.close()
+        self.streams.clear()
+        if hasattr(self.sock, 'close'):
+            self.sock.close()
+
+    def __del__(self):
+        try:
+            self.close()
+        except TypeError:
+            # This is raised when Python's GC has collected objects which
+            # would be needed when calling self.close()
+            pass
+
+
+class IPCMessageSubscriber(IPCClient):
+    '''
+    Salt IPC message subscriber
+
+    Create an IPC client to receive messages from IPC publisher
+
+    An example of a very simple IPCMessageSubscriber connecting to an IPCMessagePublisher.
+    This example assumes an already running IPCMessagePublisher.
+
+    IMPORTANT: The below example also assumes the IOLoop is NOT running.
+
+    # Import Tornado libs
+    import tornado.ioloop
+
+    # Import Salt libs
+    import salt.config
+    import salt.transport.ipc
+
+    # Create a new IO Loop.
+    # We know that this new IO Loop is not currently running.
+    io_loop = tornado.ioloop.IOLoop()
+
+    ipc_publisher_socket_path = '/var/run/ipc_publisher.ipc'
+
+    ipc_subscriber = salt.transport.ipc.IPCMessageSubscriber(ipc_server_socket_path, io_loop=io_loop)
+
+    # Connect to the server
+    # Use the associated IO Loop that isn't running.
+    io_loop.run_sync(ipc_subscriber.connect)
+
+    # Wait for some data
+    package = ipc_subscriber.read_sync()
+    '''
+    def __init__(self, socket_path, io_loop=None):
+        super(IPCMessageSubscriber, self).__init__(
+            socket_path, io_loop=io_loop)
+        self._read_stream_future = None
+        self._saved_data = []
+        self._read_in_progress = Lock()
+
+    @tornado.gen.coroutine
+    def _read(self, timeout, callback=None):
+        try:
+            yield self._read_in_progress.acquire(timeout=0.00000001)
+        except tornado.gen.TimeoutError:
+            raise tornado.gen.Return(None)
+
+        log.debug('IPC Subscriber is starting reading')
+        exc_to_raise = None
+        ret = None
+        try:
+            while True:
+                if self._read_stream_future is None:
+                    self._read_stream_future = self.stream.read_bytes(4096, partial=True)
+
+                if timeout is None:
+                    wire_bytes = yield self._read_stream_future
+                else:
+                    wire_bytes = yield FutureWithTimeout(self.io_loop,
+                                                         self._read_stream_future,
+                                                         timeout)
+                self._read_stream_future = None
+
+                # Remove the timeout once we get some data or an exception
+                # occurs. We will assume that the rest of the data is already
+                # there or is coming soon if an exception doesn't occur.
+                timeout = None
+
+                self.unpacker.feed(wire_bytes)
+                first_sync_msg = True
+                for framed_msg in self.unpacker:
+                    if callback:
+                        self.io_loop.spawn_callback(callback, framed_msg['body'])
+                    elif first_sync_msg:
+                        ret = framed_msg['body']
+                        first_sync_msg = False
+                    else:
+                        self._saved_data.append(framed_msg['body'])
+                if not first_sync_msg:
+                    # We read at least one piece of data and we're on sync run
+                    break
+        except TornadoTimeoutError:
+            # In the timeout case, just return None.
+            # Keep 'self._read_stream_future' alive.
+            ret = None
+        except StreamClosedError as exc:
+            log.trace('Subscriber disconnected from IPC %s', self.socket_path)
+            self._read_stream_future = None
+            exc_to_raise = exc
+        except Exception as exc:
+            log.error('Exception occurred in Subscriber while handling stream: %s', exc)
+            self._read_stream_future = None
+            exc_to_raise = exc
+
+        self._read_in_progress.release()
+
+        if exc_to_raise is not None:
+            raise exc_to_raise  # pylint: disable=E0702
+        raise tornado.gen.Return(ret)
+
+    def read_sync(self, timeout=None):
+        '''
+        Read a message from an IPC socket
+
+        The socket must already be connected.
+        The associated IO Loop must NOT be running.
+        :param int timeout: Timeout when receiving message
+        :return: message data if successful. None if timed out. Will raise an
+                 exception for all other error conditions.
+        '''
+        if self._saved_data:
+            return self._saved_data.pop(0)
+        return self.io_loop.run_sync(lambda: self._read(timeout))
+
+    @tornado.gen.coroutine
+    def read_async(self, callback):
+        '''
+        Asynchronously read messages and invoke a callback when they are ready.
+
+        :param callback: A callback with the received data
+        '''
+        while not self.connected():
+            try:
+                yield self.connect(timeout=5)
+            except StreamClosedError:
+                log.trace('Subscriber closed stream on IPC %s before connect', self.socket_path)
+                yield tornado.gen.sleep(1)
+            except Exception as exc:
+                log.error('Exception occurred while Subscriber connecting: %s', exc)
+                yield tornado.gen.sleep(1)
+        yield self._read(None, callback)
+
+    def close(self):
+        '''
+        Routines to handle any cleanup before the instance shuts down.
+        Sockets and filehandles should be closed explicitly, to prevent
+        leaks.
+        '''
+        if self._closing:
+            return
+        super(IPCMessageSubscriber, self).close()
+        # This will prevent this message from showing up:
+        # '[ERROR   ] Future exception was never retrieved:
+        # StreamClosedError'
+        if self._read_stream_future is not None and self._read_stream_future.done():
+            exc = self._read_stream_future.exception()
+            if exc and not isinstance(exc, StreamClosedError):
+                log.error("Read future returned exception %r", exc)
+
+    def __del__(self):
+        if IPCMessageSubscriber in globals():
+            self.close()
diff -Naur a/salt/transport/mixins/auth.py c/salt/transport/mixins/auth.py
--- a/salt/transport/mixins/auth.py	2019-07-02 10:15:07.051874718 -0600
+++ c/salt/transport/mixins/auth.py	2019-07-02 10:58:03.175938594 -0600
@@ -24,7 +24,10 @@
 
 # Import Third Party Libs
 from salt.ext import six
-import tornado.gen
+try:
+    import tornado4.gen as tornado_gen
+except ImportError:
+    import tornado.gen as tornado_gen
 try:
     from M2Crypto import RSA
     HAS_M2 = True
@@ -51,7 +54,7 @@
             if not salt.crypt.verify_signature(master_pubkey_path, payload['load'], payload.get('sig')):
                 raise salt.crypt.AuthenticationError('Message signature failed to validate.')
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _decode_payload(self, payload):
         # we need to decrypt it
         log.trace('Decoding payload: %s', payload)
@@ -63,7 +66,7 @@
                 yield self.auth.authenticate()
                 payload['load'] = self.auth.crypticle.loads(payload['load'])
 
-        raise tornado.gen.Return(payload)
+        raise tornado_gen.Return(payload)
 
 
 # TODO: rename?
diff -Naur a/salt/transport/tcp.py c/salt/transport/tcp.py
--- a/salt/transport/tcp.py	2019-07-02 10:15:07.051874718 -0600
+++ c/salt/transport/tcp.py	2019-07-02 11:02:05.943944614 -0600
@@ -38,13 +38,24 @@
 from salt.transport import iter_transport_opts
 
 # Import Tornado Libs
-import tornado
-import tornado.tcpserver
-import tornado.gen
-import tornado.concurrent
-import tornado.tcpclient
-import tornado.netutil
-import tornado.iostream
+try:
+    from tornado4 import version_info as tornado_version_info
+    from tornado4.tcpserver import TCPServer
+    import tornado4.gen as tornado_gen
+    from tornado4.concurrent import Future as TornadoFuture
+    from tornado4.ioloop import IOLoop
+    from tornado4.iostream import IOStream, StreamClosedError
+    from tornado4.tcpclient import TCPClient
+    import tornado4.netutil
+except ImportError:
+    from tornado import version_info as tornado_version_info
+    from tornado.tcpserver import TCPServer
+    import tornado.gen as tornado_gen
+    from tornado.concurrent import Future as TornadoFuture
+    from tornado.ioloop import IOLoop
+    from tornado.iostream import IOStream, StreamClosedError
+    from tornado.tcpclient import TCPClient
+    import tornado.netutil
 
 # pylint: disable=import-error,no-name-in-module
 if six.PY2:
@@ -73,7 +84,7 @@
 if USE_LOAD_BALANCER:
     import threading
     import multiprocessing
-    import tornado.util
+    from tornado.util import errno_from_exception
     from salt.utils.process import SignalHandlingMultiprocessingProcess
 
 log = logging.getLogger(__name__)
@@ -207,7 +218,7 @@
                     # ECONNABORTED indicates that there was a connection
                     # but it was closed while still in the accept queue.
                     # (observed on FreeBSD).
-                    if tornado.util.errno_from_exception(e) == errno.ECONNABORTED:
+                    if errno_from_exception(e) == errno.ECONNABORTED:
                         continue
                     raise
 
@@ -228,7 +239,7 @@
         Only create one instance of channel per __key()
         '''
         # do we have any mapping for this io_loop
-        io_loop = kwargs.get('io_loop') or tornado.ioloop.IOLoop.current()
+        io_loop = kwargs.get('io_loop') or IOLoop.current()
         if io_loop not in cls.instance_map:
             cls.instance_map[io_loop] = weakref.WeakValueDictionary()
         loop_instance_map = cls.instance_map[io_loop]
@@ -275,7 +286,7 @@
         # crypt defaults to 'aes'
         self.crypt = kwargs.get('crypt', 'aes')
 
-        self.io_loop = kwargs.get('io_loop') or tornado.ioloop.IOLoop.current()
+        self.io_loop = kwargs.get('io_loop') or IOLoop.current()
 
         if self.crypt != 'clear':
             self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)
@@ -339,7 +350,7 @@
             'load': load,
         }
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def crypted_transfer_decode_dictentry(self, load, dictkey=None, tries=3, timeout=60):
         if not self.auth.authenticated:
             yield self.auth.authenticate()
@@ -354,9 +365,9 @@
         data = pcrypt.loads(ret[dictkey])
         if six.PY3:
             data = salt.transport.frame.decode_embedded_strs(data)
-        raise tornado.gen.Return(data)
+        raise tornado_gen.Return(data)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _crypted_transfer(self, load, tries=3, timeout=60):
         '''
         In case of authentication errors, try to renegotiate authentication
@@ -364,7 +375,7 @@
         Indeed, we can fail too early in case of a master restart during a
         minion state execution call
         '''
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def _do_transfer():
             data = yield self.message_client.send(self._package_load(self.auth.crypticle.dumps(load)),
                                                   timeout=timeout,
@@ -377,24 +388,24 @@
                 data = self.auth.crypticle.loads(data)
                 if six.PY3:
                     data = salt.transport.frame.decode_embedded_strs(data)
-            raise tornado.gen.Return(data)
+            raise tornado_gen.Return(data)
 
         if not self.auth.authenticated:
             yield self.auth.authenticate()
         try:
             ret = yield _do_transfer()
-            raise tornado.gen.Return(ret)
+            raise tornado_gen.Return(ret)
         except salt.crypt.AuthenticationError:
             yield self.auth.authenticate()
             ret = yield _do_transfer()
-            raise tornado.gen.Return(ret)
+            raise tornado_gen.Return(ret)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _uncrypted_transfer(self, load, tries=3, timeout=60):
         ret = yield self.message_client.send(self._package_load(load), timeout=timeout)
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def send(self, load, tries=3, timeout=60, raw=False):
         '''
         Send a request, return a future which will complete when we send the message
@@ -404,11 +415,11 @@
                 ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)
             else:
                 ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout)
-        except tornado.iostream.StreamClosedError:
+        except StreamClosedError:
             # Convert to 'SaltClientError' so that clients can handle this
             # exception more appropriately.
             raise SaltClientError('Connection to master lost')
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
 
 class AsyncTCPPubChannel(salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel):
@@ -420,7 +431,7 @@
         self.serial = salt.payload.Serial(self.opts)
 
         self.crypt = kwargs.get('crypt', 'aes')
-        self.io_loop = kwargs.get('io_loop') or tornado.ioloop.IOLoop.current()
+        self.io_loop = kwargs.get('io_loop') or IOLoop.current()
         self.connected = False
         self._closing = False
         self._reconnected = False
@@ -446,7 +457,7 @@
             'load': load,
         }
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def send_id(self, tok, force_auth):
         '''
         Send the minion id to the master so that the master may better
@@ -456,12 +467,12 @@
         '''
         load = {'id': self.opts['id'], 'tok': tok}
 
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def _do_transfer():
             msg = self._package_load(self.auth.crypticle.dumps(load))
             package = salt.transport.frame.frame_msg(msg, header=None)
             yield self.message_client.write_to_stream(package)
-            raise tornado.gen.Return(True)
+            raise tornado_gen.Return(True)
 
         if force_auth or not self.auth.authenticated:
             count = 0
@@ -474,13 +485,13 @@
                     count += 1
         try:
             ret = yield _do_transfer()
-            raise tornado.gen.Return(ret)
+            raise tornado_gen.Return(ret)
         except salt.crypt.AuthenticationError:
             yield self.auth.authenticate()
             ret = yield _do_transfer()
-            raise tornado.gen.Return(ret)
+            raise tornado_gen.Return(ret)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def connect_callback(self, result):
         if self._closing:
             return
@@ -543,7 +554,7 @@
             '__master_disconnected'
         )
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def connect(self):
         try:
             self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)
@@ -575,7 +586,7 @@
         if callback is None:
             return self.message_client.on_recv(callback)
 
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def wrap_callback(body):
             if not isinstance(body, dict):
                 # TODO: For some reason we need to decode here for things
@@ -667,7 +678,7 @@
                 self._socket.listen(self.backlog)
         salt.transport.mixins.auth.AESReqServerMixin.post_fork(self, payload_handler, io_loop)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def handle_message(self, stream, header, payload):
         '''
         Handle incoming messages from underylying tcp streams
@@ -677,31 +688,31 @@
                 payload = self._decode_payload(payload)
             except Exception:
                 stream.write(salt.transport.frame.frame_msg('bad load', header=header))
-                raise tornado.gen.Return()
+                raise tornado_gen.Return()
 
             # TODO helper functions to normalize payload?
             if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):
                 yield stream.write(salt.transport.frame.frame_msg(
                     'payload and load must be a dict', header=header))
-                raise tornado.gen.Return()
+                raise tornado_gen.Return()
 
             try:
                 id_ = payload['load'].get('id', '')
                 if str('\0') in id_:
                     log.error('Payload contains an id with a null byte: %s', payload)
                     stream.send(self.serial.dumps('bad load: id contains a null byte'))
-                    raise tornado.gen.Return()
+                    raise tornado_gen.Return()
             except TypeError:
                 log.error('Payload contains non-string id: %s', payload)
                 stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))
-                raise tornado.gen.Return()
+                raise tornado_gen.Return()
 
             # intercept the "_auth" commands, since the main daemon shouldn't know
             # anything about our key auth
             if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':
                 yield stream.write(salt.transport.frame.frame_msg(
                     self._auth(payload['load']), header=header))
-                raise tornado.gen.Return()
+                raise tornado_gen.Return()
 
             # TODO: test
             try:
@@ -711,7 +722,7 @@
                 stream.write('Some exception handling minion payload')
                 log.error('Some exception handling a payload from minion', exc_info=True)
                 stream.close()
-                raise tornado.gen.Return()
+                raise tornado_gen.Return()
 
             req_fun = req_opts.get('fun', 'send')
             if req_fun == 'send_clear':
@@ -728,9 +739,9 @@
                 # always attempt to return an error to the minion
                 stream.write('Server-side exception handling payload')
                 stream.close()
-        except tornado.gen.Return:
+        except tornado_gen.Return:
             raise
-        except tornado.iostream.StreamClosedError:
+        except StreamClosedError:
             # Stream was closed. This could happen if the remote side
             # closed the connection on its end (eg in a timeout or shutdown
             # situation).
@@ -739,22 +750,22 @@
             # Absorb any other exceptions
             log.error('Unexpected exception occurred: %s', exc, exc_info=True)
 
-        raise tornado.gen.Return()
+        raise tornado_gen.Return()
 
 
-class SaltMessageServer(tornado.tcpserver.TCPServer, object):
+class SaltMessageServer(TCPServer, object):
     '''
     Raw TCP server which will receive all of the TCP streams and re-assemble
     messages that are sent through to us
     '''
     def __init__(self, message_handler, *args, **kwargs):
         super(SaltMessageServer, self).__init__(*args, **kwargs)
-        self.io_loop = tornado.ioloop.IOLoop.current()
+        self.io_loop = IOLoop.current()
 
         self.clients = []
         self.message_handler = message_handler
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def handle_stream(self, stream, address):
         '''
         Handle incoming streams and add messages to the incoming queue
@@ -774,7 +785,7 @@
                     header = framed_msg['head']
                     self.io_loop.spawn_callback(self.message_handler, stream, header, framed_msg['body'])
 
-        except tornado.iostream.StreamClosedError:
+        except StreamClosedError:
             log.trace('req client disconnected %s', address)
             self.clients.remove((stream, address))
         except Exception as e:
@@ -830,7 +841,7 @@
                 pass
 
 
-class TCPClientKeepAlive(tornado.tcpclient.TCPClient):
+class TCPClientKeepAlive(TCPClient):
     '''
     Override _create_stream() in TCPClient to enable keep alive support.
     '''
@@ -850,10 +861,10 @@
         # after one connection has completed.
         sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
         _set_tcp_keepalive(sock, self.opts)
-        stream = tornado.iostream.IOStream(
+        stream = IOStream(
             sock,
             max_buffer_size=max_buffer_size)
-        if tornado.version_info < (5,):
+        if tornado_version_info < (5,):
             return stream.connect(addr)
         return stream, stream.connect(addr)
 
@@ -873,14 +884,14 @@
             message_client.close()
         self.message_clients = []
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def connect(self):
         futures = []
         for message_client in self.message_clients:
             futures.append(message_client.connect())
         for future in futures:
             yield future
-        raise tornado.gen.Return(None)
+        raise tornado_gen.Return(None)
 
     def on_recv(self, *args, **kwargs):
         for message_client in self.message_clients:
@@ -913,7 +924,7 @@
         self.connect_callback = connect_callback
         self.disconnect_callback = disconnect_callback
 
-        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        self.io_loop = io_loop or IOLoop.current()
 
         with salt.utils.asynchronous.current_ioloop(self.io_loop):
             self._tcp_client = TCPClientKeepAlive(opts, resolver=resolver)
@@ -930,7 +941,7 @@
         self._on_recv = None
         self._closing = False
         self._connecting_future = self.connect()
-        self._stream_return_future = tornado.concurrent.Future()
+        self._stream_return_future = TornadoFuture()
         self.io_loop.spawn_callback(self._stream_return)
 
     # TODO: timeout inflight sessions
@@ -945,7 +956,7 @@
             # _stream_return() completes by restarting the IO Loop.
             # This will prevent potential errors on shutdown.
             try:
-                orig_loop = tornado.ioloop.IOLoop.current()
+                orig_loop = IOLoop.current()
                 self.io_loop.make_current()
                 self._stream.close()
                 if self._read_until_future is not None:
@@ -957,7 +968,7 @@
                     # 'StreamClosedError' when the stream is closed.
                     if self._read_until_future.done():
                         self._read_until_future.exception()
-                    elif self.io_loop != tornado.ioloop.IOLoop.current(instance=False):
+                    elif self.io_loop != IOLoop.current(instance=False):
                         self.io_loop.add_future(
                             self._stream_return_future,
                             lambda future: self.io_loop.stop()
@@ -981,7 +992,7 @@
         if hasattr(self, '_connecting_future') and not self._connecting_future.done():
             future = self._connecting_future
         else:
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             self._connecting_future = future
             self.io_loop.add_callback(self._connect)
 
@@ -995,7 +1006,7 @@
         return future
 
     # TODO: tcp backoff opts
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _connect(self):
         '''
         Try to connect for the rest of time!
@@ -1006,7 +1017,7 @@
             try:
                 kwargs = {}
                 if self.source_ip or self.source_port:
-                    if tornado.version_info >= (4, 5):
+                    if tornado_version_info >= (4, 5):
                         ### source_ip and source_port are supported only in Tornado >= 4.5
                         # See http://www.tornadoweb.org/en/stable/releases/v4.5.0.html
                         # Otherwise will just ignore these args
@@ -1022,10 +1033,10 @@
                 self._connecting_future.set_result(True)
                 break
             except Exception as e:
-                yield tornado.gen.sleep(1)  # TODO: backoff
+                yield tornado_gen.sleep(1)  # TODO: backoff
                 #self._connecting_future.set_exception(e)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _stream_return(self):
         try:
             while not self._closing and (
@@ -1055,7 +1066,7 @@
                                 self.io_loop.spawn_callback(self._on_recv, header, body)
                             else:
                                 log.error('Got response for message_id %s that we are not tracking', message_id)
-                except tornado.iostream.StreamClosedError as e:
+                except StreamClosedError as e:
                     log.debug('tcp stream to %s:%s closed, unable to recv', self.host, self.port)
                     for future in six.itervalues(self.send_future_map):
                         future.set_exception(e)
@@ -1091,7 +1102,7 @@
         finally:
             self._stream_return_future.set_result(True)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _stream_send(self):
         while not self._connecting_future.done() or self._connecting_future.result() is not True:
             yield self._connecting_future
@@ -1102,7 +1113,7 @@
                 del self.send_queue[0]
             # if the connection is dead, lets fail this send, and make sure we
             # attempt to reconnect
-            except tornado.iostream.StreamClosedError as e:
+            except StreamClosedError as e:
                 if message_id in self.send_future_map:
                     self.send_future_map.pop(message_id).set_exception(e)
                 self.remove_message_timeout(message_id)
@@ -1163,7 +1174,7 @@
         message_id = self._message_id()
         header = {'mid': message_id}
 
-        future = tornado.concurrent.Future()
+        future = TornadoFuture()
         if callback is not None:
             def handle_future(future):
                 response = future.result()
@@ -1216,7 +1227,7 @@
         self.close()
 
 
-class PubServer(tornado.tcpserver.TCPServer, object):
+class PubServer(TCPServer, object):
     '''
     TCP publisher
     '''
@@ -1306,7 +1317,7 @@
                     salt.utils.event.tagify('present', 'presence')
                 )
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _stream_read(self, client):
         unpacker = msgpack.Unpacker()
         while not self._closing:
@@ -1331,7 +1342,7 @@
                         continue
                     client.id_ = load['id']
                     self._add_client_present(client)
-            except tornado.iostream.StreamClosedError as e:
+            except StreamClosedError as e:
                 log.debug('tcp stream to %s closed, unable to recv', client.address)
                 client.close()
                 self._remove_client_present(client)
@@ -1348,7 +1359,7 @@
         self.io_loop.spawn_callback(self._stream_read, client)
 
     # TODO: ACK the publish through IPC
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def publish_payload(self, package, _):
         log.debug('TCP PubServer sending payload: %s', package)
         payload = salt.transport.frame.frame_msg(package['payload'])
@@ -1368,7 +1379,7 @@
                             # Write the packed str
                             f = client.stream.write(payload)
                             self.io_loop.add_future(f, lambda f: True)
-                        except tornado.iostream.StreamClosedError:
+                        except StreamClosedError:
                             to_remove.append(client)
                 else:
                     log.debug('Publish target %s not connected', topic)
@@ -1378,7 +1389,7 @@
                     # Write the packed str
                     f = client.stream.write(payload)
                     self.io_loop.add_future(f, lambda f: True)
-                except tornado.iostream.StreamClosedError:
+                except StreamClosedError:
                     to_remove.append(client)
         for client in to_remove:
             log.debug('Subscriber at %s has disconnected from publisher', client.address)
@@ -1423,7 +1434,7 @@
 
         # Check if io_loop was set outside
         if self.io_loop is None:
-            self.io_loop = tornado.ioloop.IOLoop.current()
+            self.io_loop = IOLoop.current()
 
         # Spin up the publisher
         pub_server = PubServer(self.opts, io_loop=self.io_loop)
diff -Naur a/salt/transport/tcp.py.orig c/salt/transport/tcp.py.orig
--- a/salt/transport/tcp.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/transport/tcp.py.orig	2019-07-02 10:57:25.415937658 -0600
@@ -0,0 +1,1511 @@
+# -*- coding: utf-8 -*-
+'''
+TCP transport classes
+
+Wire protocol: "len(payload) msgpack({'head': SOMEHEADER, 'body': SOMEBODY})"
+
+'''
+
+# Import Python Libs
+from __future__ import absolute_import, print_function, unicode_literals
+import errno
+import logging
+import socket
+import os
+import weakref
+import time
+import threading
+import traceback
+
+# Import Salt Libs
+import salt.crypt
+import salt.utils.asynchronous
+import salt.utils.event
+import salt.utils.files
+import salt.utils.platform
+import salt.utils.process
+import salt.utils.verify
+import salt.payload
+import salt.exceptions
+import salt.transport.frame
+import salt.transport.ipc
+import salt.transport.client
+import salt.transport.server
+import salt.transport.mixins.auth
+from salt.ext import six
+from salt.ext.six.moves import queue  # pylint: disable=import-error
+from salt.exceptions import SaltReqTimeoutError, SaltClientError
+from salt.transport import iter_transport_opts
+
+# Import Tornado Libs
+import tornado
+import tornado.tcpserver
+import tornado.gen
+import tornado.concurrent
+import tornado.tcpclient
+import tornado.netutil
+import tornado.iostream
+
+# pylint: disable=import-error,no-name-in-module
+if six.PY2:
+    import urlparse
+else:
+    import urllib.parse as urlparse
+# pylint: enable=import-error,no-name-in-module
+
+# Import third party libs
+import msgpack
+try:
+    from M2Crypto import RSA
+    HAS_M2 = True
+except ImportError:
+    HAS_M2 = False
+    try:
+        from Cryptodome.Cipher import PKCS1_OAEP
+    except ImportError:
+        from Crypto.Cipher import PKCS1_OAEP
+
+if six.PY3 and salt.utils.platform.is_windows():
+    USE_LOAD_BALANCER = True
+else:
+    USE_LOAD_BALANCER = False
+
+if USE_LOAD_BALANCER:
+    import threading
+    import multiprocessing
+    import tornado.util
+    from salt.utils.process import SignalHandlingMultiprocessingProcess
+
+log = logging.getLogger(__name__)
+
+
+def _set_tcp_keepalive(sock, opts):
+    '''
+    Ensure that TCP keepalives are set for the socket.
+    '''
+    if hasattr(socket, 'SO_KEEPALIVE'):
+        if opts.get('tcp_keepalive', False):
+            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
+            if hasattr(socket, 'SOL_TCP'):
+                if hasattr(socket, 'TCP_KEEPIDLE'):
+                    tcp_keepalive_idle = opts.get('tcp_keepalive_idle', -1)
+                    if tcp_keepalive_idle > 0:
+                        sock.setsockopt(
+                            socket.SOL_TCP, socket.TCP_KEEPIDLE,
+                            int(tcp_keepalive_idle))
+                if hasattr(socket, 'TCP_KEEPCNT'):
+                    tcp_keepalive_cnt = opts.get('tcp_keepalive_cnt', -1)
+                    if tcp_keepalive_cnt > 0:
+                        sock.setsockopt(
+                            socket.SOL_TCP, socket.TCP_KEEPCNT,
+                            int(tcp_keepalive_cnt))
+                if hasattr(socket, 'TCP_KEEPINTVL'):
+                    tcp_keepalive_intvl = opts.get('tcp_keepalive_intvl', -1)
+                    if tcp_keepalive_intvl > 0:
+                        sock.setsockopt(
+                            socket.SOL_TCP, socket.TCP_KEEPINTVL,
+                            int(tcp_keepalive_intvl))
+            if hasattr(socket, 'SIO_KEEPALIVE_VALS'):
+                # Windows doesn't support TCP_KEEPIDLE, TCP_KEEPCNT, nor
+                # TCP_KEEPINTVL. Instead, it has its own proprietary
+                # SIO_KEEPALIVE_VALS.
+                tcp_keepalive_idle = opts.get('tcp_keepalive_idle', -1)
+                tcp_keepalive_intvl = opts.get('tcp_keepalive_intvl', -1)
+                # Windows doesn't support changing something equivalent to
+                # TCP_KEEPCNT.
+                if tcp_keepalive_idle > 0 or tcp_keepalive_intvl > 0:
+                    # Windows defaults may be found by using the link below.
+                    # Search for 'KeepAliveTime' and 'KeepAliveInterval'.
+                    # https://technet.microsoft.com/en-us/library/bb726981.aspx#EDAA
+                    # If one value is set and the other isn't, we still need
+                    # to send both values to SIO_KEEPALIVE_VALS and they both
+                    # need to be valid. So in that case, use the Windows
+                    # default.
+                    if tcp_keepalive_idle <= 0:
+                        tcp_keepalive_idle = 7200
+                    if tcp_keepalive_intvl <= 0:
+                        tcp_keepalive_intvl = 1
+                    # The values expected are in milliseconds, so multiply by
+                    # 1000.
+                    sock.ioctl(socket.SIO_KEEPALIVE_VALS, (
+                        1, int(tcp_keepalive_idle * 1000),
+                        int(tcp_keepalive_intvl * 1000)))
+        else:
+            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 0)
+
+
+if USE_LOAD_BALANCER:
+    class LoadBalancerServer(SignalHandlingMultiprocessingProcess):
+        '''
+        Raw TCP server which runs in its own process and will listen
+        for incoming connections. Each incoming connection will be
+        sent via multiprocessing queue to the workers.
+        Since the queue is shared amongst workers, only one worker will
+        handle a given connection.
+        '''
+        # TODO: opts!
+        # Based on default used in tornado.netutil.bind_sockets()
+        backlog = 128
+
+        def __init__(self, opts, socket_queue, **kwargs):
+            super(LoadBalancerServer, self).__init__(**kwargs)
+            self.opts = opts
+            self.socket_queue = socket_queue
+            self._socket = None
+
+        # __setstate__ and __getstate__ are only used on Windows.
+        # We do this so that __init__ will be invoked on Windows in the child
+        # process so that a register_after_fork() equivalent will work on
+        # Windows.
+        def __setstate__(self, state):
+            self._is_child = True
+            self.__init__(
+                state['opts'],
+                state['socket_queue'],
+                log_queue=state['log_queue'],
+                log_queue_level=state['log_queue_level']
+            )
+
+        def __getstate__(self):
+            return {
+                'opts': self.opts,
+                'socket_queue': self.socket_queue,
+                'log_queue': self.log_queue,
+                'log_queue_level': self.log_queue_level
+            }
+
+        def close(self):
+            if self._socket is not None:
+                self._socket.shutdown(socket.SHUT_RDWR)
+                self._socket.close()
+                self._socket = None
+
+        def __del__(self):
+            self.close()
+
+        def run(self):
+            '''
+            Start the load balancer
+            '''
+            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            _set_tcp_keepalive(self._socket, self.opts)
+            self._socket.setblocking(1)
+            self._socket.bind((self.opts['interface'], int(self.opts['ret_port'])))
+            self._socket.listen(self.backlog)
+
+            while True:
+                try:
+                    # Wait for a connection to occur since the socket is
+                    # blocking.
+                    connection, address = self._socket.accept()
+                    # Wait for a free slot to be available to put
+                    # the connection into.
+                    # Sockets are picklable on Windows in Python 3.
+                    self.socket_queue.put((connection, address), True, None)
+                except socket.error as e:
+                    # ECONNABORTED indicates that there was a connection
+                    # but it was closed while still in the accept queue.
+                    # (observed on FreeBSD).
+                    if tornado.util.errno_from_exception(e) == errno.ECONNABORTED:
+                        continue
+                    raise
+
+
+# TODO: move serial down into message library
+class AsyncTCPReqChannel(salt.transport.client.ReqChannel):
+    '''
+    Encapsulate sending routines to tcp.
+
+    Note: this class returns a singleton
+    '''
+    # This class is only a singleton per minion/master pair
+    # mapping of io_loop -> {key -> channel}
+    instance_map = weakref.WeakKeyDictionary()
+
+    def __new__(cls, opts, **kwargs):
+        '''
+        Only create one instance of channel per __key()
+        '''
+        # do we have any mapping for this io_loop
+        io_loop = kwargs.get('io_loop') or tornado.ioloop.IOLoop.current()
+        if io_loop not in cls.instance_map:
+            cls.instance_map[io_loop] = weakref.WeakValueDictionary()
+        loop_instance_map = cls.instance_map[io_loop]
+
+        key = cls.__key(opts, **kwargs)
+        obj = loop_instance_map.get(key)
+        if obj is None:
+            log.debug('Initializing new AsyncTCPReqChannel for %s', key)
+            # we need to make a local variable for this, as we are going to store
+            # it in a WeakValueDictionary-- which will remove the item if no one
+            # references it-- this forces a reference while we return to the caller
+            obj = object.__new__(cls)
+            obj.__singleton_init__(opts, **kwargs)
+            obj._instance_key = key
+            loop_instance_map[key] = obj
+            obj._refcount = 1
+            obj._refcount_lock = threading.RLock()
+        else:
+            with obj._refcount_lock:
+                obj._refcount += 1
+            log.debug('Re-using AsyncTCPReqChannel for %s', key)
+        return obj
+
+    @classmethod
+    def __key(cls, opts, **kwargs):
+        if 'master_uri' in kwargs:
+            opts['master_uri'] = kwargs['master_uri']
+        return (opts['pki_dir'],     # where the keys are stored
+                opts['id'],          # minion ID
+                opts['master_uri'],
+                kwargs.get('crypt', 'aes'),  # TODO: use the same channel for crypt
+                )
+
+    # has to remain empty for singletons, since __init__ will *always* be called
+    def __init__(self, opts, **kwargs):
+        pass
+
+    # an init for the singleton instance to call
+    def __singleton_init__(self, opts, **kwargs):
+        self.opts = dict(opts)
+
+        self.serial = salt.payload.Serial(self.opts)
+
+        # crypt defaults to 'aes'
+        self.crypt = kwargs.get('crypt', 'aes')
+
+        self.io_loop = kwargs.get('io_loop') or tornado.ioloop.IOLoop.current()
+
+        if self.crypt != 'clear':
+            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)
+
+        resolver = kwargs.get('resolver')
+
+        parse = urlparse.urlparse(self.opts['master_uri'])
+        master_host, master_port = parse.netloc.rsplit(':', 1)
+        self.master_addr = (master_host, int(master_port))
+        self._closing = False
+        self.message_client = SaltMessageClientPool(self.opts,
+                                                    args=(self.opts, master_host, int(master_port),),
+                                                    kwargs={'io_loop': self.io_loop, 'resolver': resolver,
+                                                            'source_ip': self.opts.get('source_ip'),
+                                                            'source_port': self.opts.get('source_ret_port')})
+
+    def close(self):
+        if self._closing:
+            return
+
+        if self._refcount > 1:
+            # Decrease refcount
+            with self._refcount_lock:
+                self._refcount -= 1
+            log.debug(
+                'This is not the last %s instance. Not closing yet.',
+                self.__class__.__name__
+            )
+            return
+
+        log.debug('Closing %s instance', self.__class__.__name__)
+        self._closing = True
+        self.message_client.close()
+
+        # Remove the entry from the instance map so that a closed entry may not
+        # be reused.
+        # This forces this operation even if the reference count of the entry
+        # has not yet gone to zero.
+        if self.io_loop in self.__class__.instance_map:
+            loop_instance_map = self.__class__.instance_map[self.io_loop]
+            if self._instance_key in loop_instance_map:
+                del loop_instance_map[self._instance_key]
+            if not loop_instance_map:
+                del self.__class__.instance_map[self.io_loop]
+
+    def __del__(self):
+        with self._refcount_lock:
+            # Make sure we actually close no matter if something
+            # went wrong with our ref counting
+            self._refcount = 1
+        try:
+            self.close()
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+
+    def _package_load(self, load):
+        return {
+            'enc': self.crypt,
+            'load': load,
+        }
+
+    @tornado.gen.coroutine
+    def crypted_transfer_decode_dictentry(self, load, dictkey=None, tries=3, timeout=60):
+        if not self.auth.authenticated:
+            yield self.auth.authenticate()
+        ret = yield self.message_client.send(self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout)
+        key = self.auth.get_keys()
+        if HAS_M2:
+            aes = key.private_decrypt(ret['key'], RSA.pkcs1_oaep_padding)
+        else:
+            cipher = PKCS1_OAEP.new(key)
+            aes = cipher.decrypt(ret['key'])
+        pcrypt = salt.crypt.Crypticle(self.opts, aes)
+        data = pcrypt.loads(ret[dictkey])
+        if six.PY3:
+            data = salt.transport.frame.decode_embedded_strs(data)
+        raise tornado.gen.Return(data)
+
+    @tornado.gen.coroutine
+    def _crypted_transfer(self, load, tries=3, timeout=60):
+        '''
+        In case of authentication errors, try to renegotiate authentication
+        and retry the method.
+        Indeed, we can fail too early in case of a master restart during a
+        minion state execution call
+        '''
+        @tornado.gen.coroutine
+        def _do_transfer():
+            data = yield self.message_client.send(self._package_load(self.auth.crypticle.dumps(load)),
+                                                  timeout=timeout,
+                                                  )
+            # we may not have always data
+            # as for example for saltcall ret submission, this is a blind
+            # communication, we do not subscribe to return events, we just
+            # upload the results to the master
+            if data:
+                data = self.auth.crypticle.loads(data)
+                if six.PY3:
+                    data = salt.transport.frame.decode_embedded_strs(data)
+            raise tornado.gen.Return(data)
+
+        if not self.auth.authenticated:
+            yield self.auth.authenticate()
+        try:
+            ret = yield _do_transfer()
+            raise tornado.gen.Return(ret)
+        except salt.crypt.AuthenticationError:
+            yield self.auth.authenticate()
+            ret = yield _do_transfer()
+            raise tornado.gen.Return(ret)
+
+    @tornado.gen.coroutine
+    def _uncrypted_transfer(self, load, tries=3, timeout=60):
+        ret = yield self.message_client.send(self._package_load(load), timeout=timeout)
+        raise tornado.gen.Return(ret)
+
+    @tornado.gen.coroutine
+    def send(self, load, tries=3, timeout=60, raw=False):
+        '''
+        Send a request, return a future which will complete when we send the message
+        '''
+        try:
+            if self.crypt == 'clear':
+                ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)
+            else:
+                ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout)
+        except tornado.iostream.StreamClosedError:
+            # Convert to 'SaltClientError' so that clients can handle this
+            # exception more appropriately.
+            raise SaltClientError('Connection to master lost')
+        raise tornado.gen.Return(ret)
+
+
+class AsyncTCPPubChannel(salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel):
+    def __init__(self,
+                 opts,
+                 **kwargs):
+        self.opts = opts
+
+        self.serial = salt.payload.Serial(self.opts)
+
+        self.crypt = kwargs.get('crypt', 'aes')
+        self.io_loop = kwargs.get('io_loop') or tornado.ioloop.IOLoop.current()
+        self.connected = False
+        self._closing = False
+        self._reconnected = False
+        self.event = salt.utils.event.get_event(
+            'minion',
+            opts=self.opts,
+            listen=False
+        )
+
+    def close(self):
+        if self._closing:
+            return
+        self._closing = True
+        if hasattr(self, 'message_client'):
+            self.message_client.close()
+
+    def __del__(self):
+        self.close()
+
+    def _package_load(self, load):
+        return {
+            'enc': self.crypt,
+            'load': load,
+        }
+
+    @tornado.gen.coroutine
+    def send_id(self, tok, force_auth):
+        '''
+        Send the minion id to the master so that the master may better
+        track the connection state of the minion.
+        In case of authentication errors, try to renegotiate authentication
+        and retry the method.
+        '''
+        load = {'id': self.opts['id'], 'tok': tok}
+
+        @tornado.gen.coroutine
+        def _do_transfer():
+            msg = self._package_load(self.auth.crypticle.dumps(load))
+            package = salt.transport.frame.frame_msg(msg, header=None)
+            yield self.message_client.write_to_stream(package)
+            raise tornado.gen.Return(True)
+
+        if force_auth or not self.auth.authenticated:
+            count = 0
+            while count <= self.opts['tcp_authentication_retries'] or self.opts['tcp_authentication_retries'] < 0:
+                try:
+                    yield self.auth.authenticate()
+                    break
+                except SaltClientError as exc:
+                    log.debug(exc)
+                    count += 1
+        try:
+            ret = yield _do_transfer()
+            raise tornado.gen.Return(ret)
+        except salt.crypt.AuthenticationError:
+            yield self.auth.authenticate()
+            ret = yield _do_transfer()
+            raise tornado.gen.Return(ret)
+
+    @tornado.gen.coroutine
+    def connect_callback(self, result):
+        if self._closing:
+            return
+        # Force re-auth on reconnect since the master
+        # may have been restarted
+        yield self.send_id(self.tok, self._reconnected)
+        self.connected = True
+        self.event.fire_event(
+            {'master': self.opts['master']},
+            '__master_connected'
+        )
+        if self._reconnected:
+            # On reconnects, fire a master event to notify that the minion is
+            # available.
+            if self.opts.get('__role') == 'syndic':
+                data = 'Syndic {0} started at {1}'.format(
+                    self.opts['id'],
+                    time.asctime()
+                )
+                tag = salt.utils.event.tagify(
+                    [self.opts['id'], 'start'],
+                    'syndic'
+                )
+            else:
+                data = 'Minion {0} started at {1}'.format(
+                    self.opts['id'],
+                    time.asctime()
+                )
+                tag = salt.utils.event.tagify(
+                    [self.opts['id'], 'start'],
+                    'minion'
+                )
+            load = {'id': self.opts['id'],
+                    'cmd': '_minion_event',
+                    'pretag': None,
+                    'tok': self.tok,
+                    'data': data,
+                    'tag': tag}
+            req_channel = salt.utils.asynchronous.SyncWrapper(
+                AsyncTCPReqChannel, (self.opts,)
+            )
+            try:
+                req_channel.send(load, timeout=60)
+            except salt.exceptions.SaltReqTimeoutError:
+                log.info('fire_master failed: master could not be contacted. Request timed out.')
+            except Exception:
+                log.info('fire_master failed: %s', traceback.format_exc())
+            finally:
+                # SyncWrapper will call either close() or destroy(), whichever is available
+                del req_channel
+        else:
+            self._reconnected = True
+
+    def disconnect_callback(self):
+        if self._closing:
+            return
+        self.connected = False
+        self.event.fire_event(
+            {'master': self.opts['master']},
+            '__master_disconnected'
+        )
+
+    @tornado.gen.coroutine
+    def connect(self):
+        try:
+            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)
+            self.tok = self.auth.gen_token(b'salt')
+            if not self.auth.authenticated:
+                yield self.auth.authenticate()
+            if self.auth.authenticated:
+                self.message_client = SaltMessageClientPool(
+                    self.opts,
+                    args=(self.opts, self.opts['master_ip'], int(self.auth.creds['publish_port']),),
+                    kwargs={'io_loop': self.io_loop,
+                            'connect_callback': self.connect_callback,
+                            'disconnect_callback': self.disconnect_callback,
+                            'source_ip': self.opts.get('source_ip'),
+                            'source_port': self.opts.get('source_publish_port')})
+                yield self.message_client.connect()  # wait for the client to be connected
+                self.connected = True
+        # TODO: better exception handling...
+        except KeyboardInterrupt:
+            raise
+        except Exception as exc:
+            if '-|RETRY|-' not in six.text_type(exc):
+                raise SaltClientError('Unable to sign_in to master: {0}'.format(exc))  # TODO: better error message
+
+    def on_recv(self, callback):
+        '''
+        Register an on_recv callback
+        '''
+        if callback is None:
+            return self.message_client.on_recv(callback)
+
+        @tornado.gen.coroutine
+        def wrap_callback(body):
+            if not isinstance(body, dict):
+                # TODO: For some reason we need to decode here for things
+                #       to work. Fix this.
+                body = msgpack.loads(body)
+                if six.PY3:
+                    body = salt.transport.frame.decode_embedded_strs(body)
+            ret = yield self._decode_payload(body)
+            callback(ret)
+        return self.message_client.on_recv(wrap_callback)
+
+
+class TCPReqServerChannel(salt.transport.mixins.auth.AESReqServerMixin, salt.transport.server.ReqServerChannel):
+    # TODO: opts!
+    backlog = 5
+
+    def __init__(self, opts):
+        salt.transport.server.ReqServerChannel.__init__(self, opts)
+        self._socket = None
+
+    @property
+    def socket(self):
+        return self._socket
+
+    def close(self):
+        if self._socket is not None:
+            try:
+                self._socket.shutdown(socket.SHUT_RDWR)
+            except socket.error as exc:
+                if exc.errno == errno.ENOTCONN:
+                    # We may try to shutdown a socket which is already disconnected.
+                    # Ignore this condition and continue.
+                    pass
+                else:
+                    raise exc
+            self._socket.close()
+            self._socket = None
+        if hasattr(self.req_server, 'stop'):
+            try:
+                self.req_server.stop()
+            except Exception as exc:
+                log.exception('TCPReqServerChannel close generated an exception: %s', str(exc))
+
+    def __del__(self):
+        self.close()
+
+    def pre_fork(self, process_manager):
+        '''
+        Pre-fork we need to create the zmq router device
+        '''
+        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)
+        if USE_LOAD_BALANCER:
+            self.socket_queue = multiprocessing.Queue()
+            process_manager.add_process(
+                LoadBalancerServer, args=(self.opts, self.socket_queue)
+            )
+        elif not salt.utils.platform.is_windows():
+            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            _set_tcp_keepalive(self._socket, self.opts)
+            self._socket.setblocking(0)
+            self._socket.bind((self.opts['interface'], int(self.opts['ret_port'])))
+
+    def post_fork(self, payload_handler, io_loop):
+        '''
+        After forking we need to create all of the local sockets to listen to the
+        router
+
+        payload_handler: function to call with your payloads
+        '''
+        self.payload_handler = payload_handler
+        self.io_loop = io_loop
+        self.serial = salt.payload.Serial(self.opts)
+        with salt.utils.asynchronous.current_ioloop(self.io_loop):
+            if USE_LOAD_BALANCER:
+                self.req_server = LoadBalancerWorker(self.socket_queue,
+                                                     self.handle_message,
+                                                     ssl_options=self.opts.get('ssl'))
+            else:
+                if salt.utils.platform.is_windows():
+                    self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                    self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+                    _set_tcp_keepalive(self._socket, self.opts)
+                    self._socket.setblocking(0)
+                    self._socket.bind((self.opts['interface'], int(self.opts['ret_port'])))
+                self.req_server = SaltMessageServer(self.handle_message,
+                                                    ssl_options=self.opts.get('ssl'))
+                self.req_server.add_socket(self._socket)
+                self._socket.listen(self.backlog)
+        salt.transport.mixins.auth.AESReqServerMixin.post_fork(self, payload_handler, io_loop)
+
+    @tornado.gen.coroutine
+    def handle_message(self, stream, header, payload):
+        '''
+        Handle incoming messages from underylying tcp streams
+        '''
+        try:
+            try:
+                payload = self._decode_payload(payload)
+            except Exception:
+                stream.write(salt.transport.frame.frame_msg('bad load', header=header))
+                raise tornado.gen.Return()
+
+            # TODO helper functions to normalize payload?
+            if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):
+                yield stream.write(salt.transport.frame.frame_msg(
+                    'payload and load must be a dict', header=header))
+                raise tornado.gen.Return()
+
+            try:
+                id_ = payload['load'].get('id', '')
+                if str('\0') in id_:
+                    log.error('Payload contains an id with a null byte: %s', payload)
+                    stream.send(self.serial.dumps('bad load: id contains a null byte'))
+                    raise tornado.gen.Return()
+            except TypeError:
+                log.error('Payload contains non-string id: %s', payload)
+                stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))
+                raise tornado.gen.Return()
+
+            # intercept the "_auth" commands, since the main daemon shouldn't know
+            # anything about our key auth
+            if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':
+                yield stream.write(salt.transport.frame.frame_msg(
+                    self._auth(payload['load']), header=header))
+                raise tornado.gen.Return()
+
+            # TODO: test
+            try:
+                ret, req_opts = yield self.payload_handler(payload)
+            except Exception as e:
+                # always attempt to return an error to the minion
+                stream.write('Some exception handling minion payload')
+                log.error('Some exception handling a payload from minion', exc_info=True)
+                stream.close()
+                raise tornado.gen.Return()
+
+            req_fun = req_opts.get('fun', 'send')
+            if req_fun == 'send_clear':
+                stream.write(salt.transport.frame.frame_msg(ret, header=header))
+            elif req_fun == 'send':
+                stream.write(salt.transport.frame.frame_msg(self.crypticle.dumps(ret), header=header))
+            elif req_fun == 'send_private':
+                stream.write(salt.transport.frame.frame_msg(self._encrypt_private(ret,
+                                                             req_opts['key'],
+                                                             req_opts['tgt'],
+                                                             ), header=header))
+            else:
+                log.error('Unknown req_fun %s', req_fun)
+                # always attempt to return an error to the minion
+                stream.write('Server-side exception handling payload')
+                stream.close()
+        except tornado.gen.Return:
+            raise
+        except tornado.iostream.StreamClosedError:
+            # Stream was closed. This could happen if the remote side
+            # closed the connection on its end (eg in a timeout or shutdown
+            # situation).
+            log.error('Connection was unexpectedly closed', exc_info=True)
+        except Exception as exc:  # pylint: disable=broad-except
+            # Absorb any other exceptions
+            log.error('Unexpected exception occurred: %s', exc, exc_info=True)
+
+        raise tornado.gen.Return()
+
+
+class SaltMessageServer(tornado.tcpserver.TCPServer, object):
+    '''
+    Raw TCP server which will receive all of the TCP streams and re-assemble
+    messages that are sent through to us
+    '''
+    def __init__(self, message_handler, *args, **kwargs):
+        super(SaltMessageServer, self).__init__(*args, **kwargs)
+        self.io_loop = tornado.ioloop.IOLoop.current()
+
+        self.clients = []
+        self.message_handler = message_handler
+
+    @tornado.gen.coroutine
+    def handle_stream(self, stream, address):
+        '''
+        Handle incoming streams and add messages to the incoming queue
+        '''
+        log.trace('Req client %s connected', address)
+        self.clients.append((stream, address))
+        unpacker = msgpack.Unpacker()
+        try:
+            while True:
+                wire_bytes = yield stream.read_bytes(4096, partial=True)
+                unpacker.feed(wire_bytes)
+                for framed_msg in unpacker:
+                    if six.PY3:
+                        framed_msg = salt.transport.frame.decode_embedded_strs(
+                            framed_msg
+                        )
+                    header = framed_msg['head']
+                    self.io_loop.spawn_callback(self.message_handler, stream, header, framed_msg['body'])
+
+        except tornado.iostream.StreamClosedError:
+            log.trace('req client disconnected %s', address)
+            self.clients.remove((stream, address))
+        except Exception as e:
+            log.trace('other master-side exception: %s', e)
+            self.clients.remove((stream, address))
+            stream.close()
+
+    def shutdown(self):
+        '''
+        Shutdown the whole server
+        '''
+        for item in self.clients:
+            client, address = item
+            client.close()
+            self.clients.remove(item)
+
+
+if USE_LOAD_BALANCER:
+    class LoadBalancerWorker(SaltMessageServer):
+        '''
+        This will receive TCP connections from 'LoadBalancerServer' via
+        a multiprocessing queue.
+        Since the queue is shared amongst workers, only one worker will handle
+        a given connection.
+        '''
+        def __init__(self, socket_queue, message_handler, *args, **kwargs):
+            super(LoadBalancerWorker, self).__init__(
+                message_handler, *args, **kwargs)
+            self.socket_queue = socket_queue
+            self._stop = threading.Event()
+            self.thread = threading.Thread(target=self.socket_queue_thread)
+            self.thread.start()
+
+        def stop(self):
+            self._stop.set()
+            self.thread.join()
+
+        def socket_queue_thread(self):
+            try:
+                while True:
+                    try:
+                        client_socket, address = self.socket_queue.get(True, 1)
+                    except queue.Empty:
+                        if self._stop.is_set():
+                            break
+                        continue
+                    # 'self.io_loop' initialized in super class
+                    # 'tornado.tcpserver.TCPServer'.
+                    # 'self._handle_connection' defined in same super class.
+                    self.io_loop.spawn_callback(
+                        self._handle_connection, client_socket, address)
+            except (KeyboardInterrupt, SystemExit):
+                pass
+
+
+class TCPClientKeepAlive(tornado.tcpclient.TCPClient):
+    '''
+    Override _create_stream() in TCPClient to enable keep alive support.
+    '''
+    def __init__(self, opts, resolver=None):
+        self.opts = opts
+        super(TCPClientKeepAlive, self).__init__(resolver=resolver)
+
+    def _create_stream(self, max_buffer_size, af, addr, **kwargs):  # pylint: disable=unused-argument
+        '''
+        Override _create_stream() in TCPClient.
+
+        Tornado 4.5 added the kwargs 'source_ip' and 'source_port'.
+        Due to this, use **kwargs to swallow these and any future
+        kwargs to maintain compatibility.
+        '''
+        # Always connect in plaintext; we'll convert to ssl if necessary
+        # after one connection has completed.
+        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        _set_tcp_keepalive(sock, self.opts)
+        stream = tornado.iostream.IOStream(
+            sock,
+            max_buffer_size=max_buffer_size)
+        if tornado.version_info < (5,):
+            return stream.connect(addr)
+        return stream, stream.connect(addr)
+
+
+class SaltMessageClientPool(salt.transport.MessageClientPool):
+    '''
+    Wrapper class of SaltMessageClient to avoid blocking waiting while writing data to socket.
+    '''
+    def __init__(self, opts, args=None, kwargs=None):
+        super(SaltMessageClientPool, self).__init__(SaltMessageClient, opts, args=args, kwargs=kwargs)
+
+    def __del__(self):
+        self.close()
+
+    def close(self):
+        for message_client in self.message_clients:
+            message_client.close()
+        self.message_clients = []
+
+    @tornado.gen.coroutine
+    def connect(self):
+        futures = []
+        for message_client in self.message_clients:
+            futures.append(message_client.connect())
+        for future in futures:
+            yield future
+        raise tornado.gen.Return(None)
+
+    def on_recv(self, *args, **kwargs):
+        for message_client in self.message_clients:
+            message_client.on_recv(*args, **kwargs)
+
+    def send(self, *args, **kwargs):
+        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))
+        return message_clients[0].send(*args, **kwargs)
+
+    def write_to_stream(self, *args, **kwargs):
+        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))
+        return message_clients[0]._stream.write(*args, **kwargs)
+
+
+# TODO consolidate with IPCClient
+# TODO: limit in-flight messages.
+# TODO: singleton? Something to not re-create the tcp connection so much
+class SaltMessageClient(object):
+    '''
+    Low-level message sending client
+    '''
+    def __init__(self, opts, host, port, io_loop=None, resolver=None,
+                 connect_callback=None, disconnect_callback=None,
+                 source_ip=None, source_port=None):
+        self.opts = opts
+        self.host = host
+        self.port = port
+        self.source_ip = source_ip
+        self.source_port = source_port
+        self.connect_callback = connect_callback
+        self.disconnect_callback = disconnect_callback
+
+        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+
+        with salt.utils.asynchronous.current_ioloop(self.io_loop):
+            self._tcp_client = TCPClientKeepAlive(opts, resolver=resolver)
+
+        self._mid = 1
+        self._max_messages = int((1 << 31) - 2)  # number of IDs before we wrap
+
+        # TODO: max queue size
+        self.send_queue = []  # queue of messages to be sent
+        self.send_future_map = {}  # mapping of request_id -> Future
+        self.send_timeout_map = {}  # request_id -> timeout_callback
+
+        self._read_until_future = None
+        self._on_recv = None
+        self._closing = False
+        self._connecting_future = self.connect()
+        self._stream_return_future = tornado.concurrent.Future()
+        self.io_loop.spawn_callback(self._stream_return)
+
+    # TODO: timeout inflight sessions
+    def close(self):
+        if self._closing:
+            return
+        self._closing = True
+        if hasattr(self, '_stream') and not self._stream.closed():
+            # If _stream_return() hasn't completed, it means the IO
+            # Loop is stopped (such as when using
+            # 'salt.utils.asynchronous.SyncWrapper'). Ensure that
+            # _stream_return() completes by restarting the IO Loop.
+            # This will prevent potential errors on shutdown.
+            try:
+                orig_loop = tornado.ioloop.IOLoop.current()
+                self.io_loop.make_current()
+                self._stream.close()
+                if self._read_until_future is not None:
+                    # This will prevent this message from showing up:
+                    # '[ERROR   ] Future exception was never retrieved:
+                    # StreamClosedError'
+                    # This happens because the logic is always waiting to read
+                    # the next message and the associated read future is marked
+                    # 'StreamClosedError' when the stream is closed.
+                    if self._read_until_future.done():
+                        self._read_until_future.exception()
+                    elif self.io_loop != tornado.ioloop.IOLoop.current(instance=False):
+                        self.io_loop.add_future(
+                            self._stream_return_future,
+                            lambda future: self.io_loop.stop()
+                        )
+                        self.io_loop.start()
+            finally:
+                orig_loop.make_current()
+        self._tcp_client.close()
+        # Clear callback references to allow the object that they belong to
+        # to be deleted.
+        self.connect_callback = None
+        self.disconnect_callback = None
+
+    def __del__(self):
+        self.close()
+
+    def connect(self):
+        '''
+        Ask for this client to reconnect to the origin
+        '''
+        if hasattr(self, '_connecting_future') and not self._connecting_future.done():
+            future = self._connecting_future
+        else:
+            future = tornado.concurrent.Future()
+            self._connecting_future = future
+            self.io_loop.add_callback(self._connect)
+
+            # Add the callback only when a new future is created
+            if self.connect_callback is not None:
+                def handle_future(future):
+                    response = future.result()
+                    self.io_loop.add_callback(self.connect_callback, response)
+                future.add_done_callback(handle_future)
+
+        return future
+
+    # TODO: tcp backoff opts
+    @tornado.gen.coroutine
+    def _connect(self):
+        '''
+        Try to connect for the rest of time!
+        '''
+        while True:
+            if self._closing:
+                break
+            try:
+                kwargs = {}
+                if self.source_ip or self.source_port:
+                    if tornado.version_info >= (4, 5):
+                        ### source_ip and source_port are supported only in Tornado >= 4.5
+                        # See http://www.tornadoweb.org/en/stable/releases/v4.5.0.html
+                        # Otherwise will just ignore these args
+                        kwargs = {'source_ip': self.source_ip,
+                                  'source_port': self.source_port}
+                    else:
+                        log.warning('If you need a certain source IP/port, consider upgrading Tornado >= 4.5')
+                with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                    self._stream = yield self._tcp_client.connect(self.host,
+                                                                  self.port,
+                                                                  ssl_options=self.opts.get('ssl'),
+                                                                  **kwargs)
+                self._connecting_future.set_result(True)
+                break
+            except Exception as e:
+                yield tornado.gen.sleep(1)  # TODO: backoff
+                #self._connecting_future.set_exception(e)
+
+    @tornado.gen.coroutine
+    def _stream_return(self):
+        try:
+            while not self._closing and (
+                    not self._connecting_future.done() or
+                    self._connecting_future.result() is not True):
+                yield self._connecting_future
+            unpacker = msgpack.Unpacker()
+            while not self._closing:
+                try:
+                    self._read_until_future = self._stream.read_bytes(4096, partial=True)
+                    wire_bytes = yield self._read_until_future
+                    unpacker.feed(wire_bytes)
+                    for framed_msg in unpacker:
+                        if six.PY3:
+                            framed_msg = salt.transport.frame.decode_embedded_strs(
+                                framed_msg
+                            )
+                        header = framed_msg['head']
+                        body = framed_msg['body']
+                        message_id = header.get('mid')
+
+                        if message_id in self.send_future_map:
+                            self.send_future_map.pop(message_id).set_result(body)
+                            self.remove_message_timeout(message_id)
+                        else:
+                            if self._on_recv is not None:
+                                self.io_loop.spawn_callback(self._on_recv, header, body)
+                            else:
+                                log.error('Got response for message_id %s that we are not tracking', message_id)
+                except tornado.iostream.StreamClosedError as e:
+                    log.debug('tcp stream to %s:%s closed, unable to recv', self.host, self.port)
+                    for future in six.itervalues(self.send_future_map):
+                        future.set_exception(e)
+                    self.send_future_map = {}
+                    if self._closing:
+                        return
+                    if self.disconnect_callback:
+                        self.disconnect_callback()
+                    # if the last connect finished, then we need to make a new one
+                    if self._connecting_future.done():
+                        self._connecting_future = self.connect()
+                    yield self._connecting_future
+                except TypeError:
+                    # This is an invalid transport
+                    if 'detect_mode' in self.opts:
+                        log.info('There was an error trying to use TCP transport; '
+                                 'attempting to fallback to another transport')
+                    else:
+                        raise SaltClientError
+                except Exception as e:
+                    log.error('Exception parsing response', exc_info=True)
+                    for future in six.itervalues(self.send_future_map):
+                        future.set_exception(e)
+                    self.send_future_map = {}
+                    if self._closing:
+                        return
+                    if self.disconnect_callback:
+                        self.disconnect_callback()
+                    # if the last connect finished, then we need to make a new one
+                    if self._connecting_future.done():
+                        self._connecting_future = self.connect()
+                    yield self._connecting_future
+        finally:
+            self._stream_return_future.set_result(True)
+
+    @tornado.gen.coroutine
+    def _stream_send(self):
+        while not self._connecting_future.done() or self._connecting_future.result() is not True:
+            yield self._connecting_future
+        while len(self.send_queue) > 0:
+            message_id, item = self.send_queue[0]
+            try:
+                yield self._stream.write(item)
+                del self.send_queue[0]
+            # if the connection is dead, lets fail this send, and make sure we
+            # attempt to reconnect
+            except tornado.iostream.StreamClosedError as e:
+                if message_id in self.send_future_map:
+                    self.send_future_map.pop(message_id).set_exception(e)
+                self.remove_message_timeout(message_id)
+                del self.send_queue[0]
+                if self._closing:
+                    return
+                if self.disconnect_callback:
+                    self.disconnect_callback()
+                # if the last connect finished, then we need to make a new one
+                if self._connecting_future.done():
+                    self._connecting_future = self.connect()
+                yield self._connecting_future
+
+    def _message_id(self):
+        wrap = False
+        while self._mid in self.send_future_map:
+            if self._mid >= self._max_messages:
+                if wrap:
+                    # this shouldn't ever happen, but just in case
+                    raise Exception('Unable to find available messageid')
+                self._mid = 1
+                wrap = True
+            else:
+                self._mid += 1
+
+        return self._mid
+
+    # TODO: return a message object which takes care of multiplexing?
+    def on_recv(self, callback):
+        '''
+        Register a callback for received messages (that we didn't initiate)
+        '''
+        if callback is None:
+            self._on_recv = callback
+        else:
+            def wrap_recv(header, body):
+                callback(body)
+            self._on_recv = wrap_recv
+
+    def remove_message_timeout(self, message_id):
+        if message_id not in self.send_timeout_map:
+            return
+        timeout = self.send_timeout_map.pop(message_id)
+        self.io_loop.remove_timeout(timeout)
+
+    def timeout_message(self, message_id):
+        if message_id in self.send_timeout_map:
+            del self.send_timeout_map[message_id]
+        if message_id in self.send_future_map:
+            self.send_future_map.pop(message_id).set_exception(
+                SaltReqTimeoutError('Message timed out')
+            )
+
+    def send(self, msg, timeout=None, callback=None, raw=False):
+        '''
+        Send given message, and return a future
+        '''
+        message_id = self._message_id()
+        header = {'mid': message_id}
+
+        future = tornado.concurrent.Future()
+        if callback is not None:
+            def handle_future(future):
+                response = future.result()
+                self.io_loop.add_callback(callback, response)
+            future.add_done_callback(handle_future)
+        # Add this future to the mapping
+        self.send_future_map[message_id] = future
+
+        if self.opts.get('detect_mode') is True:
+            timeout = 1
+
+        if timeout is not None:
+            send_timeout = self.io_loop.call_later(timeout, self.timeout_message, message_id)
+            self.send_timeout_map[message_id] = send_timeout
+
+        # if we don't have a send queue, we need to spawn the callback to do the sending
+        if len(self.send_queue) == 0:
+            self.io_loop.spawn_callback(self._stream_send)
+        self.send_queue.append((message_id, salt.transport.frame.frame_msg(msg, header=header)))
+        return future
+
+
+class Subscriber(object):
+    '''
+    Client object for use with the TCP publisher server
+    '''
+    def __init__(self, stream, address):
+        self.stream = stream
+        self.address = address
+        self._closing = False
+        self._read_until_future = None
+        self.id_ = None
+
+    def close(self):
+        if self._closing:
+            return
+        self._closing = True
+        if not self.stream.closed():
+            self.stream.close()
+            if self._read_until_future is not None and self._read_until_future.done():
+                # This will prevent this message from showing up:
+                # '[ERROR   ] Future exception was never retrieved:
+                # StreamClosedError'
+                # This happens because the logic is always waiting to read
+                # the next message and the associated read future is marked
+                # 'StreamClosedError' when the stream is closed.
+                self._read_until_future.exception()
+
+    def __del__(self):
+        self.close()
+
+
+class PubServer(tornado.tcpserver.TCPServer, object):
+    '''
+    TCP publisher
+    '''
+    def __init__(self, opts, io_loop=None):
+        super(PubServer, self).__init__(ssl_options=opts.get('ssl'))
+        self.io_loop = io_loop
+        self.opts = opts
+        self._closing = False
+        self.clients = set()
+        self.aes_funcs = salt.master.AESFuncs(self.opts)
+        self.present = {}
+        self.presence_events = False
+        if self.opts.get('presence_events', False):
+            tcp_only = True
+            for transport, _ in iter_transport_opts(self.opts):
+                if transport != 'tcp':
+                    tcp_only = False
+            if tcp_only:
+                # Only when the transport is TCP only, the presence events will
+                # be handled here. Otherwise, it will be handled in the
+                # 'Maintenance' process.
+                self.presence_events = True
+
+        if self.presence_events:
+            self.event = salt.utils.event.get_event(
+                'master',
+                opts=self.opts,
+                listen=False
+            )
+
+    def close(self):
+        if self._closing:
+            return
+        self._closing = True
+
+    def __del__(self):
+        self.close()
+
+    def _add_client_present(self, client):
+        id_ = client.id_
+        if id_ in self.present:
+            clients = self.present[id_]
+            clients.add(client)
+        else:
+            self.present[id_] = {client}
+            if self.presence_events:
+                data = {'new': [id_],
+                        'lost': []}
+                self.event.fire_event(
+                    data,
+                    salt.utils.event.tagify('change', 'presence')
+                )
+                data = {'present': list(self.present.keys())}
+                self.event.fire_event(
+                    data,
+                    salt.utils.event.tagify('present', 'presence')
+                )
+
+    def _remove_client_present(self, client):
+        id_ = client.id_
+        if id_ is None or id_ not in self.present:
+            # This is possible if _remove_client_present() is invoked
+            # before the minion's id is validated.
+            return
+
+        clients = self.present[id_]
+        if client not in clients:
+            # Since _remove_client_present() is potentially called from
+            # _stream_read() and/or publish_payload(), it is possible for
+            # it to be called twice, in which case we will get here.
+            # This is not an abnormal case, so no logging is required.
+            return
+
+        clients.remove(client)
+        if len(clients) == 0:
+            del self.present[id_]
+            if self.presence_events:
+                data = {'new': [],
+                        'lost': [id_]}
+                self.event.fire_event(
+                    data,
+                    salt.utils.event.tagify('change', 'presence')
+                )
+                data = {'present': list(self.present.keys())}
+                self.event.fire_event(
+                    data,
+                    salt.utils.event.tagify('present', 'presence')
+                )
+
+    @tornado.gen.coroutine
+    def _stream_read(self, client):
+        unpacker = msgpack.Unpacker()
+        while not self._closing:
+            try:
+                client._read_until_future = client.stream.read_bytes(4096, partial=True)
+                wire_bytes = yield client._read_until_future
+                unpacker.feed(wire_bytes)
+                for framed_msg in unpacker:
+                    if six.PY3:
+                        framed_msg = salt.transport.frame.decode_embedded_strs(
+                            framed_msg
+                        )
+                    body = framed_msg['body']
+                    if body['enc'] != 'aes':
+                        # We only accept 'aes' encoded messages for 'id'
+                        continue
+                    crypticle = salt.crypt.Crypticle(self.opts, salt.master.SMaster.secrets['aes']['secret'].value)
+                    load = crypticle.loads(body['load'])
+                    if six.PY3:
+                        load = salt.transport.frame.decode_embedded_strs(load)
+                    if not self.aes_funcs.verify_minion(load['id'], load['tok']):
+                        continue
+                    client.id_ = load['id']
+                    self._add_client_present(client)
+            except tornado.iostream.StreamClosedError as e:
+                log.debug('tcp stream to %s closed, unable to recv', client.address)
+                client.close()
+                self._remove_client_present(client)
+                self.clients.discard(client)
+                break
+            except Exception as e:
+                log.error('Exception parsing response', exc_info=True)
+                continue
+
+    def handle_stream(self, stream, address):
+        log.trace('Subscriber at %s connected', address)
+        client = Subscriber(stream, address)
+        self.clients.add(client)
+        self.io_loop.spawn_callback(self._stream_read, client)
+
+    # TODO: ACK the publish through IPC
+    @tornado.gen.coroutine
+    def publish_payload(self, package, _):
+        log.debug('TCP PubServer sending payload: %s', package)
+        payload = salt.transport.frame.frame_msg(package['payload'])
+
+        to_remove = []
+        if 'topic_lst' in package:
+            topic_lst = package['topic_lst']
+            for topic in topic_lst:
+                if topic in self.present:
+                    # This will rarely be a list of more than 1 item. It will
+                    # be more than 1 item if the minion disconnects from the
+                    # master in an unclean manner (eg cable yank), then
+                    # restarts and the master is yet to detect the disconnect
+                    # via TCP keep-alive.
+                    for client in self.present[topic]:
+                        try:
+                            # Write the packed str
+                            f = client.stream.write(payload)
+                            self.io_loop.add_future(f, lambda f: True)
+                        except tornado.iostream.StreamClosedError:
+                            to_remove.append(client)
+                else:
+                    log.debug('Publish target %s not connected', topic)
+        else:
+            for client in self.clients:
+                try:
+                    # Write the packed str
+                    f = client.stream.write(payload)
+                    self.io_loop.add_future(f, lambda f: True)
+                except tornado.iostream.StreamClosedError:
+                    to_remove.append(client)
+        for client in to_remove:
+            log.debug('Subscriber at %s has disconnected from publisher', client.address)
+            client.close()
+            self._remove_client_present(client)
+            self.clients.discard(client)
+        log.trace('TCP PubServer finished publishing payload')
+
+
+class TCPPubServerChannel(salt.transport.server.PubServerChannel):
+    # TODO: opts!
+    # Based on default used in tornado.netutil.bind_sockets()
+    backlog = 128
+
+    def __init__(self, opts):
+        self.opts = opts
+        self.serial = salt.payload.Serial(self.opts)  # TODO: in init?
+        self.ckminions = salt.utils.minions.CkMinions(opts)
+        self.io_loop = None
+
+    def __setstate__(self, state):
+        salt.master.SMaster.secrets = state['secrets']
+        self.__init__(state['opts'])
+
+    def __getstate__(self):
+        return {'opts': self.opts,
+                'secrets': salt.master.SMaster.secrets}
+
+    def _publish_daemon(self, **kwargs):
+        '''
+        Bind to the interface specified in the configuration file
+        '''
+        salt.utils.process.appendproctitle(self.__class__.__name__)
+
+        log_queue = kwargs.get('log_queue')
+        if log_queue is not None:
+            salt.log.setup.set_multiprocessing_logging_queue(log_queue)
+        log_queue_level = kwargs.get('log_queue_level')
+        if log_queue_level is not None:
+            salt.log.setup.set_multiprocessing_logging_level(log_queue_level)
+        salt.log.setup.setup_multiprocessing_logging(log_queue)
+
+        # Check if io_loop was set outside
+        if self.io_loop is None:
+            self.io_loop = tornado.ioloop.IOLoop.current()
+
+        # Spin up the publisher
+        pub_server = PubServer(self.opts, io_loop=self.io_loop)
+        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        _set_tcp_keepalive(sock, self.opts)
+        sock.setblocking(0)
+        sock.bind((self.opts['interface'], int(self.opts['publish_port'])))
+        sock.listen(self.backlog)
+        # pub_server will take ownership of the socket
+        pub_server.add_socket(sock)
+
+        # Set up Salt IPC server
+        if self.opts.get('ipc_mode', '') == 'tcp':
+            pull_uri = int(self.opts.get('tcp_master_publish_pull', 4514))
+        else:
+            pull_uri = os.path.join(self.opts['sock_dir'], 'publish_pull.ipc')
+
+        pull_sock = salt.transport.ipc.IPCMessageServer(
+            pull_uri,
+            io_loop=self.io_loop,
+            payload_handler=pub_server.publish_payload,
+        )
+
+        # Securely create socket
+        log.info('Starting the Salt Puller on %s', pull_uri)
+        with salt.utils.files.set_umask(0o177):
+            pull_sock.start()
+
+        # run forever
+        try:
+            self.io_loop.start()
+        except (KeyboardInterrupt, SystemExit):
+            salt.log.setup.shutdown_multiprocessing_logging()
+
+    def pre_fork(self, process_manager, kwargs=None):
+        '''
+        Do anything necessary pre-fork. Since this is on the master side this will
+        primarily be used to create IPC channels and create our daemon process to
+        do the actual publishing
+        '''
+        process_manager.add_process(self._publish_daemon, kwargs=kwargs)
+
+    def publish(self, load):
+        '''
+        Publish "load" to minions
+        '''
+        payload = {'enc': 'aes'}
+
+        crypticle = salt.crypt.Crypticle(self.opts, salt.master.SMaster.secrets['aes']['secret'].value)
+        payload['load'] = crypticle.dumps(load)
+        if self.opts['sign_pub_messages']:
+            master_pem_path = os.path.join(self.opts['pki_dir'], 'master.pem')
+            log.debug("Signing data packet")
+            payload['sig'] = salt.crypt.sign_message(master_pem_path, payload['load'])
+        # Use the Salt IPC server
+        if self.opts.get('ipc_mode', '') == 'tcp':
+            pull_uri = int(self.opts.get('tcp_master_publish_pull', 4514))
+        else:
+            pull_uri = os.path.join(self.opts['sock_dir'], 'publish_pull.ipc')
+        # TODO: switch to the actual asynchronous interface
+        #pub_sock = salt.transport.ipc.IPCMessageClient(self.opts, io_loop=self.io_loop)
+        pub_sock = salt.utils.asynchronous.SyncWrapper(
+            salt.transport.ipc.IPCMessageClient,
+            (pull_uri,)
+        )
+        pub_sock.connect()
+
+        int_payload = {'payload': self.serial.dumps(payload)}
+
+        # add some targeting stuff for lists only (for now)
+        if load['tgt_type'] == 'list':
+            if isinstance(load['tgt'], six.string_types):
+                # Fetch a list of minions that match
+                _res = self.ckminions.check_minions(load['tgt'],
+                                                    tgt_type=load['tgt_type'])
+                match_ids = _res['minions']
+
+                log.debug("Publish Side Match: %s", match_ids)
+                # Send list of miions thru so zmq can target them
+                int_payload['topic_lst'] = match_ids
+            else:
+                int_payload['topic_lst'] = load['tgt']
+        # Send it over IPC!
+        pub_sock.send(int_payload)
diff -Naur a/salt/transport/zeromq.py c/salt/transport/zeromq.py
--- a/salt/transport/zeromq.py	2019-07-02 10:15:07.051874718 -0600
+++ c/salt/transport/zeromq.py	2019-07-02 11:03:09.563946191 -0600
@@ -49,9 +49,14 @@
     HAS_ZMQ_MONITOR = False
 
 # Import Tornado Libs
-import tornado
-import tornado.gen
-import tornado.concurrent
+try:
+    import tornado4
+    import tornado4.gen as tornado_gen
+    from tornado4.concurrent import Future as TornadoFuture
+except ImportError:
+    import tornado
+    import tornado.gen as tornado_gen
+    from tornado.concurrent import Future as TornadoFuture
 
 # Import third party libs
 try:
@@ -279,7 +284,7 @@
             'load': load,
         }
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def crypted_transfer_decode_dictentry(self, load, dictkey=None, tries=3, timeout=60):
         if not self.auth.authenticated:
             # Return control back to the caller, continue when authentication succeeds
@@ -309,9 +314,9 @@
         data = pcrypt.loads(ret[dictkey])
         if six.PY3:
             data = salt.transport.frame.decode_embedded_strs(data)
-        raise tornado.gen.Return(data)
+        raise tornado_gen.Return(data)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _crypted_transfer(self, load, tries=3, timeout=60, raw=False):
         '''
         Send a load across the wire, with encryption
@@ -326,7 +331,7 @@
         :param int tries: The number of times to make before failure
         :param int timeout: The number of seconds on a response before failing
         '''
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def _do_transfer():
             # Yield control to the caller. When send() completes, resume by populating data with the Future.result
             data = yield self.message_client.send(
@@ -342,7 +347,7 @@
                 data = self.auth.crypticle.loads(data, raw)
             if six.PY3 and not raw:
                 data = salt.transport.frame.decode_embedded_strs(data)
-            raise tornado.gen.Return(data)
+            raise tornado_gen.Return(data)
         if not self.auth.authenticated:
             # Return control back to the caller, resume when authentication succeeds
             yield self.auth.authenticate()
@@ -353,9 +358,9 @@
             # If auth error, return control back to the caller, continue when authentication succeeds
             yield self.auth.authenticate()
             ret = yield _do_transfer()
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _uncrypted_transfer(self, load, tries=3, timeout=60):
         '''
         Send a load across the wire in cleartext
@@ -370,9 +375,9 @@
             tries=tries,
         )
 
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def send(self, load, tries=3, timeout=60, raw=False):
         '''
         Send a request, return a future which will complete when we send the message
@@ -381,7 +386,7 @@
             ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)
         else:
             ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout, raw=raw)
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
 
 class AsyncZeroMQPubChannel(salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel):
@@ -501,7 +506,7 @@
         self.close()
 
     # TODO: this is the time to see if we are connected, maybe use the req channel to guess?
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def connect(self):
         if not self.auth.authenticated:
             yield self.auth.authenticate()
@@ -519,7 +524,7 @@
                                source_ip=self.opts.get('source_ip'),
                                source_port=self.opts.get('source_publish_port'))
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _decode_messages(self, messages):
         '''
         Take the zmq messages, decrypt/decode them into a payload
@@ -536,7 +541,7 @@
             if (self.opts.get('__role') != 'syndic' and message_target not in ('broadcast', self.hexid)) or \
                 (self.opts.get('__role') == 'syndic' and message_target not in ('broadcast', 'syndic')):
                 log.debug('Publish received for not this minion: %s', message_target)
-                raise tornado.gen.Return(None)
+                raise tornado_gen.Return(None)
             payload = self.serial.loads(messages[1])
         else:
             raise Exception(('Invalid number of messages ({0}) in zeromq pub'
@@ -544,7 +549,7 @@
         # Yield control back to the caller. When the payload has been decoded, assign
         # the decoded payload to 'ret' and resume operation
         ret = yield self._decode_payload(payload)
-        raise tornado.gen.Return(ret)
+        raise tornado_gen.Return(ret)
 
     @property
     def stream(self):
@@ -564,7 +569,7 @@
         if callback is None:
             return self.stream.on_recv(None)
 
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def wrap_callback(messages):
             payload = yield self._decode_messages(messages)
             if payload is not None:
@@ -704,7 +709,7 @@
         self.stream = zmq.eventloop.zmqstream.ZMQStream(self._socket, io_loop=self.io_loop)
         self.stream.on_recv_stream(self.handle_message)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def handle_message(self, stream, payload):
         '''
         Handle incoming messages from underlying TCP streams
@@ -729,30 +734,30 @@
             else:
                 log.error('Bad load from minion: %s: %s', exc_type, exc)
             stream.send(self.serial.dumps('bad load'))
-            raise tornado.gen.Return()
+            raise tornado_gen.Return()
 
         # TODO helper functions to normalize payload?
         if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):
             log.error('payload and load must be a dict. Payload was: %s and load was %s', payload, payload.get('load'))
             stream.send(self.serial.dumps('payload and load must be a dict'))
-            raise tornado.gen.Return()
+            raise tornado_gen.Return()
 
         try:
             id_ = payload['load'].get('id', '')
             if str('\0') in id_:
                 log.error('Payload contains an id with a null byte: %s', payload)
                 stream.send(self.serial.dumps('bad load: id contains a null byte'))
-                raise tornado.gen.Return()
+                raise tornado_gen.Return()
         except TypeError:
             log.error('Payload contains non-string id: %s', payload)
             stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))
-            raise tornado.gen.Return()
+            raise tornado_gen.Return()
 
         # intercept the "_auth" commands, since the main daemon shouldn't know
         # anything about our key auth
         if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':
             stream.send(self.serial.dumps(self._auth(payload['load'])))
-            raise tornado.gen.Return()
+            raise tornado_gen.Return()
 
         # TODO: test
         try:
@@ -763,7 +768,7 @@
             # always attempt to return an error to the minion
             stream.send('Some exception handling minion payload')
             log.error('Some exception handling a payload from minion', exc_info=True)
-            raise tornado.gen.Return()
+            raise tornado_gen.Return()
 
         req_fun = req_opts.get('fun', 'send')
         if req_fun == 'send_clear':
@@ -779,7 +784,7 @@
             log.error('Unknown req_fun %s', req_fun)
             # always attempt to return an error to the minion
             stream.send('Server-side exception handling payload')
-        raise tornado.gen.Return()
+        raise tornado_gen.Return()
 
     def __setup_signals(self):
         signal.signal(signal.SIGINT, self._handle_signals)
@@ -842,7 +847,7 @@
         self.ckminions = salt.utils.minions.CkMinions(self.opts)
 
     def connect(self):
-        return tornado.gen.sleep(5)
+        return tornado_gen.sleep(5)
 
     def _publish_daemon(self, log_queue=None):
         '''
@@ -1190,7 +1195,7 @@
         self.socket.connect(self.addr)
         self.stream = zmq.eventloop.zmqstream.ZMQStream(self.socket, io_loop=self.io_loop)
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _internal_send_recv(self):
         while len(self.send_queue) > 0:
             message = self.send_queue[0]
@@ -1257,7 +1262,7 @@
         Return a future which will be completed when the message has a response
         '''
         if future is None:
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             future.tries = tries
             future.attempts = 0
             future.timeout = timeout
diff -Naur a/salt/transport/zeromq.py.orig c/salt/transport/zeromq.py.orig
--- a/salt/transport/zeromq.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/transport/zeromq.py.orig	2019-07-02 10:57:25.415937658 -0600
@@ -0,0 +1,1346 @@
+# -*- coding: utf-8 -*-
+'''
+Zeromq transport classes
+'''
+
+# Import Python Libs
+from __future__ import absolute_import, print_function, unicode_literals
+import os
+import sys
+import copy
+import errno
+import signal
+import socket
+import hashlib
+import logging
+import weakref
+import threading
+from random import randint
+
+# Import Salt Libs
+import salt.auth
+import salt.crypt
+import salt.log.setup
+import salt.utils.event
+import salt.utils.files
+import salt.utils.minions
+import salt.utils.process
+import salt.utils.stringutils
+import salt.utils.verify
+import salt.utils.zeromq
+import salt.utils.versions
+import salt.payload
+import salt.transport.client
+import salt.transport.server
+import salt.transport.mixins.auth
+from salt.ext import six
+from salt.exceptions import SaltReqTimeoutError, SaltException
+from salt._compat import ipaddress
+
+from salt.utils.zeromq import zmq, ZMQDefaultLoop, install_zmq, ZMQ_VERSION_INFO, LIBZMQ_VERSION_INFO
+import zmq.error
+import zmq.eventloop.ioloop
+import zmq.eventloop.zmqstream
+
+try:
+    import zmq.utils.monitor
+    HAS_ZMQ_MONITOR = True
+except ImportError:
+    HAS_ZMQ_MONITOR = False
+
+# Import Tornado Libs
+import tornado
+import tornado.gen
+import tornado.concurrent
+
+# Import third party libs
+try:
+    from M2Crypto import RSA
+    HAS_M2 = True
+except ImportError:
+    HAS_M2 = False
+    try:
+        from Cryptodome.Cipher import PKCS1_OAEP
+    except ImportError:
+        from Crypto.Cipher import PKCS1_OAEP
+
+log = logging.getLogger(__name__)
+
+
+def _get_master_uri(master_ip,
+                    master_port,
+                    source_ip=None,
+                    source_port=None):
+    '''
+    Return the ZeroMQ URI to connect the Minion to the Master.
+    It supports different source IP / port, given the ZeroMQ syntax:
+    // Connecting using a IP address and bind to an IP address
+    rc = zmq_connect(socket, "tcp://192.168.1.17:5555;192.168.1.1:5555"); assert (rc == 0);
+    Source: http://api.zeromq.org/4-1:zmq-tcp
+    '''
+    from salt.utils.zeromq import ip_bracket
+
+    master_uri = 'tcp://{master_ip}:{master_port}'.format(
+                  master_ip=ip_bracket(master_ip), master_port=master_port)
+
+    if source_ip or source_port:
+        if LIBZMQ_VERSION_INFO >= (4, 1, 6) and ZMQ_VERSION_INFO >= (16, 0, 1):
+            # The source:port syntax for ZeroMQ has been added in libzmq 4.1.6
+            # which is included in the pyzmq wheels starting with 16.0.1.
+            if source_ip and source_port:
+                master_uri = 'tcp://{source_ip}:{source_port};{master_ip}:{master_port}'.format(
+                             source_ip=ip_bracket(source_ip), source_port=source_port,
+                             master_ip=ip_bracket(master_ip), master_port=master_port)
+            elif source_ip and not source_port:
+                master_uri = 'tcp://{source_ip}:0;{master_ip}:{master_port}'.format(
+                             source_ip=ip_bracket(source_ip),
+                             master_ip=ip_bracket(master_ip), master_port=master_port)
+            elif source_port and not source_ip:
+                ip_any = '0.0.0.0' if ipaddress.ip_address(master_ip).version == 4 else ip_bracket('::')
+                master_uri = 'tcp://{ip_any}:{source_port};{master_ip}:{master_port}'.format(
+                             ip_any=ip_any, source_port=source_port,
+                             master_ip=ip_bracket(master_ip), master_port=master_port)
+        else:
+            log.warning('Unable to connect to the Master using a specific source IP / port')
+            log.warning('Consider upgrading to pyzmq >= 16.0.1 and libzmq >= 4.1.6')
+            log.warning('Specific source IP / port for connecting to master returner port: configuraion ignored')
+
+    return master_uri
+
+
+class AsyncZeroMQReqChannel(salt.transport.client.ReqChannel):
+    '''
+    Encapsulate sending routines to ZeroMQ.
+
+    ZMQ Channels default to 'crypt=aes'
+    '''
+    # This class is only a singleton per minion/master pair
+    # mapping of io_loop -> {key -> channel}
+    instance_map = weakref.WeakKeyDictionary()
+
+    def __new__(cls, opts, **kwargs):
+        '''
+        Only create one instance of channel per __key()
+        '''
+
+        # do we have any mapping for this io_loop
+        io_loop = kwargs.get('io_loop')
+        if io_loop is None:
+            install_zmq()
+            io_loop = ZMQDefaultLoop.current()
+        if io_loop not in cls.instance_map:
+            cls.instance_map[io_loop] = weakref.WeakValueDictionary()
+        loop_instance_map = cls.instance_map[io_loop]
+
+        key = cls.__key(opts, **kwargs)
+        obj = loop_instance_map.get(key)
+        if obj is None:
+            log.debug('Initializing new AsyncZeroMQReqChannel for %s', key)
+            # we need to make a local variable for this, as we are going to store
+            # it in a WeakValueDictionary-- which will remove the item if no one
+            # references it-- this forces a reference while we return to the caller
+            obj = object.__new__(cls)
+            obj.__singleton_init__(opts, **kwargs)
+            obj._instance_key = key
+            loop_instance_map[key] = obj
+            obj._refcount = 1
+            obj._refcount_lock = threading.RLock()
+            log.trace('Inserted key into loop_instance_map id %s for key %s and process %s',
+                      id(loop_instance_map), key, os.getpid())
+        else:
+            with obj._refcount_lock:
+                obj._refcount += 1
+            log.debug('Re-using AsyncZeroMQReqChannel for %s', key)
+        return obj
+
+    def __deepcopy__(self, memo):
+        cls = self.__class__
+        result = cls.__new__(cls, copy.deepcopy(self.opts, memo))  # pylint: disable=too-many-function-args
+        memo[id(self)] = result
+        for key in self.__dict__:
+            if key in ('_io_loop', '_refcount', '_refcount_lock'):
+                continue
+                # The _io_loop has a thread Lock which will fail to be deep
+                # copied. Skip it because it will just be recreated on the
+                # new copy.
+            if key == 'message_client':
+                # Recreate the message client because it will fail to be deep
+                # copied. The reason is the same as the io_loop skip above.
+                setattr(result, key,
+                        AsyncReqMessageClientPool(result.opts,
+                                                  args=(result.opts, self.master_uri,),
+                                                  kwargs={'io_loop': self._io_loop}))
+
+                continue
+            setattr(result, key, copy.deepcopy(self.__dict__[key], memo))
+        return result
+
+    @classmethod
+    def __key(cls, opts, **kwargs):
+        return (opts['pki_dir'],     # where the keys are stored
+                opts['id'],          # minion ID
+                kwargs.get('master_uri', opts.get('master_uri')),  # master ID
+                kwargs.get('crypt', 'aes'),  # TODO: use the same channel for crypt
+                )
+
+    # has to remain empty for singletons, since __init__ will *always* be called
+    def __init__(self, opts, **kwargs):
+        pass
+
+    # an init for the singleton instance to call
+    def __singleton_init__(self, opts, **kwargs):
+        self.opts = dict(opts)
+        self.ttype = 'zeromq'
+
+        # crypt defaults to 'aes'
+        self.crypt = kwargs.get('crypt', 'aes')
+
+        if 'master_uri' in kwargs:
+            self.opts['master_uri'] = kwargs['master_uri']
+
+        self._io_loop = kwargs.get('io_loop')
+        if self._io_loop is None:
+            install_zmq()
+            self._io_loop = ZMQDefaultLoop.current()
+
+        if self.crypt != 'clear':
+            # we don't need to worry about auth as a kwarg, since its a singleton
+            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self._io_loop)
+        log.debug('Connecting the Minion to the Master URI (for the return server): %s', self.master_uri)
+        self.message_client = AsyncReqMessageClientPool(self.opts,
+                                                        args=(self.opts, self.master_uri,),
+                                                        kwargs={'io_loop': self._io_loop})
+        self._closing = False
+
+    def close(self):
+        '''
+        Since the message_client creates sockets and assigns them to the IOLoop we have to
+        specifically destroy them, since we aren't the only ones with references to the FDs
+        '''
+        if self._closing:
+            return
+
+        if self._refcount > 1:
+            # Decrease refcount
+            with self._refcount_lock:
+                self._refcount -= 1
+            log.debug(
+                'This is not the last %s instance. Not closing yet.',
+                self.__class__.__name__
+            )
+            return
+
+        log.debug('Closing %s instance', self.__class__.__name__)
+        self._closing = True
+        if hasattr(self, 'message_client'):
+            self.message_client.close()
+
+        # Remove the entry from the instance map so that a closed entry may not
+        # be reused.
+        # This forces this operation even if the reference count of the entry
+        # has not yet gone to zero.
+        if self._io_loop in self.__class__.instance_map:
+            loop_instance_map = self.__class__.instance_map[self._io_loop]
+            if self._instance_key in loop_instance_map:
+                del loop_instance_map[self._instance_key]
+            if not loop_instance_map:
+                del self.__class__.instance_map[self._io_loop]
+
+    def __del__(self):
+        with self._refcount_lock:
+            # Make sure we actually close no matter if something
+            # went wrong with our ref counting
+            self._refcount = 1
+        try:
+            self.close()
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+
+    @property
+    def master_uri(self):
+        if 'master_uri' in self.opts:
+            return self.opts['master_uri']
+
+        # if by chance master_uri is not there..
+        if 'master_ip' in self.opts:
+            return _get_master_uri(self.opts['master_ip'],
+                                   self.opts['master_port'],
+                                   source_ip=self.opts.get('source_ip'),
+                                   source_port=self.opts.get('source_ret_port'))
+
+        # if we've reached here something is very abnormal
+        raise SaltException('ReqChannel: missing master_uri/master_ip in self.opts')
+
+    def _package_load(self, load):
+        return {
+            'enc': self.crypt,
+            'load': load,
+        }
+
+    @tornado.gen.coroutine
+    def crypted_transfer_decode_dictentry(self, load, dictkey=None, tries=3, timeout=60):
+        if not self.auth.authenticated:
+            # Return control back to the caller, continue when authentication succeeds
+            yield self.auth.authenticate()
+        # Return control to the caller. When send() completes, resume by populating ret with the Future.result
+        ret = yield self.message_client.send(
+            self._package_load(self.auth.crypticle.dumps(load)),
+            timeout=timeout,
+            tries=tries,
+        )
+        key = self.auth.get_keys()
+        if 'key' not in ret:
+            # Reauth in the case our key is deleted on the master side.
+            yield self.auth.authenticate()
+            ret = yield self.message_client.send(
+                self._package_load(self.auth.crypticle.dumps(load)),
+                timeout=timeout,
+                tries=tries,
+            )
+        if HAS_M2:
+            aes = key.private_decrypt(ret['key'],
+                                      RSA.pkcs1_oaep_padding)
+        else:
+            cipher = PKCS1_OAEP.new(key)
+            aes = cipher.decrypt(ret['key'])
+        pcrypt = salt.crypt.Crypticle(self.opts, aes)
+        data = pcrypt.loads(ret[dictkey])
+        if six.PY3:
+            data = salt.transport.frame.decode_embedded_strs(data)
+        raise tornado.gen.Return(data)
+
+    @tornado.gen.coroutine
+    def _crypted_transfer(self, load, tries=3, timeout=60, raw=False):
+        '''
+        Send a load across the wire, with encryption
+
+        In case of authentication errors, try to renegotiate authentication
+        and retry the method.
+
+        Indeed, we can fail too early in case of a master restart during a
+        minion state execution call
+
+        :param dict load: A load to send across the wire
+        :param int tries: The number of times to make before failure
+        :param int timeout: The number of seconds on a response before failing
+        '''
+        @tornado.gen.coroutine
+        def _do_transfer():
+            # Yield control to the caller. When send() completes, resume by populating data with the Future.result
+            data = yield self.message_client.send(
+                self._package_load(self.auth.crypticle.dumps(load)),
+                timeout=timeout,
+                tries=tries,
+            )
+            # we may not have always data
+            # as for example for saltcall ret submission, this is a blind
+            # communication, we do not subscribe to return events, we just
+            # upload the results to the master
+            if data:
+                data = self.auth.crypticle.loads(data, raw)
+            if six.PY3 and not raw:
+                data = salt.transport.frame.decode_embedded_strs(data)
+            raise tornado.gen.Return(data)
+        if not self.auth.authenticated:
+            # Return control back to the caller, resume when authentication succeeds
+            yield self.auth.authenticate()
+        try:
+            # We did not get data back the first time. Retry.
+            ret = yield _do_transfer()
+        except salt.crypt.AuthenticationError:
+            # If auth error, return control back to the caller, continue when authentication succeeds
+            yield self.auth.authenticate()
+            ret = yield _do_transfer()
+        raise tornado.gen.Return(ret)
+
+    @tornado.gen.coroutine
+    def _uncrypted_transfer(self, load, tries=3, timeout=60):
+        '''
+        Send a load across the wire in cleartext
+
+        :param dict load: A load to send across the wire
+        :param int tries: The number of times to make before failure
+        :param int timeout: The number of seconds on a response before failing
+        '''
+        ret = yield self.message_client.send(
+            self._package_load(load),
+            timeout=timeout,
+            tries=tries,
+        )
+
+        raise tornado.gen.Return(ret)
+
+    @tornado.gen.coroutine
+    def send(self, load, tries=3, timeout=60, raw=False):
+        '''
+        Send a request, return a future which will complete when we send the message
+        '''
+        if self.crypt == 'clear':
+            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)
+        else:
+            ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout, raw=raw)
+        raise tornado.gen.Return(ret)
+
+
+class AsyncZeroMQPubChannel(salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel):
+    '''
+    A transport channel backed by ZeroMQ for a Salt Publisher to use to
+    publish commands to connected minions
+    '''
+    def __init__(self,
+                 opts,
+                 **kwargs):
+        self.opts = opts
+        self.ttype = 'zeromq'
+        self.io_loop = kwargs.get('io_loop')
+
+        if self.io_loop is None:
+            install_zmq()
+            self.io_loop = ZMQDefaultLoop.current()
+
+        self.hexid = hashlib.sha1(salt.utils.stringutils.to_bytes(self.opts['id'])).hexdigest()
+        self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)
+        self.serial = salt.payload.Serial(self.opts)
+        self.context = zmq.Context()
+        self._socket = self.context.socket(zmq.SUB)
+
+        if self.opts['zmq_filtering']:
+            # TODO: constants file for "broadcast"
+            self._socket.setsockopt(zmq.SUBSCRIBE, b'broadcast')
+            if self.opts.get('__role') == 'syndic':
+                self._socket.setsockopt(zmq.SUBSCRIBE, b'syndic')
+            else:
+                self._socket.setsockopt(
+                    zmq.SUBSCRIBE,
+                    salt.utils.stringutils.to_bytes(self.hexid)
+                )
+        else:
+            self._socket.setsockopt(zmq.SUBSCRIBE, b'')
+
+        self._socket.setsockopt(zmq.IDENTITY, salt.utils.stringutils.to_bytes(self.opts['id']))
+
+        # TODO: cleanup all the socket opts stuff
+        if hasattr(zmq, 'TCP_KEEPALIVE'):
+            self._socket.setsockopt(
+                zmq.TCP_KEEPALIVE, self.opts['tcp_keepalive']
+            )
+            self._socket.setsockopt(
+                zmq.TCP_KEEPALIVE_IDLE, self.opts['tcp_keepalive_idle']
+            )
+            self._socket.setsockopt(
+                zmq.TCP_KEEPALIVE_CNT, self.opts['tcp_keepalive_cnt']
+            )
+            self._socket.setsockopt(
+                zmq.TCP_KEEPALIVE_INTVL, self.opts['tcp_keepalive_intvl']
+            )
+
+        recon_delay = self.opts['recon_default']
+
+        if self.opts['recon_randomize']:
+            recon_delay = randint(self.opts['recon_default'],
+                                  self.opts['recon_default'] + self.opts['recon_max'])
+
+            log.debug(
+                "Generated random reconnect delay between '%sms' and '%sms' (%s)",
+                self.opts['recon_default'],
+                self.opts['recon_default'] + self.opts['recon_max'],
+                recon_delay
+            )
+
+        log.debug("Setting zmq_reconnect_ivl to '%sms'", recon_delay)
+        self._socket.setsockopt(zmq.RECONNECT_IVL, recon_delay)
+
+        if hasattr(zmq, 'RECONNECT_IVL_MAX'):
+            log.debug(
+                "Setting zmq_reconnect_ivl_max to '%sms'",
+                self.opts['recon_default'] + self.opts['recon_max']
+            )
+
+            self._socket.setsockopt(
+                zmq.RECONNECT_IVL_MAX, self.opts['recon_max']
+            )
+
+        if (self.opts['ipv6'] is True or ':' in self.opts['master_ip']) and hasattr(zmq, 'IPV4ONLY'):
+            # IPv6 sockets work for both IPv6 and IPv4 addresses
+            self._socket.setsockopt(zmq.IPV4ONLY, 0)
+
+        if HAS_ZMQ_MONITOR and self.opts['zmq_monitor']:
+            self._monitor = ZeroMQSocketMonitor(self._socket)
+            self._monitor.start_io_loop(self.io_loop)
+
+    def close(self):
+        if hasattr(self, '_monitor') and self._monitor is not None:
+            self._monitor.stop()
+            self._monitor = None
+        if hasattr(self, '_stream'):
+            if ZMQ_VERSION_INFO < (14, 3, 0):
+                # stream.close() doesn't work properly on pyzmq < 14.3.0
+                self._stream.io_loop.remove_handler(self._stream.socket)
+                self._stream.socket.close(0)
+            else:
+                self._stream.close(0)
+        elif hasattr(self, '_socket'):
+            self._socket.close(0)
+        if hasattr(self, 'context') and self.context.closed is False:
+            self.context.term()
+
+    def destroy(self):
+        # Bacwards compat
+        salt.utils.versions.warn_until(
+            'Sodium',
+            'Calling {0}.destroy() is deprecated. Please call {0}.close() instead.'.format(
+                self.__class__.__name__
+            ),
+            stacklevel=3
+        )
+        self.close()
+
+    def __del__(self):
+        self.close()
+
+    # TODO: this is the time to see if we are connected, maybe use the req channel to guess?
+    @tornado.gen.coroutine
+    def connect(self):
+        if not self.auth.authenticated:
+            yield self.auth.authenticate()
+        self.publish_port = self.auth.creds['publish_port']
+        log.debug('Connecting the Minion to the Master publish port, using the URI: %s', self.master_pub)
+        self._socket.connect(self.master_pub)
+
+    @property
+    def master_pub(self):
+        '''
+        Return the master publish port
+        '''
+        return _get_master_uri(self.opts['master_ip'],
+                               self.publish_port,
+                               source_ip=self.opts.get('source_ip'),
+                               source_port=self.opts.get('source_publish_port'))
+
+    @tornado.gen.coroutine
+    def _decode_messages(self, messages):
+        '''
+        Take the zmq messages, decrypt/decode them into a payload
+
+        :param list messages: A list of messages to be decoded
+        '''
+        messages_len = len(messages)
+        # if it was one message, then its old style
+        if messages_len == 1:
+            payload = self.serial.loads(messages[0])
+        # 2 includes a header which says who should do it
+        elif messages_len == 2:
+            message_target = salt.utils.stringutils.to_str(messages[0])
+            if (self.opts.get('__role') != 'syndic' and message_target not in ('broadcast', self.hexid)) or \
+                (self.opts.get('__role') == 'syndic' and message_target not in ('broadcast', 'syndic')):
+                log.debug('Publish received for not this minion: %s', message_target)
+                raise tornado.gen.Return(None)
+            payload = self.serial.loads(messages[1])
+        else:
+            raise Exception(('Invalid number of messages ({0}) in zeromq pub'
+                             'message from master').format(len(messages_len)))
+        # Yield control back to the caller. When the payload has been decoded, assign
+        # the decoded payload to 'ret' and resume operation
+        ret = yield self._decode_payload(payload)
+        raise tornado.gen.Return(ret)
+
+    @property
+    def stream(self):
+        '''
+        Return the current zmqstream, creating one if necessary
+        '''
+        if not hasattr(self, '_stream'):
+            self._stream = zmq.eventloop.zmqstream.ZMQStream(self._socket, io_loop=self.io_loop)
+        return self._stream
+
+    def on_recv(self, callback):
+        '''
+        Register a callback for received messages (that we didn't initiate)
+
+        :param func callback: A function which should be called when data is received
+        '''
+        if callback is None:
+            return self.stream.on_recv(None)
+
+        @tornado.gen.coroutine
+        def wrap_callback(messages):
+            payload = yield self._decode_messages(messages)
+            if payload is not None:
+                callback(payload)
+        return self.stream.on_recv(wrap_callback)
+
+
+class ZeroMQReqServerChannel(salt.transport.mixins.auth.AESReqServerMixin,
+                             salt.transport.server.ReqServerChannel):
+
+    def __init__(self, opts):
+        salt.transport.server.ReqServerChannel.__init__(self, opts)
+        self._closing = False
+
+    def zmq_device(self):
+        '''
+        Multiprocessing target for the zmq queue device
+        '''
+        self.__setup_signals()
+        salt.utils.process.appendproctitle('MWorkerQueue')
+        self.context = zmq.Context(self.opts['worker_threads'])
+        # Prepare the zeromq sockets
+        self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)
+        self.clients = self.context.socket(zmq.ROUTER)
+        if self.opts['ipv6'] is True and hasattr(zmq, 'IPV4ONLY'):
+            # IPv6 sockets work for both IPv6 and IPv4 addresses
+            self.clients.setsockopt(zmq.IPV4ONLY, 0)
+        self.clients.setsockopt(zmq.BACKLOG, self.opts.get('zmq_backlog', 1000))
+        self._start_zmq_monitor()
+        self.workers = self.context.socket(zmq.DEALER)
+
+        if self.opts.get('ipc_mode', '') == 'tcp':
+            self.w_uri = 'tcp://127.0.0.1:{0}'.format(
+                self.opts.get('tcp_master_workers', 4515)
+                )
+        else:
+            self.w_uri = 'ipc://{0}'.format(
+                os.path.join(self.opts['sock_dir'], 'workers.ipc')
+                )
+
+        log.info('Setting up the master communication server')
+        self.clients.bind(self.uri)
+        self.workers.bind(self.w_uri)
+
+        while True:
+            if self.clients.closed or self.workers.closed:
+                break
+            try:
+                zmq.device(zmq.QUEUE, self.clients, self.workers)
+            except zmq.ZMQError as exc:
+                if exc.errno == errno.EINTR:
+                    continue
+                raise exc
+            except (KeyboardInterrupt, SystemExit):
+                break
+
+    def close(self):
+        '''
+        Cleanly shutdown the router socket
+        '''
+        if self._closing:
+            return
+        log.info('MWorkerQueue under PID %s is closing', os.getpid())
+        self._closing = True
+        # pylint: disable=E0203
+        if getattr(self, '_monitor', None) is not None:
+            self._monitor.stop()
+            self._monitor = None
+        if getattr(self, '_w_monitor', None) is not None:
+            self._w_monitor.stop()
+            self._w_monitor = None
+        if hasattr(self, 'clients') and self.clients.closed is False:
+            self.clients.close()
+        if hasattr(self, 'workers') and self.workers.closed is False:
+            self.workers.close()
+        if hasattr(self, 'stream'):
+            self.stream.close()
+        if hasattr(self, '_socket') and self._socket.closed is False:
+            self._socket.close()
+        if hasattr(self, 'context') and self.context.closed is False:
+            self.context.term()
+        # pylint: enable=E0203
+
+    def pre_fork(self, process_manager):
+        '''
+        Pre-fork we need to create the zmq router device
+
+        :param func process_manager: An instance of salt.utils.process.ProcessManager
+        '''
+        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)
+        process_manager.add_process(self.zmq_device)
+
+    def _start_zmq_monitor(self):
+        '''
+        Starts ZMQ monitor for debugging purposes.
+        :return:
+        '''
+        # Socket monitor shall be used the only for debug
+        # purposes so using threading doesn't look too bad here
+
+        if HAS_ZMQ_MONITOR and self.opts['zmq_monitor']:
+            log.debug('Starting ZMQ monitor')
+            import threading
+            self._w_monitor = ZeroMQSocketMonitor(self._socket)
+            threading.Thread(target=self._w_monitor.start_poll).start()
+            log.debug('ZMQ monitor has been started started')
+
+    def post_fork(self, payload_handler, io_loop):
+        '''
+        After forking we need to create all of the local sockets to listen to the
+        router
+
+        :param func payload_handler: A function to called to handle incoming payloads as
+                                     they are picked up off the wire
+        :param IOLoop io_loop: An instance of a Tornado IOLoop, to handle event scheduling
+        '''
+        self.payload_handler = payload_handler
+        self.io_loop = io_loop
+
+        self.context = zmq.Context(1)
+        self._socket = self.context.socket(zmq.REP)
+        self._start_zmq_monitor()
+
+        if self.opts.get('ipc_mode', '') == 'tcp':
+            self.w_uri = 'tcp://127.0.0.1:{0}'.format(
+                self.opts.get('tcp_master_workers', 4515)
+                )
+        else:
+            self.w_uri = 'ipc://{0}'.format(
+                os.path.join(self.opts['sock_dir'], 'workers.ipc')
+                )
+        log.info('Worker binding to socket %s', self.w_uri)
+        self._socket.connect(self.w_uri)
+
+        salt.transport.mixins.auth.AESReqServerMixin.post_fork(self, payload_handler, io_loop)
+
+        self.stream = zmq.eventloop.zmqstream.ZMQStream(self._socket, io_loop=self.io_loop)
+        self.stream.on_recv_stream(self.handle_message)
+
+    @tornado.gen.coroutine
+    def handle_message(self, stream, payload):
+        '''
+        Handle incoming messages from underlying TCP streams
+
+        :stream ZMQStream stream: A ZeroMQ stream.
+        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html
+
+        :param dict payload: A payload to process
+        '''
+        try:
+            payload = self.serial.loads(payload[0])
+            payload = self._decode_payload(payload)
+        except Exception as exc:
+            exc_type = type(exc).__name__
+            if exc_type == 'AuthenticationError':
+                log.debug(
+                    'Minion failed to auth to master. Since the payload is '
+                    'encrypted, it is not known which minion failed to '
+                    'authenticate. It is likely that this is a transient '
+                    'failure due to the master rotating its public key.'
+                )
+            else:
+                log.error('Bad load from minion: %s: %s', exc_type, exc)
+            stream.send(self.serial.dumps('bad load'))
+            raise tornado.gen.Return()
+
+        # TODO helper functions to normalize payload?
+        if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):
+            log.error('payload and load must be a dict. Payload was: %s and load was %s', payload, payload.get('load'))
+            stream.send(self.serial.dumps('payload and load must be a dict'))
+            raise tornado.gen.Return()
+
+        try:
+            id_ = payload['load'].get('id', '')
+            if str('\0') in id_:
+                log.error('Payload contains an id with a null byte: %s', payload)
+                stream.send(self.serial.dumps('bad load: id contains a null byte'))
+                raise tornado.gen.Return()
+        except TypeError:
+            log.error('Payload contains non-string id: %s', payload)
+            stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))
+            raise tornado.gen.Return()
+
+        # intercept the "_auth" commands, since the main daemon shouldn't know
+        # anything about our key auth
+        if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':
+            stream.send(self.serial.dumps(self._auth(payload['load'])))
+            raise tornado.gen.Return()
+
+        # TODO: test
+        try:
+            # Take the payload_handler function that was registered when we created the channel
+            # and call it, returning control to the caller until it completes
+            ret, req_opts = yield self.payload_handler(payload)
+        except Exception as e:
+            # always attempt to return an error to the minion
+            stream.send('Some exception handling minion payload')
+            log.error('Some exception handling a payload from minion', exc_info=True)
+            raise tornado.gen.Return()
+
+        req_fun = req_opts.get('fun', 'send')
+        if req_fun == 'send_clear':
+            stream.send(self.serial.dumps(ret))
+        elif req_fun == 'send':
+            stream.send(self.serial.dumps(self.crypticle.dumps(ret)))
+        elif req_fun == 'send_private':
+            stream.send(self.serial.dumps(self._encrypt_private(ret,
+                                                                req_opts['key'],
+                                                                req_opts['tgt'],
+                                                                )))
+        else:
+            log.error('Unknown req_fun %s', req_fun)
+            # always attempt to return an error to the minion
+            stream.send('Server-side exception handling payload')
+        raise tornado.gen.Return()
+
+    def __setup_signals(self):
+        signal.signal(signal.SIGINT, self._handle_signals)
+        signal.signal(signal.SIGTERM, self._handle_signals)
+
+    def _handle_signals(self, signum, sigframe):
+        msg = '{0} received a '.format(self.__class__.__name__)
+        if signum == signal.SIGINT:
+            msg += 'SIGINT'
+        elif signum == signal.SIGTERM:
+            msg += 'SIGTERM'
+        msg += '. Exiting'
+        log.debug(msg)
+        self.close()
+        sys.exit(salt.defaults.exitcodes.EX_OK)
+
+
+def _set_tcp_keepalive(zmq_socket, opts):
+    '''
+    Ensure that TCP keepalives are set as specified in "opts".
+
+    Warning: Failure to set TCP keepalives on the salt-master can result in
+    not detecting the loss of a minion when the connection is lost or when
+    it's host has been terminated without first closing the socket.
+    Salt's Presence System depends on this connection status to know if a minion
+    is "present".
+
+    Warning: Failure to set TCP keepalives on minions can result in frequent or
+    unexpected disconnects!
+    '''
+    if hasattr(zmq, 'TCP_KEEPALIVE') and opts:
+        if 'tcp_keepalive' in opts:
+            zmq_socket.setsockopt(
+                zmq.TCP_KEEPALIVE, opts['tcp_keepalive']
+            )
+        if 'tcp_keepalive_idle' in opts:
+            zmq_socket.setsockopt(
+                zmq.TCP_KEEPALIVE_IDLE, opts['tcp_keepalive_idle']
+            )
+        if 'tcp_keepalive_cnt' in opts:
+            zmq_socket.setsockopt(
+                zmq.TCP_KEEPALIVE_CNT, opts['tcp_keepalive_cnt']
+            )
+        if 'tcp_keepalive_intvl' in opts:
+            zmq_socket.setsockopt(
+                zmq.TCP_KEEPALIVE_INTVL, opts['tcp_keepalive_intvl']
+            )
+
+
+class ZeroMQPubServerChannel(salt.transport.server.PubServerChannel):
+    '''
+    Encapsulate synchronous operations for a publisher channel
+    '''
+
+    _sock_data = threading.local()
+
+    def __init__(self, opts):
+        self.opts = opts
+        self.serial = salt.payload.Serial(self.opts)  # TODO: in init?
+        self.ckminions = salt.utils.minions.CkMinions(self.opts)
+
+    def connect(self):
+        return tornado.gen.sleep(5)
+
+    def _publish_daemon(self, log_queue=None):
+        '''
+        Bind to the interface specified in the configuration file
+        '''
+        salt.utils.process.appendproctitle(self.__class__.__name__)
+        if log_queue:
+            salt.log.setup.set_multiprocessing_logging_queue(log_queue)
+            salt.log.setup.setup_multiprocessing_logging(log_queue)
+
+        # Set up the context
+        context = zmq.Context(1)
+        # Prepare minion publish socket
+        pub_sock = context.socket(zmq.PUB)
+        _set_tcp_keepalive(pub_sock, self.opts)
+        # if 2.1 >= zmq < 3.0, we only have one HWM setting
+        try:
+            pub_sock.setsockopt(zmq.HWM, self.opts.get('pub_hwm', 1000))
+        # in zmq >= 3.0, there are separate send and receive HWM settings
+        except AttributeError:
+            # Set the High Water Marks. For more information on HWM, see:
+            # http://api.zeromq.org/4-1:zmq-setsockopt
+            pub_sock.setsockopt(zmq.SNDHWM, self.opts.get('pub_hwm', 1000))
+            pub_sock.setsockopt(zmq.RCVHWM, self.opts.get('pub_hwm', 1000))
+        if self.opts['ipv6'] is True and hasattr(zmq, 'IPV4ONLY'):
+            # IPv6 sockets work for both IPv6 and IPv4 addresses
+            pub_sock.setsockopt(zmq.IPV4ONLY, 0)
+        pub_sock.setsockopt(zmq.BACKLOG, self.opts.get('zmq_backlog', 1000))
+        pub_sock.setsockopt(zmq.LINGER, -1)
+        pub_uri = 'tcp://{interface}:{publish_port}'.format(**self.opts)
+        # Prepare minion pull socket
+        pull_sock = context.socket(zmq.PULL)
+        pull_sock.setsockopt(zmq.LINGER, -1)
+
+        if self.opts.get('ipc_mode', '') == 'tcp':
+            pull_uri = 'tcp://127.0.0.1:{0}'.format(
+                self.opts.get('tcp_master_publish_pull', 4514)
+                )
+        else:
+            pull_uri = 'ipc://{0}'.format(
+                os.path.join(self.opts['sock_dir'], 'publish_pull.ipc')
+                )
+        salt.utils.zeromq.check_ipc_path_max_len(pull_uri)
+
+        # Start the minion command publisher
+        log.info('Starting the Salt Publisher on %s', pub_uri)
+        pub_sock.bind(pub_uri)
+
+        # Securely create socket
+        log.info('Starting the Salt Puller on %s', pull_uri)
+        with salt.utils.files.set_umask(0o177):
+            pull_sock.bind(pull_uri)
+
+        try:
+            while True:
+                # Catch and handle EINTR from when this process is sent
+                # SIGUSR1 gracefully so we don't choke and die horribly
+                try:
+                    log.debug('Publish daemon getting data from puller %s', pull_uri)
+                    package = pull_sock.recv()
+                    log.debug('Publish daemon received payload. size=%d', len(package))
+
+                    unpacked_package = salt.payload.unpackage(package)
+                    if six.PY3:
+                        unpacked_package = salt.transport.frame.decode_embedded_strs(unpacked_package)
+                    payload = unpacked_package['payload']
+                    log.trace('Accepted unpacked package from puller')
+                    if self.opts['zmq_filtering']:
+                        # if you have a specific topic list, use that
+                        if 'topic_lst' in unpacked_package:
+                            for topic in unpacked_package['topic_lst']:
+                                log.trace('Sending filtered data over publisher %s', pub_uri)
+                                # zmq filters are substring match, hash the topic
+                                # to avoid collisions
+                                htopic = salt.utils.stringutils.to_bytes(hashlib.sha1(salt.utils.stringutils.to_bytes(topic)).hexdigest())
+                                pub_sock.send(htopic, flags=zmq.SNDMORE)
+                                pub_sock.send(payload)
+                                log.trace('Filtered data has been sent')
+
+                            # Syndic broadcast
+                            if self.opts.get('order_masters'):
+                                log.trace('Sending filtered data to syndic')
+                                pub_sock.send(b'syndic', flags=zmq.SNDMORE)
+                                pub_sock.send(payload)
+                                log.trace('Filtered data has been sent to syndic')
+                        # otherwise its a broadcast
+                        else:
+                            # TODO: constants file for "broadcast"
+                            log.trace('Sending broadcasted data over publisher %s', pub_uri)
+                            pub_sock.send(b'broadcast', flags=zmq.SNDMORE)
+                            pub_sock.send(payload)
+                            log.trace('Broadcasted data has been sent')
+                    else:
+                        log.trace('Sending ZMQ-unfiltered data over publisher %s', pub_uri)
+                        pub_sock.send(payload)
+                        log.trace('Unfiltered data has been sent')
+                except zmq.ZMQError as exc:
+                    if exc.errno == errno.EINTR:
+                        continue
+                    raise exc
+
+        except KeyboardInterrupt:
+            log.trace('Publish daemon caught Keyboard interupt, tearing down')
+        # Cleanly close the sockets if we're shutting down
+        if pub_sock.closed is False:
+            pub_sock.close()
+        if pull_sock.closed is False:
+            pull_sock.close()
+        if context.closed is False:
+            context.term()
+
+    def pre_fork(self, process_manager, kwargs=None):
+        '''
+        Do anything necessary pre-fork. Since this is on the master side this will
+        primarily be used to create IPC channels and create our daemon process to
+        do the actual publishing
+
+        :param func process_manager: A ProcessManager, from salt.utils.process.ProcessManager
+        '''
+        process_manager.add_process(self._publish_daemon, kwargs=kwargs)
+
+    @property
+    def pub_sock(self):
+        '''
+        This thread's zmq publisher socket. This socket is stored on the class
+        so that multiple instantiations in the same thread will re-use a single
+        zmq socket.
+        '''
+        try:
+            return self._sock_data.sock
+        except AttributeError:
+            pass
+
+    def pub_connect(self):
+        '''
+        Create and connect this thread's zmq socket. If a publisher socket
+        already exists "pub_close" is called before creating and connecting a
+        new socket.
+        '''
+        if self.pub_sock:
+            self.pub_close()
+        ctx = zmq.Context.instance()
+        self._sock_data.sock = ctx.socket(zmq.PUSH)
+        self.pub_sock.setsockopt(zmq.LINGER, -1)
+        if self.opts.get('ipc_mode', '') == 'tcp':
+            pull_uri = 'tcp://127.0.0.1:{0}'.format(
+                self.opts.get('tcp_master_publish_pull', 4514)
+                )
+        else:
+            pull_uri = 'ipc://{0}'.format(
+                os.path.join(self.opts['sock_dir'], 'publish_pull.ipc')
+                )
+        log.debug("Connecting to pub server: %s", pull_uri)
+        self.pub_sock.connect(pull_uri)
+        return self._sock_data.sock
+
+    def pub_close(self):
+        '''
+        Disconnect an existing publisher socket and remove it from the local
+        thread's cache.
+        '''
+        if hasattr(self._sock_data, 'sock'):
+            self._sock_data.sock.close()
+            delattr(self._sock_data, 'sock')
+
+    def publish(self, load):
+        '''
+        Publish "load" to minions. This send the load to the publisher daemon
+        process with does the actual sending to minions.
+
+        :param dict load: A load to be sent across the wire to minions
+        '''
+        payload = {'enc': 'aes'}
+        crypticle = salt.crypt.Crypticle(self.opts, salt.master.SMaster.secrets['aes']['secret'].value)
+        payload['load'] = crypticle.dumps(load)
+        if self.opts['sign_pub_messages']:
+            master_pem_path = os.path.join(self.opts['pki_dir'], 'master.pem')
+            log.debug("Signing data packet")
+            payload['sig'] = salt.crypt.sign_message(master_pem_path, payload['load'])
+        int_payload = {'payload': self.serial.dumps(payload)}
+
+        # add some targeting stuff for lists only (for now)
+        if load['tgt_type'] == 'list':
+            int_payload['topic_lst'] = load['tgt']
+
+        # If zmq_filtering is enabled, target matching has to happen master side
+        match_targets = ["pcre", "glob", "list"]
+        if self.opts['zmq_filtering'] and load['tgt_type'] in match_targets:
+            # Fetch a list of minions that match
+            _res = self.ckminions.check_minions(load['tgt'],
+                                                tgt_type=load['tgt_type'])
+            match_ids = _res['minions']
+
+            log.debug("Publish Side Match: %s", match_ids)
+            # Send list of miions thru so zmq can target them
+            int_payload['topic_lst'] = match_ids
+        payload = self.serial.dumps(int_payload)
+        log.debug(
+            'Sending payload to publish daemon. jid=%s size=%d',
+            load.get('jid', None), len(payload),
+        )
+        if not self.pub_sock:
+            self.pub_connect()
+        self.pub_sock.send(payload)
+        log.debug('Sent payload to publish daemon.')
+
+
+class AsyncReqMessageClientPool(salt.transport.MessageClientPool):
+    '''
+    Wrapper class of AsyncReqMessageClientPool to avoid blocking waiting while writing data to socket.
+    '''
+    def __init__(self, opts, args=None, kwargs=None):
+        super(AsyncReqMessageClientPool, self).__init__(AsyncReqMessageClient, opts, args=args, kwargs=kwargs)
+        self._closing = False
+
+    def close(self):
+        if self._closing:
+            return
+
+        self._closing = True
+        for message_client in self.message_clients:
+            message_client.close()
+        self.message_clients = []
+
+    def send(self, *args, **kwargs):
+        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))
+        return message_clients[0].send(*args, **kwargs)
+
+    def destroy(self):
+        # Bacwards compat
+        salt.utils.versions.warn_until(
+            'Sodium',
+            'Calling {0}.destroy() is deprecated. Please call {0}.close() instead.'.format(
+                self.__class__.__name__
+            ),
+            stacklevel=3
+        )
+        self.close()
+
+    def __del__(self):
+        self.close()
+
+
+# TODO: unit tests!
+class AsyncReqMessageClient(object):
+    '''
+    This class wraps the underlying zeromq REQ socket and gives a future-based
+    interface to sending and recieving messages. This works around the primary
+    limitation of serialized send/recv on the underlying socket by queueing the
+    message sends in this class. In the future if we decide to attempt to multiplex
+    we can manage a pool of REQ/REP sockets-- but for now we'll just do them in serial
+    '''
+    def __init__(self, opts, addr, linger=0, io_loop=None):
+        '''
+        Create an asynchronous message client
+
+        :param dict opts: The salt opts dictionary
+        :param str addr: The interface IP address to bind to
+        :param int linger: The number of seconds to linger on a ZMQ socket. See
+                           http://api.zeromq.org/2-1:zmq-setsockopt [ZMQ_LINGER]
+        :param IOLoop io_loop: A Tornado IOLoop event scheduler [tornado.ioloop.IOLoop]
+        '''
+        self.opts = opts
+        self.addr = addr
+        self.linger = linger
+        if io_loop is None:
+            install_zmq()
+            ZMQDefaultLoop.current()
+        else:
+            self.io_loop = io_loop
+
+        self.serial = salt.payload.Serial(self.opts)
+        self.context = zmq.Context()
+
+        # wire up sockets
+        self._init_socket()
+
+        self.send_queue = []
+        # mapping of message -> future
+        self.send_future_map = {}
+
+        self.send_timeout_map = {}  # message -> timeout
+        self._closing = False
+
+    # TODO: timeout all in-flight sessions, or error
+    def close(self):
+        if self._closing:
+            return
+
+        self._closing = True
+        if hasattr(self, 'stream') and self.stream is not None:
+            if ZMQ_VERSION_INFO < (14, 3, 0):
+                # stream.close() doesn't work properly on pyzmq < 14.3.0
+                if self.stream.socket:
+                    self.stream.socket.close()
+                self.stream.io_loop.remove_handler(self.stream.socket)
+                # set this to None, more hacks for messed up pyzmq
+                self.stream.socket = None
+                self.socket.close()
+            else:
+                self.stream.close()
+                self.socket = None
+            self.stream = None
+        if self.context.closed is False:
+            self.context.term()
+
+    def destroy(self):
+        # Bacwards compat
+        salt.utils.versions.warn_until(
+            'Sodium',
+            'Calling {0}.destroy() is deprecated. Please call {0}.close() instead.'.format(
+                self.__class__.__name__
+            ),
+            stacklevel=3
+        )
+        self.close()
+
+    def __del__(self):
+        self.close()
+
+    def _init_socket(self):
+        if hasattr(self, 'stream'):
+            self.stream.close()  # pylint: disable=E0203
+            self.socket.close()  # pylint: disable=E0203
+            del self.stream
+            del self.socket
+
+        self.socket = self.context.socket(zmq.REQ)
+
+        # socket options
+        if hasattr(zmq, 'RECONNECT_IVL_MAX'):
+            self.socket.setsockopt(
+                zmq.RECONNECT_IVL_MAX, 5000
+            )
+
+        _set_tcp_keepalive(self.socket, self.opts)
+        if self.addr.startswith('tcp://['):
+            # Hint PF type if bracket enclosed IPv6 address
+            if hasattr(zmq, 'IPV6'):
+                self.socket.setsockopt(zmq.IPV6, 1)
+            elif hasattr(zmq, 'IPV4ONLY'):
+                self.socket.setsockopt(zmq.IPV4ONLY, 0)
+        self.socket.linger = self.linger
+        log.debug('Trying to connect to: %s', self.addr)
+        self.socket.connect(self.addr)
+        self.stream = zmq.eventloop.zmqstream.ZMQStream(self.socket, io_loop=self.io_loop)
+
+    @tornado.gen.coroutine
+    def _internal_send_recv(self):
+        while len(self.send_queue) > 0:
+            message = self.send_queue[0]
+            future = self.send_future_map.get(message, None)
+            if future is None:
+                # Timedout
+                del self.send_queue[0]
+                continue
+
+            # send
+            def mark_future(msg):
+                if not future.done():
+                    data = self.serial.loads(msg[0])
+                    future.set_result(data)
+            self.stream.on_recv(mark_future)
+            self.stream.send(message)
+
+            try:
+                ret = yield future
+            except Exception as err:  # pylint: disable=W0702
+                log.debug('Re-init ZMQ socket: %s', err)
+                self._init_socket()  # re-init the zmq socket (no other way in zmq)
+                del self.send_queue[0]
+                continue
+            del self.send_queue[0]
+            self.send_future_map.pop(message, None)
+            self.remove_message_timeout(message)
+
+    def remove_message_timeout(self, message):
+        if message not in self.send_timeout_map:
+            return
+        timeout = self.send_timeout_map.pop(message, None)
+        if timeout is not None:
+            # Hasn't been already timedout
+            self.io_loop.remove_timeout(timeout)
+
+    def timeout_message(self, message):
+        '''
+        Handle a message timeout by removing it from the sending queue
+        and informing the caller
+
+        :raises: SaltReqTimeoutError
+        '''
+        future = self.send_future_map.pop(message, None)
+        # In a race condition the message might have been sent by the time
+        # we're timing it out. Make sure the future is not None
+        if future is not None:
+            del self.send_timeout_map[message]
+            if future.attempts < future.tries:
+                future.attempts += 1
+                log.debug('SaltReqTimeoutError, retrying. (%s/%s)', future.attempts, future.tries)
+                self.send(
+                    message,
+                    timeout=future.timeout,
+                    tries=future.tries,
+                    future=future,
+                )
+
+            else:
+                future.set_exception(SaltReqTimeoutError('Message timed out'))
+
+    def send(self, message, timeout=None, tries=3, future=None, callback=None, raw=False):
+        '''
+        Return a future which will be completed when the message has a response
+        '''
+        if future is None:
+            future = tornado.concurrent.Future()
+            future.tries = tries
+            future.attempts = 0
+            future.timeout = timeout
+            # if a future wasn't passed in, we need to serialize the message
+            message = self.serial.dumps(message)
+        if callback is not None:
+            def handle_future(future):
+                response = future.result()
+                self.io_loop.add_callback(callback, response)
+            future.add_done_callback(handle_future)
+        # Add this future to the mapping
+        self.send_future_map[message] = future
+
+        if self.opts.get('detect_mode') is True:
+            timeout = 1
+
+        if timeout is not None:
+            send_timeout = self.io_loop.call_later(timeout, self.timeout_message, message)
+            self.send_timeout_map[message] = send_timeout
+
+        if len(self.send_queue) == 0:
+            self.io_loop.spawn_callback(self._internal_send_recv)
+
+        self.send_queue.append(message)
+
+        return future
+
+
+class ZeroMQSocketMonitor(object):
+    __EVENT_MAP = None
+
+    def __init__(self, socket):
+        '''
+        Create ZMQ monitor sockets
+
+        More information:
+            http://api.zeromq.org/4-0:zmq-socket-monitor
+        '''
+        self._socket = socket
+        self._monitor_socket = self._socket.get_monitor_socket()
+        self._monitor_stream = None
+
+    def start_io_loop(self, io_loop):
+        log.trace("Event monitor start!")
+        self._monitor_stream = zmq.eventloop.zmqstream.ZMQStream(self._monitor_socket, io_loop=io_loop)
+        self._monitor_stream.on_recv(self.monitor_callback)
+
+    def start_poll(self):
+        log.trace("Event monitor start!")
+        try:
+            while self._monitor_socket is not None and self._monitor_socket.poll():
+                msg = self._monitor_socket.recv_multipart()
+                self.monitor_callback(msg)
+        except (AttributeError, zmq.error.ContextTerminated):
+            # We cannot log here because we'll get an interrupted system call in trying
+            # to flush the logging buffer as we terminate
+            pass
+
+    @property
+    def event_map(self):
+        if ZeroMQSocketMonitor.__EVENT_MAP is None:
+            event_map = {}
+            for name in dir(zmq):
+                if name.startswith('EVENT_'):
+                    value = getattr(zmq, name)
+                    event_map[value] = name
+            ZeroMQSocketMonitor.__EVENT_MAP = event_map
+        return ZeroMQSocketMonitor.__EVENT_MAP
+
+    def monitor_callback(self, msg):
+        evt = zmq.utils.monitor.parse_monitor_message(msg)
+        evt['description'] = self.event_map[evt['event']]
+        log.debug("ZeroMQ event: %s", evt)
+        if evt['event'] == zmq.EVENT_MONITOR_STOPPED:
+            self.stop()
+
+    def stop(self):
+        if self._socket is None:
+            return
+        self._socket.disable_monitor()
+        self._socket = None
+        self._monitor_socket = None
+        if self._monitor_stream is not None:
+            self._monitor_stream.close()
+            self._monitor_stream = None
+        log.trace("Event monitor done!")
diff -Naur a/salt/utils/asynchronous.py c/salt/utils/asynchronous.py
--- a/salt/utils/asynchronous.py	2019-07-02 10:15:07.023874717 -0600
+++ c/salt/utils/asynchronous.py	2019-07-02 10:58:03.179938595 -0600
@@ -5,8 +5,13 @@
 
 from __future__ import absolute_import, print_function, unicode_literals
 
-import tornado.ioloop
-import tornado.concurrent
+try:
+    from tornado4.ioloop import IOLoop
+    from tornado4.concurrent import Future as TornadoFuture
+except ImportError:
+    from tornado.ioloop import IOLoop
+    from tornado.concurrent import Future as TornadoFuture
+
 import contextlib
 from salt.utils import zeromq
 
@@ -16,7 +21,7 @@
     '''
     A context manager that will set the current ioloop to io_loop for the context
     '''
-    orig_loop = tornado.ioloop.IOLoop.current()
+    orig_loop = IOLoop.current()
     io_loop.make_current()
     try:
         yield
@@ -60,7 +65,7 @@
                 # Overload the ioloop for the func call-- since it might call .current()
                 with current_ioloop(self.io_loop):
                     ret = attr(*args, **kwargs)
-                    if isinstance(ret, tornado.concurrent.Future):
+                    if isinstance(ret, TornadoFuture):
                         ret = self._block_future(ret)
                     return ret
             return wrap
diff -Naur a/salt/utils/event.py c/salt/utils/event.py
--- a/salt/utils/event.py	2019-07-02 10:15:07.023874717 -0600
+++ c/salt/utils/event.py	2019-07-02 10:58:03.179938595 -0600
@@ -71,8 +71,12 @@
 
 # Import third party libs
 from salt.ext import six
-import tornado.ioloop
-import tornado.iostream
+try:
+    from tornado4.ioloop import IOLoop
+    from tornado4.iostream import StreamClosedError
+except:
+    from tornado.ioloop import IOLoop
+    from tornado.iostream import StreamClosedError
 
 # Import salt libs
 import salt.config
@@ -239,7 +243,7 @@
             self.io_loop = io_loop
             self._run_io_loop_sync = False
         else:
-            self.io_loop = tornado.ioloop.IOLoop()
+            self.io_loop = IOLoop()
             self._run_io_loop_sync = True
         self.cpub = False
         self.cpush = False
@@ -539,7 +543,7 @@
                 # Trigger that at least a single iteration has gone through
                 run_once = True
             try:
-                # tornado.ioloop.IOLoop.run_sync() timeouts are in seconds.
+                # IOLoop.run_sync() timeouts are in seconds.
                 # IPCMessageSubscriber.read_sync() uses this type of timeout.
                 if not self.cpub and not self.connect_pub(timeout=wait):
                     break
@@ -551,7 +555,7 @@
                 ret = {'data': data, 'tag': mtag}
             except KeyboardInterrupt:
                 return {'tag': 'salt/event/exit', 'data': {}}
-            except tornado.iostream.StreamClosedError:
+            except StreamClosedError:
                 if self.raise_errors:
                     raise
                 else:
@@ -639,7 +643,7 @@
                         try:
                             ret = self._get_event(wait, tag, match_func, no_block)
                             break
-                        except tornado.iostream.StreamClosedError:
+                        except StreamClosedError:
                             self.close_pub()
                             self.connect_pub(timeout=wait)
                             continue
@@ -951,7 +955,7 @@
         default_minion_sock_dir = self.opts['sock_dir']
         self.opts.update(opts)
 
-        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        self.io_loop = io_loop or IOLoop.current()
         self._closing = False
 
         hash_type = getattr(hashlib, self.opts['hash_type'])
@@ -1082,7 +1086,7 @@
         Bind the pub and pull sockets for events
         '''
         salt.utils.process.appendproctitle(self.__class__.__name__)
-        self.io_loop = tornado.ioloop.IOLoop()
+        self.io_loop = IOLoop()
         with salt.utils.asynchronous.current_ioloop(self.io_loop):
             if self.opts['ipc_mode'] == 'tcp':
                 epub_uri = int(self.opts['tcp_master_pub_port'])
diff -Naur a/salt/utils/event.py.orig c/salt/utils/event.py.orig
--- a/salt/utils/event.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/utils/event.py.orig	2019-07-02 10:57:25.387937657 -0600
@@ -0,0 +1,1403 @@
+# -*- coding: utf-8 -*-
+'''
+Manage events
+
+Events are all fired off via a zeromq 'pub' socket, and listened to with local
+zeromq 'sub' sockets
+
+
+All of the formatting is self contained in the event module, so we should be
+able to modify the structure in the future since the same module used to read
+events is the same module used to fire off events.
+
+Old style event messages were comprised of two parts delimited at the 20 char
+point. The first 20 characters are used for the zeromq subscriber to match
+publications and 20 characters was chosen because it was at the time a few more
+characters than the length of a jid (Job ID).  Any tags of length less than 20
+characters were padded with "|" chars out to 20 characters.
+
+Although not explicit, the data for an event comprised a python dict that was
+serialized by msgpack.
+
+New style event messages support event tags longer than 20 characters while
+still being backwards compatible with old style tags.
+
+The longer tags better enable name spaced event tags which tend to be longer.
+Moreover, the constraint that the event data be a python dict is now an
+explicit constraint and fire-event will now raise a ValueError if not. Tags
+must be ascii safe strings, that is, have values less than 0x80
+
+Since the msgpack dict (map) indicators have values greater than or equal to
+0x80 it can be unambiguously determined if the start of data is at char 21
+or not.
+
+In the new style, when the tag is longer than 20 characters, an end of tag
+string is appended to the tag given by the string constant TAGEND, that is, two
+line feeds '\n\n'.  When the tag is less than 20 characters then the tag is
+padded with pipes "|" out to 20 characters as before.  When the tag is exactly
+20 characters no padded is done.
+
+The get_event method intelligently figures out if the tag is longer than 20
+characters.
+
+
+The convention for namespacing is to use dot characters "." as the name space
+delimiter. The name space "salt" is reserved by SaltStack for internal events.
+
+For example:
+Namespaced tag
+    'salt.runner.manage.status.start'
+
+'''
+
+from __future__ import absolute_import, unicode_literals, print_function
+
+# Import python libs
+import os
+import time
+import fnmatch
+import hashlib
+import logging
+import datetime
+import sys
+
+try:
+    from collections.abc import MutableMapping
+except ImportError:
+    from collections import MutableMapping
+
+from multiprocessing.util import Finalize
+from salt.ext.six.moves import range
+
+# Import third party libs
+from salt.ext import six
+import tornado.ioloop
+import tornado.iostream
+
+# Import salt libs
+import salt.config
+import salt.payload
+import salt.utils.asynchronous
+import salt.utils.cache
+import salt.utils.dicttrim
+import salt.utils.files
+import salt.utils.platform
+import salt.utils.process
+import salt.utils.stringutils
+import salt.utils.zeromq
+import salt.log.setup
+import salt.defaults.exitcodes
+import salt.transport.ipc
+import salt.transport.client
+
+log = logging.getLogger(__name__)
+
+# The SUB_EVENT set is for functions that require events fired based on
+# component executions, like the state system
+SUB_EVENT = ('state.highstate', 'state.sls')
+
+TAGEND = str('\n\n')  # long tag delimiter
+TAGPARTER = str('/')  # name spaced tag delimiter
+SALT = 'salt'  # base prefix for all salt/ events
+# dict map of namespaced base tag prefixes for salt events
+TAGS = {
+    'auth': 'auth',  # prefix for all salt/auth events
+    'job': 'job',  # prefix for all salt/job events (minion jobs)
+    'key': 'key',  # prefix for all salt/key events
+    'minion': 'minion',  # prefix for all salt/minion events
+                         # (minion sourced events)
+    'syndic': 'syndic',  # prefix for all salt/syndic events
+                         # (syndic minion sourced events)
+    'run': 'run',  # prefix for all salt/run events (salt runners)
+    'wheel': 'wheel',  # prefix for all salt/wheel events
+    'cloud': 'cloud',  # prefix for all salt/cloud events
+    'fileserver': 'fileserver',  # prefix for all salt/fileserver events
+    'queue': 'queue',  # prefix for all salt/queue events
+}
+
+
+def get_event(
+        node, sock_dir=None, transport='zeromq',
+        opts=None, listen=True, io_loop=None, keep_loop=False, raise_errors=False):
+    '''
+    Return an event object suitable for the named transport
+
+    :param IOLoop io_loop: Pass in an io_loop if you want asynchronous
+                           operation for obtaining events. Eg use of
+                           set_event_handler() API. Otherwise, operation
+                           will be synchronous.
+    '''
+    sock_dir = sock_dir or opts['sock_dir']
+    # TODO: AIO core is separate from transport
+    if transport in ('zeromq', 'tcp'):
+        if node == 'master':
+            return MasterEvent(sock_dir,
+                               opts,
+                               listen=listen,
+                               io_loop=io_loop,
+                               keep_loop=keep_loop,
+                               raise_errors=raise_errors)
+        return SaltEvent(node,
+                         sock_dir,
+                         opts,
+                         listen=listen,
+                         io_loop=io_loop,
+                         keep_loop=keep_loop,
+                         raise_errors=raise_errors)
+    elif transport == 'raet':
+        import salt.utils.raetevent
+        return salt.utils.raetevent.RAETEvent(node,
+                                              sock_dir=sock_dir,
+                                              listen=listen,
+                                              opts=opts)
+
+
+def get_master_event(opts, sock_dir, listen=True, io_loop=None, raise_errors=False):
+    '''
+    Return an event object suitable for the named transport
+    '''
+    # TODO: AIO core is separate from transport
+    if opts['transport'] in ('zeromq', 'tcp', 'detect'):
+        return MasterEvent(sock_dir, opts, listen=listen, io_loop=io_loop, raise_errors=raise_errors)
+    elif opts['transport'] == 'raet':
+        import salt.utils.raetevent
+        return salt.utils.raetevent.MasterEvent(
+            opts=opts, sock_dir=sock_dir, listen=listen
+        )
+
+
+def fire_args(opts, jid, tag_data, prefix=''):
+    '''
+    Fire an event containing the arguments passed to an orchestration job
+    '''
+    try:
+        tag_suffix = [jid, 'args']
+    except NameError:
+        pass
+    else:
+        tag = tagify(tag_suffix, prefix)
+        try:
+            _event = get_master_event(opts, opts['sock_dir'], listen=False)
+            _event.fire_event(tag_data, tag=tag)
+        except Exception as exc:
+            # Don't let a problem here hold up the rest of the orchestration
+            log.warning(
+                'Failed to fire args event %s with data %s: %s',
+                tag, tag_data, exc
+            )
+
+
+def tagify(suffix='', prefix='', base=SALT):
+    '''
+    convenience function to build a namespaced event tag string
+    from joining with the TABPART character the base, prefix and suffix
+
+    If string prefix is a valid key in TAGS Then use the value of key prefix
+    Else use prefix string
+
+    If suffix is a list Then join all string elements of suffix individually
+    Else use string suffix
+
+    '''
+    parts = [base, TAGS.get(prefix, prefix)]
+    if hasattr(suffix, 'append'):  # list so extend parts
+        parts.extend(suffix)
+    else:  # string so append
+        parts.append(suffix)
+
+    for index, _ in enumerate(parts):
+        try:
+            parts[index] = salt.utils.stringutils.to_str(parts[index])
+        except TypeError:
+            parts[index] = str(parts[index])
+    return TAGPARTER.join([part for part in parts if part])
+
+
+class SaltEvent(object):
+    '''
+    Warning! Use the get_event function or the code will not be
+    RAET compatible
+    The base class used to manage salt events
+    '''
+    def __init__(
+            self, node, sock_dir=None,
+            opts=None, listen=True, io_loop=None,
+            keep_loop=False, raise_errors=False):
+        '''
+        :param IOLoop io_loop: Pass in an io_loop if you want asynchronous
+                               operation for obtaining events. Eg use of
+                               set_event_handler() API. Otherwise, operation
+                               will be synchronous.
+        :param Bool keep_loop: Pass a boolean to determine if we want to keep
+                               the io loop or destroy it when the event handle
+                               is destroyed. This is useful when using event
+                               loops from within third party asynchronous code
+        '''
+        self.serial = salt.payload.Serial({'serial': 'msgpack'})
+        self.keep_loop = keep_loop
+        if io_loop is not None:
+            self.io_loop = io_loop
+            self._run_io_loop_sync = False
+        else:
+            self.io_loop = tornado.ioloop.IOLoop()
+            self._run_io_loop_sync = True
+        self.cpub = False
+        self.cpush = False
+        self.subscriber = None
+        self.pusher = None
+        self.raise_errors = raise_errors
+
+        if opts is None:
+            opts = {}
+        if node == 'master':
+            self.opts = salt.config.DEFAULT_MASTER_OPTS.copy()
+        else:
+            self.opts = salt.config.DEFAULT_MINION_OPTS.copy()
+        self.opts.update(opts)
+
+        if sock_dir is None:
+            sock_dir = self.opts['sock_dir']
+        else:
+            self.opts['sock_dir'] = sock_dir
+
+        if salt.utils.platform.is_windows() and 'ipc_mode' not in opts:
+            self.opts['ipc_mode'] = 'tcp'
+        self.puburi, self.pulluri = self.__load_uri(sock_dir, node)
+        self.pending_tags = []
+        self.pending_events = []
+        self.__load_cache_regex()
+        if listen and not self.cpub:
+            # Only connect to the publisher at initialization time if
+            # we know we want to listen. If we connect to the publisher
+            # and don't read out events from the buffer on an on-going basis,
+            # the buffer will grow resulting in big memory usage.
+            self.connect_pub()
+
+    @classmethod
+    def __load_cache_regex(cls):
+        '''
+        Initialize the regular expression cache and put it in the
+        class namespace. The regex search strings will be prepend with '^'
+        '''
+        # This is in the class namespace, to minimize cache memory
+        # usage and maximize cache hits
+        # The prepend='^' is to reduce differences in behavior between
+        # the default 'startswith' and the optional 'regex' match_type
+        cls.cache_regex = salt.utils.cache.CacheRegex(prepend='^')
+
+    def __load_uri(self, sock_dir, node):
+        '''
+        Return the string URI for the location of the pull and pub sockets to
+        use for firing and listening to events
+        '''
+        if node == 'master':
+            if self.opts['ipc_mode'] == 'tcp':
+                puburi = int(self.opts['tcp_master_pub_port'])
+                pulluri = int(self.opts['tcp_master_pull_port'])
+            else:
+                puburi = os.path.join(
+                    sock_dir,
+                    'master_event_pub.ipc'
+                )
+                pulluri = os.path.join(
+                    sock_dir,
+                    'master_event_pull.ipc'
+                )
+        else:
+            if self.opts['ipc_mode'] == 'tcp':
+                puburi = int(self.opts['tcp_pub_port'])
+                pulluri = int(self.opts['tcp_pull_port'])
+            else:
+                hash_type = getattr(hashlib, self.opts['hash_type'])
+                # Only use the first 10 chars to keep longer hashes from exceeding the
+                # max socket path length.
+                id_hash = hash_type(salt.utils.stringutils.to_bytes(self.opts['id'])).hexdigest()[:10]
+                puburi = os.path.join(
+                    sock_dir,
+                    'minion_event_{0}_pub.ipc'.format(id_hash)
+                )
+                pulluri = os.path.join(
+                    sock_dir,
+                    'minion_event_{0}_pull.ipc'.format(id_hash)
+                )
+        log.debug('%s PUB socket URI: %s', self.__class__.__name__, puburi)
+        log.debug('%s PULL socket URI: %s', self.__class__.__name__, pulluri)
+        return puburi, pulluri
+
+    def subscribe(self, tag=None, match_type=None):
+        '''
+        Subscribe to events matching the passed tag.
+
+        If you do not subscribe to a tag, events will be discarded by calls to
+        get_event that request a different tag. In contexts where many different
+        jobs are outstanding it is important to subscribe to prevent one call
+        to get_event from discarding a response required by a subsequent call
+        to get_event.
+        '''
+        if tag is None:
+            return
+        match_func = self._get_match_func(match_type)
+        self.pending_tags.append([tag, match_func])
+
+    def unsubscribe(self, tag, match_type=None):
+        '''
+        Un-subscribe to events matching the passed tag.
+        '''
+        if tag is None:
+            return
+        match_func = self._get_match_func(match_type)
+
+        self.pending_tags.remove([tag, match_func])
+
+        old_events = self.pending_events
+        self.pending_events = []
+        for evt in old_events:
+            if any(pmatch_func(evt['tag'], ptag) for ptag, pmatch_func in self.pending_tags):
+                self.pending_events.append(evt)
+
+    def connect_pub(self, timeout=None):
+        '''
+        Establish the publish connection
+        '''
+        if self.cpub:
+            return True
+
+        if self._run_io_loop_sync:
+            with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                if self.subscriber is None:
+                    self.subscriber = salt.transport.ipc.IPCMessageSubscriber(
+                        self.puburi,
+                        io_loop=self.io_loop
+                    )
+                try:
+                    self.io_loop.run_sync(
+                        lambda: self.subscriber.connect(timeout=timeout))
+                    self.cpub = True
+                except Exception:
+                    pass
+        else:
+            if self.subscriber is None:
+                self.subscriber = salt.transport.ipc.IPCMessageSubscriber(
+                    self.puburi,
+                    io_loop=self.io_loop
+                )
+
+            # For the asynchronous case, the connect will be defered to when
+            # set_event_handler() is invoked.
+            self.cpub = True
+        return self.cpub
+
+    def close_pub(self):
+        '''
+        Close the publish connection (if established)
+        '''
+        if not self.cpub:
+            return
+
+        self.subscriber.close()
+        self.subscriber = None
+        self.pending_events = []
+        self.cpub = False
+
+    def connect_pull(self, timeout=1):
+        '''
+        Establish a connection with the event pull socket
+        Default timeout is 1 s
+        '''
+        if self.cpush:
+            return True
+
+        if self._run_io_loop_sync:
+            with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                if self.pusher is None:
+                    self.pusher = salt.transport.ipc.IPCMessageClient(
+                        self.pulluri,
+                        io_loop=self.io_loop
+                    )
+                try:
+                    self.io_loop.run_sync(
+                        lambda: self.pusher.connect(timeout=timeout))
+                    self.cpush = True
+                except Exception:
+                    pass
+        else:
+            if self.pusher is None:
+                self.pusher = salt.transport.ipc.IPCMessageClient(
+                    self.pulluri,
+                    io_loop=self.io_loop
+                )
+            # For the asynchronous case, the connect will be deferred to when
+            # fire_event() is invoked.
+            self.cpush = True
+        return self.cpush
+
+    @classmethod
+    def unpack(cls, raw, serial=None):
+        if serial is None:
+            serial = salt.payload.Serial({'serial': 'msgpack'})
+
+        if six.PY2:
+            mtag, sep, mdata = raw.partition(TAGEND)  # split tag from data
+            data = serial.loads(mdata, encoding='utf-8')
+        else:
+            mtag, sep, mdata = raw.partition(salt.utils.stringutils.to_bytes(TAGEND))  # split tag from data
+            mtag = salt.utils.stringutils.to_str(mtag)
+            data = serial.loads(mdata, encoding='utf-8')
+        return mtag, data
+
+    def _get_match_func(self, match_type=None):
+        if match_type is None:
+            match_type = self.opts['event_match_type']
+        return getattr(self, '_match_tag_{0}'.format(match_type), None)
+
+    def _check_pending(self, tag, match_func=None):
+        """Check the pending_events list for events that match the tag
+
+        :param tag: The tag to search for
+        :type tag: str
+        :param tags_regex: List of re expressions to search for also
+        :type tags_regex: list[re.compile()]
+        :return:
+        """
+        if match_func is None:
+            match_func = self._get_match_func()
+        old_events = self.pending_events
+        self.pending_events = []
+        ret = None
+        for evt in old_events:
+            if match_func(evt['tag'], tag):
+                if ret is None:
+                    ret = evt
+                    log.trace('get_event() returning cached event = %s', ret)
+                else:
+                    self.pending_events.append(evt)
+            elif any(pmatch_func(evt['tag'], ptag) for ptag, pmatch_func in self.pending_tags):
+                self.pending_events.append(evt)
+            else:
+                log.trace('get_event() discarding cached event that no longer has any subscriptions = %s', evt)
+        return ret
+
+    @staticmethod
+    def _match_tag_startswith(event_tag, search_tag):
+        '''
+        Check if the event_tag matches the search check.
+        Uses startswith to check.
+        Return True (matches) or False (no match)
+        '''
+        return event_tag.startswith(search_tag)
+
+    @staticmethod
+    def _match_tag_endswith(event_tag, search_tag):
+        '''
+        Check if the event_tag matches the search check.
+        Uses endswith to check.
+        Return True (matches) or False (no match)
+        '''
+        return event_tag.endswith(search_tag)
+
+    @staticmethod
+    def _match_tag_find(event_tag, search_tag):
+        '''
+        Check if the event_tag matches the search check.
+        Uses find to check.
+        Return True (matches) or False (no match)
+        '''
+        return event_tag.find(search_tag) >= 0
+
+    def _match_tag_regex(self, event_tag, search_tag):
+        '''
+        Check if the event_tag matches the search check.
+        Uses regular expression search to check.
+        Return True (matches) or False (no match)
+        '''
+        return self.cache_regex.get(search_tag).search(event_tag) is not None
+
+    def _match_tag_fnmatch(self, event_tag, search_tag):
+        '''
+        Check if the event_tag matches the search check.
+        Uses fnmatch to check.
+        Return True (matches) or False (no match)
+        '''
+        return fnmatch.fnmatch(event_tag, search_tag)
+
+    def _get_event(self, wait, tag, match_func=None, no_block=False):
+        if match_func is None:
+            match_func = self._get_match_func()
+        start = time.time()
+        timeout_at = start + wait
+        run_once = False
+        if no_block is True:
+            wait = 0
+        elif wait == 0:
+            # If no_block is False and wait is 0, that
+            # means an infinite timeout.
+            wait = None
+        while (run_once is False and not wait) or time.time() <= timeout_at:
+            if no_block is True:
+                if run_once is True:
+                    break
+                # Trigger that at least a single iteration has gone through
+                run_once = True
+            try:
+                # tornado.ioloop.IOLoop.run_sync() timeouts are in seconds.
+                # IPCMessageSubscriber.read_sync() uses this type of timeout.
+                if not self.cpub and not self.connect_pub(timeout=wait):
+                    break
+
+                raw = self.subscriber.read_sync(timeout=wait)
+                if raw is None:
+                    break
+                mtag, data = self.unpack(raw, self.serial)
+                ret = {'data': data, 'tag': mtag}
+            except KeyboardInterrupt:
+                return {'tag': 'salt/event/exit', 'data': {}}
+            except tornado.iostream.StreamClosedError:
+                if self.raise_errors:
+                    raise
+                else:
+                    return None
+            except RuntimeError:
+                return None
+
+            if not match_func(ret['tag'], tag):
+                # tag not match
+                if any(pmatch_func(ret['tag'], ptag) for ptag, pmatch_func in self.pending_tags):
+                    log.trace('get_event() caching unwanted event = %s', ret)
+                    self.pending_events.append(ret)
+                if wait:  # only update the wait timeout if we had one
+                    wait = timeout_at - time.time()
+                continue
+
+            log.trace('get_event() received = %s', ret)
+            return ret
+        log.trace('_get_event() waited %s seconds and received nothing', wait)
+        return None
+
+    def get_event(self,
+                  wait=5,
+                  tag='',
+                  full=False,
+                  match_type=None,
+                  no_block=False,
+                  auto_reconnect=False):
+        '''
+        Get a single publication.
+        If no publication is available, then block for up to ``wait`` seconds.
+        Return publication if it is available or ``None`` if no publication is
+        available.
+
+        If wait is 0, then block forever.
+
+        tag
+            Only return events matching the given tag. If not specified, or set
+            to an empty string, all events are returned. It is recommended to
+            always be selective on what is to be returned in the event that
+            multiple requests are being multiplexed.
+
+        match_type
+            Set the function to match the search tag with event tags.
+             - 'startswith' : search for event tags that start with tag
+             - 'endswith' : search for event tags that end with tag
+             - 'find' : search for event tags that contain tag
+             - 'regex' : regex search '^' + tag event tags
+             - 'fnmatch' : fnmatch tag event tags matching
+            Default is opts['event_match_type'] or 'startswith'
+
+            .. versionadded:: 2015.8.0
+
+        no_block
+            Define if getting the event should be a blocking call or not.
+            Defaults to False to keep backwards compatibility.
+
+            .. versionadded:: 2015.8.0
+
+        Notes:
+
+        Searches cached publications first. If no cached publications are found
+        that match the given tag specification, new publications are received
+        and checked.
+
+        If a publication is received that does not match the tag specification,
+        it is DISCARDED unless it is subscribed to via subscribe() which will
+        cause it to be cached.
+
+        If a caller is not going to call get_event immediately after sending a
+        request, it MUST subscribe the result to ensure the response is not lost
+        should other regions of code call get_event for other purposes.
+        '''
+        assert self._run_io_loop_sync
+
+        match_func = self._get_match_func(match_type)
+
+        ret = self._check_pending(tag, match_func)
+        if ret is None:
+            with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                if auto_reconnect:
+                    raise_errors = self.raise_errors
+                    self.raise_errors = True
+                    while True:
+                        try:
+                            ret = self._get_event(wait, tag, match_func, no_block)
+                            break
+                        except tornado.iostream.StreamClosedError:
+                            self.close_pub()
+                            self.connect_pub(timeout=wait)
+                            continue
+                    self.raise_errors = raise_errors
+                else:
+                    ret = self._get_event(wait, tag, match_func, no_block)
+
+        if ret is None or full:
+            return ret
+        else:
+            return ret['data']
+
+    def get_event_noblock(self):
+        '''
+        Get the raw event without blocking or any other niceties
+        '''
+        assert self._run_io_loop_sync
+
+        if not self.cpub:
+            if not self.connect_pub():
+                return None
+        raw = self.subscriber.read_sync(timeout=0)
+        if raw is None:
+            return None
+        mtag, data = self.unpack(raw, self.serial)
+        return {'data': data, 'tag': mtag}
+
+    def get_event_block(self):
+        '''
+        Get the raw event in a blocking fashion. This is slower, but it decreases the
+        possibility of dropped events.
+        '''
+        assert self._run_io_loop_sync
+
+        if not self.cpub:
+            if not self.connect_pub():
+                return None
+        raw = self.subscriber.read_sync(timeout=None)
+        if raw is None:
+            return None
+        mtag, data = self.unpack(raw, self.serial)
+        return {'data': data, 'tag': mtag}
+
+    def iter_events(self, tag='', full=False, match_type=None, auto_reconnect=False):
+        '''
+        Creates a generator that continuously listens for events
+        '''
+        while True:
+            data = self.get_event(tag=tag, full=full, match_type=match_type,
+                                  auto_reconnect=auto_reconnect)
+            if data is None:
+                continue
+            yield data
+
+    def fire_event(self, data, tag, timeout=1000):
+        '''
+        Send a single event into the publisher with payload dict "data" and
+        event identifier "tag"
+
+        The default is 1000 ms
+        '''
+        if not six.text_type(tag):  # no empty tags allowed
+            raise ValueError('Empty tag.')
+
+        if not isinstance(data, MutableMapping):  # data must be dict
+            raise ValueError(
+                'Dict object expected, not \'{0}\'.'.format(data)
+            )
+
+        if not self.cpush:
+            if timeout is not None:
+                timeout_s = float(timeout) / 1000
+            else:
+                timeout_s = None
+            if not self.connect_pull(timeout=timeout_s):
+                return False
+
+        data['_stamp'] = datetime.datetime.utcnow().isoformat()
+
+        tagend = TAGEND
+        if six.PY2:
+            dump_data = self.serial.dumps(data)
+        else:
+            # Since the pack / unpack logic here is for local events only,
+            # it is safe to change the wire protocol. The mechanism
+            # that sends events from minion to master is outside this
+            # file.
+            dump_data = self.serial.dumps(data, use_bin_type=True)
+
+        serialized_data = salt.utils.dicttrim.trim_dict(
+            dump_data,
+            self.opts['max_event_size'],
+            is_msgpacked=True,
+            use_bin_type=six.PY3
+        )
+        log.debug('Sending event: tag = %s; data = %s', tag, data)
+        event = b''.join([
+            salt.utils.stringutils.to_bytes(tag),
+            salt.utils.stringutils.to_bytes(tagend),
+            serialized_data])
+        msg = salt.utils.stringutils.to_bytes(event, 'utf-8')
+        if self._run_io_loop_sync:
+            with salt.utils.asynchronous.current_ioloop(self.io_loop):
+                try:
+                    self.io_loop.run_sync(lambda: self.pusher.send(msg))
+                except Exception as ex:
+                    log.debug(ex)
+                    raise
+        else:
+            self.io_loop.spawn_callback(self.pusher.send, msg)
+        return True
+
+    def fire_master(self, data, tag, timeout=1000):
+        ''''
+        Send a single event to the master, with the payload "data" and the
+        event identifier "tag".
+
+        Default timeout is 1000ms
+        '''
+        msg = {
+            'tag': tag,
+            'data': data,
+            'events': None,
+            'pretag': None
+        }
+        return self.fire_event(msg, "fire_master", timeout)
+
+    def destroy(self):
+        if self.subscriber is not None:
+            self.subscriber.close()
+        if self.pusher is not None:
+            self.pusher.close()
+        if self._run_io_loop_sync and not self.keep_loop:
+            self.io_loop.close()
+
+    def _fire_ret_load_specific_fun(self, load, fun_index=0):
+        '''
+        Helper function for fire_ret_load
+        '''
+        if isinstance(load['fun'], list):
+            # Multi-function job
+            fun = load['fun'][fun_index]
+            # 'retcode' was already validated to exist and be non-zero
+            # for the given function in the caller.
+            if isinstance(load['retcode'], list):
+                # Multi-function ordered
+                ret = load.get('return')
+                if isinstance(ret, list) and len(ret) > fun_index:
+                    ret = ret[fun_index]
+                else:
+                    ret = {}
+                retcode = load['retcode'][fun_index]
+            else:
+                ret = load.get('return', {})
+                ret = ret.get(fun, {})
+                retcode = load['retcode'][fun]
+        else:
+            # Single-function job
+            fun = load['fun']
+            ret = load.get('return', {})
+            retcode = load['retcode']
+
+        try:
+            for tag, data in six.iteritems(ret):
+                data['retcode'] = retcode
+                tags = tag.split('_|-')
+                if data.get('result') is False:
+                    self.fire_event(
+                        data,
+                        '{0}.{1}'.format(tags[0], tags[-1])
+                    )  # old dup event
+                    data['jid'] = load['jid']
+                    data['id'] = load['id']
+                    data['success'] = False
+                    data['return'] = 'Error: {0}.{1}'.format(
+                        tags[0], tags[-1])
+                    data['fun'] = fun
+                    data['user'] = load['user']
+                    self.fire_event(
+                        data,
+                        tagify([load['jid'],
+                                'sub',
+                                load['id'],
+                                'error',
+                                fun],
+                               'job'))
+        except Exception:
+            pass
+
+    def fire_ret_load(self, load):
+        '''
+        Fire events based on information in the return load
+        '''
+        if load.get('retcode') and load.get('fun'):
+            if isinstance(load['fun'], list):
+                # Multi-function job
+                if isinstance(load['retcode'], list):
+                    multifunc_ordered = True
+                else:
+                    multifunc_ordered = False
+
+                for fun_index in range(0, len(load['fun'])):
+                    fun = load['fun'][fun_index]
+                    if multifunc_ordered:
+                        if (len(load['retcode']) > fun_index and
+                                load['retcode'][fun_index] and
+                                fun in SUB_EVENT):
+                            # Minion fired a bad retcode, fire an event
+                            self._fire_ret_load_specific_fun(load, fun_index)
+                    else:
+                        if load['retcode'].get(fun, 0) and fun in SUB_EVENT:
+                            # Minion fired a bad retcode, fire an event
+                            self._fire_ret_load_specific_fun(load, fun_index)
+            else:
+                # Single-function job
+                if load['fun'] in SUB_EVENT:
+                    # Minion fired a bad retcode, fire an event
+                    self._fire_ret_load_specific_fun(load)
+
+    def set_event_handler(self, event_handler):
+        '''
+        Invoke the event_handler callback each time an event arrives.
+        '''
+        assert not self._run_io_loop_sync
+
+        if not self.cpub:
+            self.connect_pub()
+        # This will handle reconnects
+        return self.subscriber.read_async(event_handler)
+
+    def __del__(self):
+        # skip exceptions in destroy-- since destroy() doesn't cover interpreter
+        # shutdown-- where globals start going missing
+        try:
+            self.destroy()
+        except Exception:
+            pass
+
+
+class MasterEvent(SaltEvent):
+    '''
+    Warning! Use the get_event function or the code will not be
+    RAET compatible
+    Create a master event management object
+    '''
+    def __init__(
+            self,
+            sock_dir,
+            opts=None,
+            listen=True,
+            io_loop=None,
+            keep_loop=False,
+            raise_errors=False):
+        super(MasterEvent, self).__init__(
+            'master',
+            sock_dir,
+            opts,
+            listen=listen,
+            io_loop=io_loop,
+            keep_loop=keep_loop,
+            raise_errors=raise_errors)
+
+
+class LocalClientEvent(MasterEvent):
+    '''
+    Warning! Use the get_event function or the code will not be
+    RAET compatible
+    This class is just used to differentiate who is handling the events,
+    specially on logs, but it's the same as MasterEvent.
+    '''
+
+
+class NamespacedEvent(object):
+    '''
+    A wrapper for sending events within a specific base namespace
+    '''
+    def __init__(self, event, base, print_func=None):
+        self.event = event
+        self.base = base
+        self.print_func = print_func
+
+    def fire_event(self, data, tag):
+        self.event.fire_event(data, tagify(tag, base=self.base))
+        if self.print_func is not None:
+            self.print_func(tag, data)
+
+
+class MinionEvent(SaltEvent):
+    '''
+    Warning! Use the get_event function or the code will not be
+    RAET compatible
+    Create a master event management object
+    '''
+    def __init__(self, opts, listen=True, io_loop=None, raise_errors=False):
+        super(MinionEvent, self).__init__(
+            'minion', sock_dir=opts.get('sock_dir'),
+            opts=opts, listen=listen, io_loop=io_loop,
+            raise_errors=raise_errors)
+
+
+class AsyncEventPublisher(object):
+    '''
+    An event publisher class intended to run in an ioloop (within a single process)
+
+    TODO: remove references to "minion_event" whenever we need to use this for other things
+    '''
+    def __init__(self, opts, io_loop=None):
+        self.opts = salt.config.DEFAULT_MINION_OPTS.copy()
+        default_minion_sock_dir = self.opts['sock_dir']
+        self.opts.update(opts)
+
+        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        self._closing = False
+
+        hash_type = getattr(hashlib, self.opts['hash_type'])
+        # Only use the first 10 chars to keep longer hashes from exceeding the
+        # max socket path length.
+        id_hash = hash_type(salt.utils.stringutils.to_bytes(self.opts['id'])).hexdigest()[:10]
+        epub_sock_path = os.path.join(
+            self.opts['sock_dir'],
+            'minion_event_{0}_pub.ipc'.format(id_hash)
+        )
+        if os.path.exists(epub_sock_path):
+            os.unlink(epub_sock_path)
+        epull_sock_path = os.path.join(
+            self.opts['sock_dir'],
+            'minion_event_{0}_pull.ipc'.format(id_hash)
+        )
+        if os.path.exists(epull_sock_path):
+            os.unlink(epull_sock_path)
+
+        if self.opts['ipc_mode'] == 'tcp':
+            epub_uri = int(self.opts['tcp_pub_port'])
+            epull_uri = int(self.opts['tcp_pull_port'])
+        else:
+            epub_uri = epub_sock_path
+            epull_uri = epull_sock_path
+
+        log.debug('%s PUB socket URI: %s',
+                  self.__class__.__name__, epub_uri)
+        log.debug('%s PULL socket URI: %s',
+                  self.__class__.__name__, epull_uri)
+
+        minion_sock_dir = self.opts['sock_dir']
+
+        if not os.path.isdir(minion_sock_dir):
+            # Let's try to create the directory defined on the configuration
+            # file
+            try:
+                os.makedirs(minion_sock_dir, 0o755)
+            except OSError as exc:
+                log.error('Could not create SOCK_DIR: %s', exc)
+                # Let's not fail yet and try using the default path
+                if minion_sock_dir == default_minion_sock_dir:
+                    # We're already trying the default system path, stop now!
+                    raise
+
+                if not os.path.isdir(default_minion_sock_dir):
+                    try:
+                        os.makedirs(default_minion_sock_dir, 0o755)
+                    except OSError as exc:
+                        log.error('Could not create SOCK_DIR: %s', exc)
+                        # Let's stop at this stage
+                        raise
+
+        self.publisher = salt.transport.ipc.IPCMessagePublisher(
+            self.opts,
+            epub_uri,
+            io_loop=self.io_loop
+        )
+
+        self.puller = salt.transport.ipc.IPCMessageServer(
+            epull_uri,
+            io_loop=self.io_loop,
+            payload_handler=self.handle_publish
+        )
+
+        log.info('Starting pull socket on %s', epull_uri)
+        with salt.utils.files.set_umask(0o177):
+            self.publisher.start()
+            self.puller.start()
+
+    def handle_publish(self, package, _):
+        '''
+        Get something from epull, publish it out epub, and return the package (or None)
+        '''
+        try:
+            self.publisher.publish(package)
+            return package
+        # Add an extra fallback in case a forked process leeks through
+        except Exception:
+            log.critical('Unexpected error while polling minion events',
+                         exc_info=True)
+            return None
+
+    def close(self):
+        if self._closing:
+            return
+        self._closing = True
+        if hasattr(self, 'publisher'):
+            self.publisher.close()
+        if hasattr(self, 'puller'):
+            self.puller.close()
+
+    def __del__(self):
+        self.close()
+
+
+class EventPublisher(salt.utils.process.SignalHandlingMultiprocessingProcess):
+    '''
+    The interface that takes master events and republishes them out to anyone
+    who wants to listen
+    '''
+    def __init__(self, opts, **kwargs):
+        super(EventPublisher, self).__init__(**kwargs)
+        self.opts = salt.config.DEFAULT_MASTER_OPTS.copy()
+        self.opts.update(opts)
+        self._closing = False
+
+    # __setstate__ and __getstate__ are only used on Windows.
+    # We do this so that __init__ will be invoked on Windows in the child
+    # process so that a register_after_fork() equivalent will work on Windows.
+    def __setstate__(self, state):
+        self._is_child = True
+        self.__init__(
+            state['opts'],
+            log_queue=state['log_queue'],
+            log_queue_level=state['log_queue_level']
+        )
+
+    def __getstate__(self):
+        return {
+            'opts': self.opts,
+            'log_queue': self.log_queue,
+            'log_queue_level': self.log_queue_level
+        }
+
+    def run(self):
+        '''
+        Bind the pub and pull sockets for events
+        '''
+        salt.utils.process.appendproctitle(self.__class__.__name__)
+        self.io_loop = tornado.ioloop.IOLoop()
+        with salt.utils.asynchronous.current_ioloop(self.io_loop):
+            if self.opts['ipc_mode'] == 'tcp':
+                epub_uri = int(self.opts['tcp_master_pub_port'])
+                epull_uri = int(self.opts['tcp_master_pull_port'])
+            else:
+                epub_uri = os.path.join(
+                    self.opts['sock_dir'],
+                    'master_event_pub.ipc'
+                )
+                epull_uri = os.path.join(
+                    self.opts['sock_dir'],
+                    'master_event_pull.ipc'
+                )
+
+            self.publisher = salt.transport.ipc.IPCMessagePublisher(
+                self.opts,
+                epub_uri,
+                io_loop=self.io_loop
+            )
+
+            self.puller = salt.transport.ipc.IPCMessageServer(
+                epull_uri,
+                io_loop=self.io_loop,
+                payload_handler=self.handle_publish,
+            )
+
+            # Start the master event publisher
+            with salt.utils.files.set_umask(0o177):
+                self.publisher.start()
+                self.puller.start()
+                if (self.opts['ipc_mode'] != 'tcp' and (
+                        self.opts['publisher_acl'] or
+                        self.opts['external_auth'])):
+                    os.chmod(os.path.join(
+                        self.opts['sock_dir'], 'master_event_pub.ipc'), 0o666)
+
+            # Make sure the IO loop and respective sockets are closed and
+            # destroyed
+            Finalize(self, self.close, exitpriority=15)
+
+            self.io_loop.start()
+
+    def handle_publish(self, package, _):
+        '''
+        Get something from epull, publish it out epub, and return the package (or None)
+        '''
+        try:
+            self.publisher.publish(package)
+            return package
+        # Add an extra fallback in case a forked process leeks through
+        except Exception:
+            log.critical('Unexpected error while polling master events',
+                         exc_info=True)
+            return None
+
+    def close(self):
+        if self._closing:
+            return
+        self._closing = True
+        if hasattr(self, 'publisher'):
+            self.publisher.close()
+        if hasattr(self, 'puller'):
+            self.puller.close()
+        if hasattr(self, 'io_loop'):
+            self.io_loop.close()
+
+    def _handle_signals(self, signum, sigframe):
+        self.close()
+        super(EventPublisher, self)._handle_signals(signum, sigframe)
+
+    def __del__(self):
+        self.close()
+
+
+class EventReturn(salt.utils.process.SignalHandlingMultiprocessingProcess):
+    '''
+    A dedicated process which listens to the master event bus and queues
+    and forwards events to the specified returner.
+    '''
+    def __new__(cls, *args, **kwargs):
+        if sys.platform.startswith('win'):
+            # This is required for Windows.  On Linux, when a process is
+            # forked, the module namespace is copied and the current process
+            # gets all of sys.modules from where the fork happens.  This is not
+            # the case for Windows.
+            import salt.minion
+        instance = super(EventReturn, cls).__new__(cls, *args, **kwargs)
+        return instance
+
+    def __init__(self, opts, **kwargs):
+        '''
+        Initialize the EventReturn system
+
+        Return an EventReturn instance
+        '''
+        super(EventReturn, self).__init__(**kwargs)
+
+        self.opts = opts
+        self.event_return_queue = self.opts['event_return_queue']
+        self.event_return_queue_max_seconds = self.opts.get('event_return_queue_max_seconds', 0)
+        local_minion_opts = self.opts.copy()
+        local_minion_opts['file_client'] = 'local'
+        self.minion = salt.minion.MasterMinion(local_minion_opts)
+        self.event_queue = []
+        self.stop = False
+
+    # __setstate__ and __getstate__ are only used on Windows.
+    # We do this so that __init__ will be invoked on Windows in the child
+    # process so that a register_after_fork() equivalent will work on Windows.
+    def __setstate__(self, state):
+        self._is_child = True
+        self.__init__(
+            state['opts'],
+            log_queue=state['log_queue'],
+            log_queue_level=state['log_queue_level']
+        )
+
+    def __getstate__(self):
+        return {
+            'opts': self.opts,
+            'log_queue': self.log_queue,
+            'log_queue_level': self.log_queue_level
+        }
+
+    def _handle_signals(self, signum, sigframe):
+        # Flush and terminate
+        if self.event_queue:
+            self.flush_events()
+        self.stop = True
+        super(EventReturn, self)._handle_signals(signum, sigframe)
+
+    def flush_events(self):
+        if isinstance(self.opts['event_return'], list):
+            # Multiple event returners
+            for r in self.opts['event_return']:
+                log.debug('Calling event returner %s, one of many.', r)
+                event_return = '{0}.event_return'.format(r)
+                self._flush_event_single(event_return)
+        else:
+            # Only a single event returner
+            log.debug('Calling event returner %s, only one '
+                      'configured.', self.opts['event_return'])
+            event_return = '{0}.event_return'.format(
+                self.opts['event_return']
+                )
+            self._flush_event_single(event_return)
+        del self.event_queue[:]
+
+    def _flush_event_single(self, event_return):
+        if event_return in self.minion.returners:
+            try:
+                self.minion.returners[event_return](self.event_queue)
+            except Exception as exc:
+                log.error('Could not store events - returner \'%s\' raised '
+                          'exception: %s', event_return, exc)
+                # don't waste processing power unnecessarily on converting a
+                # potentially huge dataset to a string
+                if log.level <= logging.DEBUG:
+                    log.debug('Event data that caused an exception: %s',
+                        self.event_queue)
+        else:
+            log.error('Could not store return for event(s) - returner '
+                      '\'%s\' not found.', event_return)
+
+    def run(self):
+        '''
+        Spin up the multiprocess event returner
+        '''
+        salt.utils.process.appendproctitle(self.__class__.__name__)
+        self.event = get_event('master', opts=self.opts, listen=True)
+        events = self.event.iter_events(full=True)
+        self.event.fire_event({}, 'salt/event_listen/start')
+        try:
+            # events below is a generator, we will iterate until we get the salt/event/exit tag
+            oldestevent = None
+            for event in events:
+
+                if event['tag'] == 'salt/event/exit':
+                    # We're done eventing
+                    self.stop = True
+                if self._filter(event):
+                    # This event passed the filter, add it to the queue
+                    self.event_queue.append(event)
+                too_long_in_queue = False
+
+                # if max_seconds is >0, then we want to make sure we flush the queue
+                # every event_return_queue_max_seconds seconds,  If it's 0, don't
+                # apply any of this logic
+                if self.event_return_queue_max_seconds > 0:
+                    rightnow = datetime.datetime.now()
+                    if not oldestevent:
+                        oldestevent = rightnow
+                    age_in_seconds = (rightnow - oldestevent).seconds
+                    if age_in_seconds > 0:
+                        log.debug('Oldest event in queue is %s seconds old.', age_in_seconds)
+                    if age_in_seconds >= self.event_return_queue_max_seconds:
+                        too_long_in_queue = True
+                        oldestevent = None
+                    else:
+                        too_long_in_queue = False
+
+                    if too_long_in_queue:
+                        log.debug('Oldest event has been in queue too long, will flush queue')
+
+                # If we are over the max queue size or the oldest item in the queue has been there too long
+                # then flush the queue
+                if len(self.event_queue) >= self.event_return_queue or too_long_in_queue:
+                    log.debug('Flushing %s events.', len(self.event_queue))
+                    self.flush_events()
+                    oldestevent = None
+                if self.stop:
+                    # We saw the salt/event/exit tag, we can stop eventing
+                    break
+        finally:  # flush all we have at this moment
+            # No matter what, make sure we flush the queue even when we are exiting
+            # and there will be no more events.
+            if self.event_queue:
+                log.debug('Flushing %s events.', len(self.event_queue))
+
+                self.flush_events()
+
+    def _filter(self, event):
+        '''
+        Take an event and run it through configured filters.
+
+        Returns True if event should be stored, else False
+        '''
+        tag = event['tag']
+        if self.opts['event_return_whitelist']:
+            ret = False
+        else:
+            ret = True
+        for whitelist_match in self.opts['event_return_whitelist']:
+            if fnmatch.fnmatch(tag, whitelist_match):
+                ret = True
+                break
+        for blacklist_match in self.opts['event_return_blacklist']:
+            if fnmatch.fnmatch(tag, blacklist_match):
+                ret = False
+                break
+        return ret
+
+
+class StateFire(object):
+    '''
+    Evaluate the data from a state run and fire events on the master and minion
+    for each returned chunk that is not "green"
+    This object is made to only run on a minion
+    '''
+    def __init__(self, opts, auth=None):
+        self.opts = opts
+        if not auth:
+            self.auth = salt.crypt.SAuth(self.opts)
+        else:
+            self.auth = auth
+
+    def fire_master(self, data, tag, preload=None):
+        '''
+        Fire an event off on the master server
+
+        CLI Example:
+
+        .. code-block:: bash
+
+            salt '*' event.fire_master 'stuff to be in the event' 'tag'
+        '''
+        load = {}
+        if preload:
+            load.update(preload)
+
+        load.update({
+            'id': self.opts['id'],
+            'tag': tag,
+            'data': data,
+            'cmd': '_minion_event',
+            'tok': self.auth.gen_token(b'salt'),
+        })
+
+        channel = salt.transport.client.ReqChannel.factory(self.opts)
+        try:
+            channel.send(load)
+        except Exception:
+            pass
+        finally:
+            channel.close()
+        return True
+
+    def fire_running(self, running):
+        '''
+        Pass in a state "running" dict, this is the return dict from a state
+        call. The dict will be processed and fire events.
+
+        By default yellows and reds fire events on the master and minion, but
+        this can be configured.
+        '''
+        load = {'id': self.opts['id'],
+                'events': [],
+                'cmd': '_minion_event'}
+        for stag in sorted(
+                running,
+                key=lambda k: running[k].get('__run_num__', 0)):
+            if running[stag]['result'] and not running[stag]['changes']:
+                continue
+            tag = 'state_{0}_{1}'.format(
+                six.text_type(running[stag]['result']),
+                'True' if running[stag]['changes'] else 'False')
+            load['events'].append({
+                'tag': tag,
+                'data': running[stag],
+            })
+        channel = salt.transport.client.ReqChannel.factory(self.opts)
+        try:
+            channel.send(load)
+        except Exception:
+            pass
+        finally:
+            channel.close()
+        return True
diff -Naur a/salt/utils/gitfs.py c/salt/utils/gitfs.py
--- a/salt/utils/gitfs.py	2019-07-02 10:15:07.023874717 -0600
+++ c/salt/utils/gitfs.py	2019-07-02 10:58:03.179938595 -0600
@@ -18,7 +18,11 @@
 import stat
 import subprocess
 import time
-import tornado.ioloop
+try:
+    from tornado4.ioloop import IOLoop
+except:
+    from tornado.ioloop import IOLoop
+
 import weakref
 from datetime import datetime
 
@@ -2686,7 +2690,7 @@
         exited.
         '''
         # No need to get the ioloop reference if we're not initializing remotes
-        io_loop = tornado.ioloop.IOLoop.current() if init_remotes else None
+        io_loop = IOLoop.current() if init_remotes else None
         if not init_remotes or io_loop not in cls.instance_map:
             # We only evaluate the second condition in this if statement if
             # we're initializing remotes, so we won't get here unless io_loop
diff -Naur a/salt/utils/gitfs.py.orig c/salt/utils/gitfs.py.orig
--- a/salt/utils/gitfs.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/utils/gitfs.py.orig	2019-07-02 10:57:25.387937657 -0600
@@ -0,0 +1,3167 @@
+# -*- coding: utf-8 -*-
+'''
+Classes which provide the shared base for GitFS, git_pillar, and winrepo
+'''
+
+# Import python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import copy
+import contextlib
+import errno
+import fnmatch
+import glob
+import hashlib
+import logging
+import os
+import shlex
+import shutil
+import stat
+import subprocess
+import time
+import tornado.ioloop
+import weakref
+from datetime import datetime
+
+# Import salt libs
+import salt.utils.configparser
+import salt.utils.data
+import salt.utils.files
+import salt.utils.gzip_util
+import salt.utils.hashutils
+import salt.utils.itertools
+import salt.utils.path
+import salt.utils.platform
+import salt.utils.stringutils
+import salt.utils.url
+import salt.utils.user
+import salt.utils.versions
+import salt.fileserver
+from salt.config import DEFAULT_MASTER_OPTS as _DEFAULT_MASTER_OPTS
+from salt.utils.odict import OrderedDict
+from salt.utils.process import os_is_running as pid_exists
+from salt.exceptions import (
+    FileserverConfigError,
+    GitLockError,
+    get_error_message
+)
+from salt.utils.event import tagify
+from salt.utils.versions import LooseVersion as _LooseVersion
+
+# Import third party libs
+from salt.ext import six
+
+VALID_REF_TYPES = _DEFAULT_MASTER_OPTS['gitfs_ref_types']
+
+# Optional per-remote params that can only be used on a per-remote basis, and
+# thus do not have defaults in salt/config.py.
+PER_REMOTE_ONLY = ('name',)
+# Params which are global only and cannot be overridden for a single remote.
+GLOBAL_ONLY = ()
+
+SYMLINK_RECURSE_DEPTH = 100
+
+# Auth support (auth params can be global or per-remote, too)
+AUTH_PROVIDERS = ('pygit2',)
+AUTH_PARAMS = ('user', 'password', 'pubkey', 'privkey', 'passphrase',
+               'insecure_auth')
+
+# GitFS only: params which can be overridden for a single saltenv. Aside from
+# 'ref', this must be a subset of the per-remote params passed to the
+# constructor for the GitProvider subclasses.
+PER_SALTENV_PARAMS = ('mountpoint', 'root', 'ref')
+
+_RECOMMEND_GITPYTHON = (
+    'GitPython is installed, you may wish to set %s_provider to '
+    '\'gitpython\' to use GitPython for %s support.'
+)
+
+_RECOMMEND_PYGIT2 = (
+    'pygit2 is installed, you may wish to set %s_provider to '
+    '\'pygit2\' to use pygit2 for for %s support.'
+)
+
+_INVALID_REPO = (
+    'Cache path %s (corresponding remote: %s) exists but is not a valid '
+    'git repository. You will need to manually delete this directory on the '
+    'master to continue to use this %s remote.'
+)
+
+log = logging.getLogger(__name__)
+
+# pylint: disable=import-error
+try:
+    import git
+    import gitdb
+    GITPYTHON_VERSION = _LooseVersion(git.__version__)
+except Exception:
+    GITPYTHON_VERSION = None
+
+try:
+    # Squelch warning on cent7 due to them upgrading cffi
+    import warnings
+    with warnings.catch_warnings():
+        warnings.simplefilter('ignore')
+        import pygit2
+    PYGIT2_VERSION = _LooseVersion(pygit2.__version__)
+    LIBGIT2_VERSION = _LooseVersion(pygit2.LIBGIT2_VERSION)
+
+    # Work around upstream bug where bytestrings were being decoded using the
+    # default encoding (which is usually ascii on Python 2). This was fixed
+    # on 2 Feb 2018, so releases prior to 0.26.3 will need a workaround.
+    if PYGIT2_VERSION <= _LooseVersion('0.26.3'):
+        try:
+            import pygit2.ffi
+            import pygit2.remote
+        except ImportError:
+            # If we couldn't import these, then we're using an old enough
+            # version where ffi isn't in use and this workaround would be
+            # useless.
+            pass
+        else:
+            def __maybe_string(ptr):
+                if not ptr:
+                    return None
+                return pygit2.ffi.string(ptr).decode('utf-8')
+
+            pygit2.remote.maybe_string = __maybe_string
+
+    # Older pygit2 releases did not raise a specific exception class, this
+    # try/except makes Salt's exception catching work on any supported release.
+    try:
+        GitError = pygit2.errors.GitError
+    except AttributeError:
+        GitError = Exception
+except Exception as exc:
+    # Exceptions other than ImportError can be raised in cases where there is a
+    # problem with cffi (such as when python-cffi is upgraded and pygit2 tries
+    # to rebuild itself against the newer cffi). Therefore, we simply will
+    # catch a generic exception, and log the exception if it is anything other
+    # than an ImportError.
+    PYGIT2_VERSION = None
+    LIBGIT2_VERSION = None
+    if not isinstance(exc, ImportError):
+        log.exception('Failed to import pygit2')
+
+# pylint: enable=import-error
+
+# Minimum versions for backend providers
+GITPYTHON_MINVER = _LooseVersion('0.3')
+PYGIT2_MINVER = _LooseVersion('0.20.3')
+LIBGIT2_MINVER = _LooseVersion('0.20.0')
+
+
+def enforce_types(key, val):
+    '''
+    Force params to be strings unless they should remain a different type
+    '''
+    non_string_params = {
+        'ssl_verify': bool,
+        'insecure_auth': bool,
+        'disable_saltenv_mapping': bool,
+        'env_whitelist': 'stringlist',
+        'env_blacklist': 'stringlist',
+        'saltenv_whitelist': 'stringlist',
+        'saltenv_blacklist': 'stringlist',
+        'refspecs': 'stringlist',
+        'ref_types': 'stringlist',
+        'update_interval': int,
+    }
+
+    def _find_global(key):
+        for item in non_string_params:
+            try:
+                if key.endswith('_' + item):
+                    ret = item
+                    break
+            except TypeError:
+                if key.endswith('_' + six.text_type(item)):
+                    ret = item
+                    break
+        else:
+            ret = None
+        return ret
+
+    if key not in non_string_params:
+        key = _find_global(key)
+        if key is None:
+            return six.text_type(val)
+
+    expected = non_string_params[key]
+    if expected == 'stringlist':
+        if not isinstance(val, (six.string_types, list)):
+            val = six.text_type(val)
+        if isinstance(val, six.string_types):
+            return [x.strip() for x in val.split(',')]
+        return [six.text_type(x) for x in val]
+    else:
+        try:
+            return expected(val)
+        except Exception as exc:
+            log.error(
+                'Failed to enforce type for key=%s with val=%s, falling back '
+                'to a string', key, val
+            )
+            return six.text_type(val)
+
+
+def failhard(role):
+    '''
+    Fatal configuration issue, raise an exception
+    '''
+    raise FileserverConfigError('Failed to load {0}'.format(role))
+
+
+class GitProvider(object):
+    '''
+    Base class for gitfs/git_pillar provider classes. Should never be used
+    directly.
+
+    self.provider should be set in the sub-class' __init__ function before
+    invoking the parent class' __init__.
+    '''
+    def __init__(self, opts, remote, per_remote_defaults, per_remote_only,
+                 override_params, cache_root, role='gitfs'):
+        self.opts = opts
+        self.role = role
+        self.global_saltenv = salt.utils.data.repack_dictlist(
+            self.opts.get('{0}_saltenv'.format(self.role), []),
+            strict=True,
+            recurse=True,
+            key_cb=six.text_type,
+            val_cb=lambda x, y: six.text_type(y))
+        self.conf = copy.deepcopy(per_remote_defaults)
+
+        # Remove the 'salt://' from the beginning of any globally-defined
+        # per-saltenv mountpoints
+        for saltenv, saltenv_conf in six.iteritems(self.global_saltenv):
+            if 'mountpoint' in saltenv_conf:
+                self.global_saltenv[saltenv]['mountpoint'] = \
+                    salt.utils.url.strip_proto(
+                        self.global_saltenv[saltenv]['mountpoint']
+                    )
+
+        per_remote_collisions = [x for x in override_params
+                                 if x in per_remote_only]
+        if per_remote_collisions:
+            log.critical(
+                'The following parameter names are restricted to per-remote '
+                'use only: %s. This is a bug, please report it.',
+                ', '.join(per_remote_collisions)
+            )
+
+        try:
+            valid_per_remote_params = override_params + per_remote_only
+        except TypeError:
+            valid_per_remote_params = \
+                list(override_params) + list(per_remote_only)
+
+        if isinstance(remote, dict):
+            self.id = next(iter(remote))
+            self.get_url()
+
+            per_remote_conf = salt.utils.data.repack_dictlist(
+                remote[self.id],
+                strict=True,
+                recurse=True,
+                key_cb=six.text_type,
+                val_cb=enforce_types)
+
+            if not per_remote_conf:
+                log.critical(
+                    'Invalid per-remote configuration for %s remote \'%s\'. '
+                    'If no per-remote parameters are being specified, there '
+                    'may be a trailing colon after the URL, which should be '
+                    'removed. Check the master configuration file.',
+                    self.role, self.id
+                )
+                failhard(self.role)
+
+            per_remote_errors = False
+            for param in (x for x in per_remote_conf
+                          if x not in valid_per_remote_params):
+                per_remote_errors = True
+                if param in AUTH_PARAMS \
+                        and self.provider not in AUTH_PROVIDERS:
+                    msg = (
+                        '{0} authentication parameter \'{1}\' (from remote '
+                        '\'{2}\') is only supported by the following '
+                        'provider(s): {3}. Current {0}_provider is \'{4}\'.'
+                        .format(
+                            self.role,
+                            param,
+                            self.id,
+                            ', '.join(AUTH_PROVIDERS),
+                            self.provider
+                        )
+                    )
+                    if self.role == 'gitfs':
+                        msg += (
+                            'See the GitFS Walkthrough in the Salt '
+                            'documentation for further information.'
+                        )
+                    log.critical(msg)
+                else:
+                    msg = (
+                        'Invalid {0} configuration parameter \'{1}\' in '
+                        'remote \'{2}\'. Valid parameters are: {3}.'.format(
+                            self.role,
+                            param,
+                            self.id,
+                            ', '.join(valid_per_remote_params)
+                        )
+                    )
+                    if self.role == 'gitfs':
+                        msg += (
+                            ' See the GitFS Walkthrough in the Salt '
+                            'documentation for further information.'
+                        )
+                    log.critical(msg)
+
+            if per_remote_errors:
+                failhard(self.role)
+
+            self.conf.update(per_remote_conf)
+        else:
+            self.id = remote
+            self.get_url()
+
+        # Winrepo doesn't support the 'root' option, but it still must be part
+        # of the GitProvider object because other code depends on it. Add it as
+        # an empty string.
+        if 'root' not in self.conf:
+            self.conf['root'] = ''
+
+        if self.role == 'winrepo' and 'name' not in self.conf:
+            # Ensure that winrepo has the 'name' parameter set if it wasn't
+            # provided. Default to the last part of the URL, minus the .git if
+            # it is present.
+            self.conf['name'] = self.url.rsplit('/', 1)[-1]
+            # Remove trailing .git from name
+            if self.conf['name'].lower().endswith('.git'):
+                self.conf['name'] = self.conf['name'][:-4]
+
+        if 'mountpoint' in self.conf:
+            # Remove the 'salt://' from the beginning of the mountpoint, as
+            # well as any additional leading/trailing slashes
+            self.conf['mountpoint'] = \
+                salt.utils.url.strip_proto(self.conf['mountpoint']).strip('/')
+        else:
+            # For providers which do not use a mountpoint, assume the
+            # filesystem is mounted at the root of the fileserver.
+            self.conf['mountpoint'] = ''
+
+        if 'saltenv' not in self.conf:
+            self.conf['saltenv'] = {}
+        else:
+            for saltenv, saltenv_conf in six.iteritems(self.conf['saltenv']):
+                if 'mountpoint' in saltenv_conf:
+                    saltenv_ptr = self.conf['saltenv'][saltenv]
+                    saltenv_ptr['mountpoint'] = \
+                        salt.utils.url.strip_proto(saltenv_ptr['mountpoint'])
+
+        for key, val in six.iteritems(self.conf):
+            if key not in PER_SALTENV_PARAMS and not hasattr(self, key):
+                setattr(self, key, val)
+
+        for key in PER_SALTENV_PARAMS:
+            if key != 'ref':
+                setattr(self, '_' + key, self.conf[key])
+            self.add_conf_overlay(key)
+
+        if not hasattr(self, 'refspecs'):
+            # This was not specified as a per-remote overrideable parameter
+            # when instantiating an instance of a GitBase subclass. Make sure
+            # that we set this attribute so we at least have a sane default and
+            # are able to fetch.
+            key = '{0}_refspecs'.format(self.role)
+            try:
+                default_refspecs = _DEFAULT_MASTER_OPTS[key]
+            except KeyError:
+                log.critical(
+                    'The \'%s\' option has no default value in '
+                    'salt/config/__init__.py.', key
+                )
+                failhard(self.role)
+
+            setattr(self, 'refspecs', default_refspecs)
+            log.debug(
+                'The \'refspecs\' option was not explicitly defined as a '
+                'configurable parameter. Falling back to %s for %s remote '
+                '\'%s\'.', default_refspecs, self.role, self.id
+            )
+
+        for item in ('env_whitelist', 'env_blacklist'):
+            val = getattr(self, item, None)
+            if val:
+                salt.utils.versions.warn_until(
+                    'Neon',
+                    'The gitfs_{0} config option (and {0} per-remote config '
+                    'option) have been renamed to gitfs_salt{0} (and '
+                    'salt{0}). Please update your configuration.'.format(item)
+                )
+                setattr(self, 'salt{0}'.format(item), val)
+
+        # Discard the conf dictionary since we have set all of the config
+        # params as attributes
+        delattr(self, 'conf')
+
+        # Normalize components of the ref_types configuration and check for
+        # invalid configuration.
+        if hasattr(self, 'ref_types'):
+            self.ref_types = [x.lower() for x in self.ref_types]
+            invalid_ref_types = [x for x in self.ref_types
+                                 if x not in VALID_REF_TYPES]
+            if invalid_ref_types:
+                log.critical(
+                    'The following ref_types for %s remote \'%s\' are '
+                    'invalid: %s. The supported values are: %s',
+                    self.role,
+                    self.id,
+                    ', '.join(invalid_ref_types),
+                    ', '.join(VALID_REF_TYPES),
+                )
+                failhard(self.role)
+
+        if not isinstance(self.url, six.string_types):
+            log.critical(
+                'Invalid %s remote \'%s\'. Remotes must be strings, you '
+                'may need to enclose the URL in quotes', self.role, self.id
+            )
+            failhard(self.role)
+
+        hash_type = getattr(hashlib, self.opts.get('hash_type', 'md5'))
+        if six.PY3:
+            # We loaded this data from yaml configuration files, so, its safe
+            # to use UTF-8
+            self.hash = hash_type(self.id.encode('utf-8')).hexdigest()
+        else:
+            self.hash = hash_type(self.id).hexdigest()
+        self.cachedir_basename = getattr(self, 'name', self.hash)
+        self.cachedir = salt.utils.path.join(cache_root, self.cachedir_basename)
+        self.linkdir = salt.utils.path.join(cache_root,
+                                            'links',
+                                            self.cachedir_basename)
+
+        if not os.path.isdir(self.cachedir):
+            os.makedirs(self.cachedir)
+
+        try:
+            self.new = self.init_remote()
+        except Exception as exc:
+            msg = ('Exception caught while initializing {0} remote \'{1}\': '
+                   '{2}'.format(self.role, self.id, exc))
+            if isinstance(self, GitPython):
+                msg += ' Perhaps git is not available.'
+            log.critical(msg, exc_info=True)
+            failhard(self.role)
+
+    def _get_envs_from_ref_paths(self, refs):
+        '''
+        Return the names of remote refs (stripped of the remote name) and tags
+        which are map to the branches and tags.
+        '''
+        def _check_ref(env_set, rname):
+            '''
+            Add the appropriate saltenv(s) to the set
+            '''
+            if rname in self.saltenv_revmap:
+                env_set.update(self.saltenv_revmap[rname])
+            else:
+                if rname == self.base:
+                    env_set.add('base')
+                elif not self.disable_saltenv_mapping:
+                    env_set.add(rname)
+
+        use_branches = 'branch' in self.ref_types
+        use_tags = 'tag' in self.ref_types
+
+        ret = set()
+        if salt.utils.stringutils.is_hex(self.base):
+            # gitfs_base or per-saltenv 'base' may point to a commit ID, which
+            # would not show up in the refs. Make sure we include it.
+            ret.add('base')
+        for ref in salt.utils.data.decode(refs):
+            if ref.startswith('refs/'):
+                ref = ref[5:]
+            rtype, rname = ref.split('/', 1)
+            if rtype == 'remotes' and use_branches:
+                parted = rname.partition('/')
+                rname = parted[2] if parted[2] else parted[0]
+                _check_ref(ret, rname)
+            elif rtype == 'tags' and use_tags:
+                _check_ref(ret, rname)
+
+        return ret
+
+    def _get_lock_file(self, lock_type='update'):
+        return salt.utils.path.join(self.gitdir, lock_type + '.lk')
+
+    @classmethod
+    def add_conf_overlay(cls, name):
+        '''
+        Programmatically determine config value based on the desired saltenv
+        '''
+        def _getconf(self, tgt_env='base'):
+            strip_sep = lambda x: x.rstrip(os.sep) \
+                if name in ('root', 'mountpoint') \
+                else x
+            if self.role != 'gitfs':
+                return strip_sep(getattr(self, '_' + name))
+            # Get saltenv-specific configuration
+            saltenv_conf = self.saltenv.get(tgt_env, {})
+            if name == 'ref':
+                def _get_per_saltenv(tgt_env):
+                    if name in saltenv_conf:
+                        return saltenv_conf[name]
+                    elif tgt_env in self.global_saltenv \
+                            and name in self.global_saltenv[tgt_env]:
+                        return self.global_saltenv[tgt_env][name]
+                    else:
+                        return None
+
+                # Return the all_saltenvs branch/tag if it is configured
+                per_saltenv_ref = _get_per_saltenv(tgt_env)
+                try:
+                    all_saltenvs_ref = self.all_saltenvs
+                    if per_saltenv_ref and all_saltenvs_ref != per_saltenv_ref:
+                        log.debug(
+                            'The per-saltenv configuration has mapped the '
+                            '\'%s\' branch/tag to saltenv \'%s\' for %s '
+                            'remote \'%s\', but this remote has '
+                            'all_saltenvs set to \'%s\'. The per-saltenv '
+                            'mapping will be ignored in favor of \'%s\'.',
+                            per_saltenv_ref, tgt_env, self.role, self.id,
+                            all_saltenvs_ref, all_saltenvs_ref
+                        )
+                    return all_saltenvs_ref
+                except AttributeError:
+                    # all_saltenvs not configured for this remote
+                    pass
+
+                if tgt_env == 'base':
+                    return self.base
+                elif self.disable_saltenv_mapping:
+                    if per_saltenv_ref is None:
+                        log.debug(
+                            'saltenv mapping is diabled for %s remote \'%s\' '
+                            'and saltenv \'%s\' is not explicitly mapped',
+                            self.role, self.id, tgt_env
+                        )
+                    return per_saltenv_ref
+                else:
+                    return per_saltenv_ref or tgt_env
+
+            if name in saltenv_conf:
+                return strip_sep(saltenv_conf[name])
+            elif tgt_env in self.global_saltenv \
+                    and name in self.global_saltenv[tgt_env]:
+                return strip_sep(self.global_saltenv[tgt_env][name])
+            else:
+                return strip_sep(getattr(self, '_' + name))
+        setattr(cls, name, _getconf)
+
+    def check_root(self):
+        '''
+        Check if the relative root path exists in the checked-out copy of the
+        remote. Return the full path to that relative root if it does exist,
+        otherwise return None.
+        '''
+        # No need to pass an environment to self.root() here since per-saltenv
+        # configuration is a gitfs-only feature and check_root() is not used
+        # for gitfs.
+        root_dir = salt.utils.path.join(self.cachedir, self.root()).rstrip(os.sep)
+        if os.path.isdir(root_dir):
+            return root_dir
+        log.error(
+            'Root path \'%s\' not present in %s remote \'%s\', '
+            'skipping.', self.root(), self.role, self.id
+        )
+        return None
+
+    def clean_stale_refs(self):
+        '''
+        Remove stale refs so that they are no longer seen as fileserver envs
+        '''
+        cleaned = []
+        cmd_str = 'git remote prune origin'
+
+        # Attempt to force all output to plain ascii english, which is what some parsing code
+        # may expect.
+        # According to stackoverflow (http://goo.gl/l74GC8), we are setting LANGUAGE as well
+        # just to be sure.
+        env = os.environ.copy()
+        env[b"LANGUAGE"] = b"C"
+        env[b"LC_ALL"] = b"C"
+
+        cmd = subprocess.Popen(
+            shlex.split(cmd_str),
+            close_fds=not salt.utils.platform.is_windows(),
+            cwd=os.path.dirname(self.gitdir),
+            env=env,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.STDOUT)
+        output = cmd.communicate()[0]
+        if six.PY3:
+            output = output.decode(__salt_system_encoding__)
+        if cmd.returncode != 0:
+            log.warning(
+                'Failed to prune stale branches for %s remote \'%s\'. '
+                'Output from \'%s\' follows:\n%s',
+                self.role, self.id, cmd_str, output
+            )
+        else:
+            marker = ' * [pruned] '
+            for line in salt.utils.itertools.split(output, '\n'):
+                if line.startswith(marker):
+                    cleaned.append(line[len(marker):].strip())
+            if cleaned:
+                log.debug(
+                    '%s pruned the following stale refs: %s',
+                    self.role, ', '.join(cleaned)
+                )
+        return cleaned
+
+    def clear_lock(self, lock_type='update'):
+        '''
+        Clear update.lk
+        '''
+        lock_file = self._get_lock_file(lock_type=lock_type)
+
+        def _add_error(errlist, exc):
+            msg = ('Unable to remove update lock for {0} ({1}): {2} '
+                   .format(self.url, lock_file, exc))
+            log.debug(msg)
+            errlist.append(msg)
+
+        success = []
+        failed = []
+
+        try:
+            os.remove(lock_file)
+        except OSError as exc:
+            if exc.errno == errno.ENOENT:
+                # No lock file present
+                pass
+            elif exc.errno == errno.EISDIR:
+                # Somehow this path is a directory. Should never happen
+                # unless some wiseguy manually creates a directory at this
+                # path, but just in case, handle it.
+                try:
+                    shutil.rmtree(lock_file)
+                except OSError as exc:
+                    _add_error(failed, exc)
+            else:
+                _add_error(failed, exc)
+        else:
+            msg = 'Removed {0} lock for {1} remote \'{2}\''.format(
+                lock_type,
+                self.role,
+                self.id
+            )
+            log.debug(msg)
+            success.append(msg)
+        return success, failed
+
+    def enforce_git_config(self):
+        '''
+        For the config options which need to be maintained in the git config,
+        ensure that the git config file is configured as desired.
+        '''
+        git_config = os.path.join(self.gitdir, 'config')
+        conf = salt.utils.configparser.GitConfigParser()
+        if not conf.read(git_config):
+            log.error('Failed to read from git config file %s', git_config)
+        else:
+            # We are currently enforcing the following git config items:
+            # 1. Fetch URL
+            # 2. refspecs used in fetch
+            # 3. http.sslVerify
+            conf_changed = False
+            remote_section = 'remote "origin"'
+
+            # 1. URL
+            try:
+                url = conf.get(remote_section, 'url')
+            except salt.utils.configparser.NoSectionError:
+                # First time we've init'ed this repo, we need to add the
+                # section for the remote to the git config
+                conf.add_section(remote_section)
+                conf_changed = True
+                url = None
+            log.debug(
+                'Current fetch URL for %s remote \'%s\': %s (desired: %s)',
+                self.role, self.id, url, self.url
+            )
+            if url != self.url:
+                conf.set(remote_section, 'url', self.url)
+                log.debug(
+                    'Fetch URL for %s remote \'%s\' set to %s',
+                    self.role, self.id, self.url
+                )
+                conf_changed = True
+
+            # 2. refspecs
+            try:
+                refspecs = sorted(
+                    conf.get(remote_section, 'fetch', as_list=True))
+            except salt.utils.configparser.NoOptionError:
+                # No 'fetch' option present in the remote section. Should never
+                # happen, but if it does for some reason, don't let it cause a
+                # traceback.
+                refspecs = []
+            desired_refspecs = sorted(self.refspecs)
+            log.debug(
+                'Current refspecs for %s remote \'%s\': %s (desired: %s)',
+                self.role, self.id, refspecs, desired_refspecs
+            )
+            if refspecs != desired_refspecs:
+                conf.set_multivar(remote_section, 'fetch', self.refspecs)
+                log.debug(
+                    'Refspecs for %s remote \'%s\' set to %s',
+                    self.role, self.id, desired_refspecs
+                )
+                conf_changed = True
+
+            # 3. http.sslVerify
+            try:
+                ssl_verify = conf.get('http', 'sslVerify')
+            except salt.utils.configparser.NoSectionError:
+                conf.add_section('http')
+                ssl_verify = None
+            except salt.utils.configparser.NoOptionError:
+                ssl_verify = None
+            desired_ssl_verify = six.text_type(self.ssl_verify).lower()
+            log.debug(
+                'Current http.sslVerify for %s remote \'%s\': %s (desired: %s)',
+                self.role, self.id, ssl_verify, desired_ssl_verify
+            )
+            if ssl_verify != desired_ssl_verify:
+                conf.set('http', 'sslVerify', desired_ssl_verify)
+                log.debug(
+                    'http.sslVerify for %s remote \'%s\' set to %s',
+                    self.role, self.id, desired_ssl_verify
+                )
+                conf_changed = True
+
+            # Write changes, if necessary
+            if conf_changed:
+                with salt.utils.files.fopen(git_config, 'w') as fp_:
+                    conf.write(fp_)
+                    log.debug(
+                        'Config updates for %s remote \'%s\' written to %s',
+                        self.role, self.id, git_config
+                    )
+
+    def fetch(self):
+        '''
+        Fetch the repo. If the local copy was updated, return True. If the
+        local copy was already up-to-date, return False.
+
+        This function requires that a _fetch() function be implemented in a
+        sub-class.
+        '''
+        try:
+            with self.gen_lock(lock_type='update'):
+                log.debug('Fetching %s remote \'%s\'', self.role, self.id)
+                # Run provider-specific fetch code
+                return self._fetch()
+        except GitLockError as exc:
+            if exc.errno == errno.EEXIST:
+                log.warning(
+                    'Update lock file is present for %s remote \'%s\', '
+                    'skipping. If this warning persists, it is possible that '
+                    'the update process was interrupted, but the lock could '
+                    'also have been manually set. Removing %s or running '
+                    '\'salt-run cache.clear_git_lock %s type=update\' will '
+                    'allow updates to continue for this remote.',
+                    self.role,
+                    self.id,
+                    self._get_lock_file(lock_type='update'),
+                    self.role,
+                )
+            return False
+
+    def _lock(self, lock_type='update', failhard=False):
+        '''
+        Place a lock file if (and only if) it does not already exist.
+        '''
+        try:
+            fh_ = os.open(self._get_lock_file(lock_type),
+                          os.O_CREAT | os.O_EXCL | os.O_WRONLY)
+            with os.fdopen(fh_, 'wb'):
+                # Write the lock file and close the filehandle
+                os.write(fh_, salt.utils.stringutils.to_bytes(six.text_type(os.getpid())))
+        except (OSError, IOError) as exc:
+            if exc.errno == errno.EEXIST:
+                with salt.utils.files.fopen(self._get_lock_file(lock_type), 'r') as fd_:
+                    try:
+                        pid = int(salt.utils.stringutils.to_unicode(fd_.readline()).rstrip())
+                    except ValueError:
+                        # Lock file is empty, set pid to 0 so it evaluates as
+                        # False.
+                        pid = 0
+                global_lock_key = self.role + '_global_lock'
+                lock_file = self._get_lock_file(lock_type=lock_type)
+                if self.opts[global_lock_key]:
+                    msg = (
+                        '{0} is enabled and {1} lockfile {2} is present for '
+                        '{3} remote \'{4}\'.'.format(
+                            global_lock_key,
+                            lock_type,
+                            lock_file,
+                            self.role,
+                            self.id,
+                        )
+                    )
+                    if pid:
+                        msg += ' Process {0} obtained the lock'.format(pid)
+                        if not pid_exists(pid):
+                            msg += (' but this process is not running. The '
+                                    'update may have been interrupted. If '
+                                    'using multi-master with shared gitfs '
+                                    'cache, the lock may have been obtained '
+                                    'by another master.')
+                    log.warning(msg)
+                    if failhard:
+                        raise exc
+                    return
+                elif pid and pid_exists(pid):
+                    log.warning('Process %d has a %s %s lock (%s)',
+                                pid, self.role, lock_type, lock_file)
+                    if failhard:
+                        raise
+                    return
+                else:
+                    if pid:
+                        log.warning(
+                            'Process %d has a %s %s lock (%s), but this '
+                            'process is not running. Cleaning up lock file.',
+                            pid, self.role, lock_type, lock_file
+                        )
+                    success, fail = self.clear_lock()
+                    if success:
+                        return self._lock(lock_type='update',
+                                          failhard=failhard)
+                    elif failhard:
+                        raise
+                    return
+            else:
+                msg = 'Unable to set {0} lock for {1} ({2}): {3} '.format(
+                    lock_type,
+                    self.id,
+                    self._get_lock_file(lock_type),
+                    exc
+                )
+                log.error(msg, exc_info=True)
+                raise GitLockError(exc.errno, msg)
+        msg = 'Set {0} lock for {1} remote \'{2}\''.format(
+            lock_type,
+            self.role,
+            self.id
+        )
+        log.debug(msg)
+        return msg
+
+    def lock(self):
+        '''
+        Place an lock file and report on the success/failure. This is an
+        interface to be used by the fileserver runner, so it is hard-coded to
+        perform an update lock. We aren't using the gen_lock()
+        contextmanager here because the lock is meant to stay and not be
+        automatically removed.
+        '''
+        success = []
+        failed = []
+        try:
+            result = self._lock(lock_type='update')
+        except GitLockError as exc:
+            failed.append(exc.strerror)
+        else:
+            if result is not None:
+                success.append(result)
+        return success, failed
+
+    @contextlib.contextmanager
+    def gen_lock(self, lock_type='update', timeout=0, poll_interval=0.5):
+        '''
+        Set and automatically clear a lock
+        '''
+        if not isinstance(lock_type, six.string_types):
+            raise GitLockError(
+                errno.EINVAL,
+                'Invalid lock_type \'{0}\''.format(lock_type)
+            )
+
+        # Make sure that we have a positive integer timeout, otherwise just set
+        # it to zero.
+        try:
+            timeout = int(timeout)
+        except ValueError:
+            timeout = 0
+        else:
+            if timeout < 0:
+                timeout = 0
+
+        if not isinstance(poll_interval, (six.integer_types, float)) \
+                or poll_interval < 0:
+            poll_interval = 0.5
+
+        if poll_interval > timeout:
+            poll_interval = timeout
+
+        lock_set = False
+        try:
+            time_start = time.time()
+            while True:
+                try:
+                    self._lock(lock_type=lock_type, failhard=True)
+                    lock_set = True
+                    yield
+                    # Break out of his loop once we've yielded the lock, to
+                    # avoid continued attempts to iterate and establish lock
+                    break
+                except (OSError, IOError, GitLockError) as exc:
+                    if not timeout or time.time() - time_start > timeout:
+                        raise GitLockError(exc.errno, exc.strerror)
+                    else:
+                        log.debug(
+                            'A %s lock is already present for %s remote '
+                            '\'%s\', sleeping %f second(s)',
+                            lock_type, self.role, self.id, poll_interval
+                        )
+                        time.sleep(poll_interval)
+                        continue
+        finally:
+            if lock_set:
+                self.clear_lock(lock_type=lock_type)
+
+    def init_remote(self):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+    def checkout(self):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+    def dir_list(self, tgt_env):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+    def env_is_exposed(self, tgt_env):
+        '''
+        Check if an environment is exposed by comparing it against a whitelist
+        and blacklist.
+        '''
+        return salt.utils.stringutils.check_whitelist_blacklist(
+            tgt_env,
+            whitelist=self.saltenv_whitelist,
+            blacklist=self.saltenv_blacklist,
+        )
+
+    def _fetch(self):
+        '''
+        Provider-specific code for fetching, must be implemented in a
+        sub-class.
+        '''
+        raise NotImplementedError()
+
+    def envs(self):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+    def file_list(self, tgt_env):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+    def find_file(self, path, tgt_env):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+    def get_checkout_target(self):
+        '''
+        Resolve dynamically-set branch
+        '''
+        if self.role == 'git_pillar' and self.branch == '__env__':
+            try:
+                return self.all_saltenvs
+            except AttributeError:
+                # all_saltenvs not configured for this remote
+                pass
+            target = self.opts.get('pillarenv') \
+                or self.opts.get('saltenv') \
+                or 'base'
+            return self.base \
+                if target == 'base' \
+                else six.text_type(target)
+        return self.branch
+
+    def get_tree(self, tgt_env):
+        '''
+        Return a tree object for the specified environment
+        '''
+        if not self.env_is_exposed(tgt_env):
+            return None
+
+        tgt_ref = self.ref(tgt_env)
+        if tgt_ref is None:
+            return None
+
+        for ref_type in self.ref_types:
+            try:
+                func_name = 'get_tree_from_{0}'.format(ref_type)
+                func = getattr(self, func_name)
+            except AttributeError:
+                log.error(
+                    '%s class is missing function \'%s\'',
+                    self.__class__.__name__, func_name
+                )
+            else:
+                candidate = func(tgt_ref)
+                if candidate is not None:
+                    return candidate
+
+        # No matches found
+        return None
+
+    def get_url(self):
+        '''
+        Examine self.id and assign self.url (and self.branch, for git_pillar)
+        '''
+        if self.role in ('git_pillar', 'winrepo'):
+            # With winrepo and git_pillar, the remote is specified in the
+            # format '<branch> <url>', so that we can get a unique identifier
+            # to hash for each remote.
+            try:
+                self.branch, self.url = self.id.split(None, 1)
+            except ValueError:
+                self.branch = self.conf['branch']
+                self.url = self.id
+        else:
+            self.url = self.id
+
+    @property
+    def linkdir_walk(self):
+        '''
+        Return the expected result of an os.walk on the linkdir, based on the
+        mountpoint value.
+        '''
+        try:
+            # Use cached linkdir_walk if we've already run this
+            return self._linkdir_walk
+        except AttributeError:
+            self._linkdir_walk = []
+            try:
+                parts = self._mountpoint.split('/')
+            except AttributeError:
+                log.error(
+                    '%s class is missing a \'_mountpoint\' attribute',
+                    self.__class__.__name__
+                )
+            else:
+                for idx, item in enumerate(parts[:-1]):
+                    try:
+                        dirs = [parts[idx + 1]]
+                    except IndexError:
+                        dirs = []
+                    self._linkdir_walk.append((
+                        salt.utils.path.join(self.linkdir, *parts[:idx + 1]),
+                        dirs,
+                        []
+                    ))
+                try:
+                    # The linkdir itself goes at the beginning
+                    self._linkdir_walk.insert(0, (self.linkdir, [parts[0]], []))
+                except IndexError:
+                    pass
+            return self._linkdir_walk
+
+    def setup_callbacks(self):
+        '''
+        Only needed in pygit2, included in the base class for simplicty of use
+        '''
+        pass
+
+    def verify_auth(self):
+        '''
+        Override this function in a sub-class to implement auth checking.
+        '''
+        self.credentials = None
+        return True
+
+    def write_file(self, blob, dest):
+        '''
+        This function must be overridden in a sub-class
+        '''
+        raise NotImplementedError()
+
+
+class GitPython(GitProvider):
+    '''
+    Interface to GitPython
+    '''
+    def __init__(self, opts, remote, per_remote_defaults, per_remote_only,
+                 override_params, cache_root, role='gitfs'):
+        self.provider = 'gitpython'
+        super(GitPython, self).__init__(
+            opts, remote, per_remote_defaults, per_remote_only,
+            override_params, cache_root, role
+        )
+
+    def checkout(self):
+        '''
+        Checkout the configured branch/tag. We catch an "Exception" class here
+        instead of a specific exception class because the exceptions raised by
+        GitPython when running these functions vary in different versions of
+        GitPython.
+        '''
+        tgt_ref = self.get_checkout_target()
+        try:
+            head_sha = self.repo.rev_parse('HEAD').hexsha
+        except Exception:
+            # Should only happen the first time we are checking out, since
+            # we fetch first before ever checking anything out.
+            head_sha = None
+
+        # 'origin/' + tgt_ref ==> matches a branch head
+        # 'tags/' + tgt_ref + '@{commit}' ==> matches tag's commit
+        for rev_parse_target, checkout_ref in (
+                ('origin/' + tgt_ref, 'origin/' + tgt_ref),
+                ('tags/' + tgt_ref, 'tags/' + tgt_ref)):
+            try:
+                target_sha = self.repo.rev_parse(rev_parse_target).hexsha
+            except Exception:
+                # ref does not exist
+                continue
+            else:
+                if head_sha == target_sha:
+                    # No need to checkout, we're already up-to-date
+                    return self.check_root()
+
+            try:
+                with self.gen_lock(lock_type='checkout'):
+                    self.repo.git.checkout(checkout_ref)
+                    log.debug(
+                        '%s remote \'%s\' has been checked out to %s',
+                        self.role,
+                        self.id,
+                        checkout_ref
+                    )
+            except GitLockError as exc:
+                if exc.errno == errno.EEXIST:
+                    # Re-raise with a different strerror containing a
+                    # more meaningful error message for the calling
+                    # function.
+                    raise GitLockError(
+                        exc.errno,
+                        'Checkout lock exists for {0} remote \'{1}\''
+                        .format(self.role, self.id)
+                    )
+                else:
+                    log.error(
+                        'Error %d encountered obtaining checkout lock '
+                        'for %s remote \'%s\'',
+                        exc.errno,
+                        self.role,
+                        self.id
+                    )
+                    return None
+            except Exception:
+                continue
+            return self.check_root()
+        log.error(
+            'Failed to checkout %s from %s remote \'%s\': remote ref does '
+            'not exist', tgt_ref, self.role, self.id
+        )
+        return None
+
+    def init_remote(self):
+        '''
+        Initialize/attach to a remote using GitPython. Return a boolean
+        which will let the calling function know whether or not a new repo was
+        initialized by this function.
+        '''
+        new = False
+        if not os.listdir(self.cachedir):
+            # Repo cachedir is empty, initialize a new repo there
+            self.repo = git.Repo.init(self.cachedir)
+            new = True
+        else:
+            # Repo cachedir exists, try to attach
+            try:
+                self.repo = git.Repo(self.cachedir)
+            except git.exc.InvalidGitRepositoryError:
+                log.error(_INVALID_REPO, self.cachedir, self.url, self.role)
+                return new
+
+        self.gitdir = salt.utils.path.join(self.repo.working_dir, '.git')
+        self.enforce_git_config()
+
+        return new
+
+    def dir_list(self, tgt_env):
+        '''
+        Get list of directories for the target environment using GitPython
+        '''
+        ret = set()
+        tree = self.get_tree(tgt_env)
+        if not tree:
+            return ret
+        if self.root(tgt_env):
+            try:
+                tree = tree / self.root(tgt_env)
+            except KeyError:
+                return ret
+            relpath = lambda path: os.path.relpath(path, self.root(tgt_env))
+        else:
+            relpath = lambda path: path
+        add_mountpoint = lambda path: salt.utils.path.join(
+            self.mountpoint(tgt_env), path, use_posixpath=True)
+        for blob in tree.traverse():
+            if isinstance(blob, git.Tree):
+                ret.add(add_mountpoint(relpath(blob.path)))
+        if self.mountpoint(tgt_env):
+            ret.add(self.mountpoint(tgt_env))
+        return ret
+
+    def envs(self):
+        '''
+        Check the refs and return a list of the ones which can be used as salt
+        environments.
+        '''
+        ref_paths = [x.path for x in self.repo.refs]
+        return self._get_envs_from_ref_paths(ref_paths)
+
+    def _fetch(self):
+        '''
+        Fetch the repo. If the local copy was updated, return True. If the
+        local copy was already up-to-date, return False.
+        '''
+        origin = self.repo.remotes[0]
+        try:
+            fetch_results = origin.fetch()
+        except AssertionError:
+            fetch_results = origin.fetch()
+
+        new_objs = False
+        for fetchinfo in fetch_results:
+            if fetchinfo.old_commit is not None:
+                log.debug(
+                    '%s has updated \'%s\' for remote \'%s\' '
+                    'from %s to %s',
+                    self.role,
+                    fetchinfo.name,
+                    self.id,
+                    fetchinfo.old_commit.hexsha[:7],
+                    fetchinfo.commit.hexsha[:7]
+                )
+                new_objs = True
+            elif fetchinfo.flags in (fetchinfo.NEW_TAG,
+                                     fetchinfo.NEW_HEAD):
+                log.debug(
+                    '%s has fetched new %s \'%s\' for remote \'%s\'',
+                    self.role,
+                    'tag' if fetchinfo.flags == fetchinfo.NEW_TAG else 'head',
+                    fetchinfo.name,
+                    self.id
+                )
+                new_objs = True
+
+        cleaned = self.clean_stale_refs()
+        return True if (new_objs or cleaned) else None
+
+    def file_list(self, tgt_env):
+        '''
+        Get file list for the target environment using GitPython
+        '''
+        files = set()
+        symlinks = {}
+        tree = self.get_tree(tgt_env)
+        if not tree:
+            # Not found, return empty objects
+            return files, symlinks
+        if self.root(tgt_env):
+            try:
+                tree = tree / self.root(tgt_env)
+            except KeyError:
+                return files, symlinks
+            relpath = lambda path: os.path.relpath(path, self.root(tgt_env))
+        else:
+            relpath = lambda path: path
+        add_mountpoint = lambda path: salt.utils.path.join(
+            self.mountpoint(tgt_env), path, use_posixpath=True)
+        for file_blob in tree.traverse():
+            if not isinstance(file_blob, git.Blob):
+                continue
+            file_path = add_mountpoint(relpath(file_blob.path))
+            files.add(file_path)
+            if stat.S_ISLNK(file_blob.mode):
+                stream = six.StringIO()
+                file_blob.stream_data(stream)
+                stream.seek(0)
+                link_tgt = stream.read()
+                stream.close()
+                symlinks[file_path] = link_tgt
+        return files, symlinks
+
+    def find_file(self, path, tgt_env):
+        '''
+        Find the specified file in the specified environment
+        '''
+        tree = self.get_tree(tgt_env)
+        if not tree:
+            # Branch/tag/SHA not found in repo
+            return None, None, None
+        blob = None
+        depth = 0
+        while True:
+            depth += 1
+            if depth > SYMLINK_RECURSE_DEPTH:
+                blob = None
+                break
+            try:
+                file_blob = tree / path
+                if stat.S_ISLNK(file_blob.mode):
+                    # Path is a symlink. The blob data corresponding to
+                    # this path's object ID will be the target of the
+                    # symlink. Follow the symlink and set path to the
+                    # location indicated in the blob data.
+                    stream = six.StringIO()
+                    file_blob.stream_data(stream)
+                    stream.seek(0)
+                    link_tgt = stream.read()
+                    stream.close()
+                    path = salt.utils.path.join(
+                        os.path.dirname(path), link_tgt, use_posixpath=True)
+                else:
+                    blob = file_blob
+                    if isinstance(blob, git.Tree):
+                        # Path is a directory, not a file.
+                        blob = None
+                    break
+            except KeyError:
+                # File not found or repo_path points to a directory
+                blob = None
+                break
+        if isinstance(blob, git.Blob):
+            return blob, blob.hexsha, blob.mode
+        return None, None, None
+
+    def get_tree_from_branch(self, ref):
+        '''
+        Return a git.Tree object matching a head ref fetched into
+        refs/remotes/origin/
+        '''
+        try:
+            return git.RemoteReference(
+                self.repo,
+                'refs/remotes/origin/{0}'.format(ref)).commit.tree
+        except ValueError:
+            return None
+
+    def get_tree_from_tag(self, ref):
+        '''
+        Return a git.Tree object matching a tag ref fetched into refs/tags/
+        '''
+        try:
+            return git.TagReference(
+                self.repo,
+                'refs/tags/{0}'.format(ref)).commit.tree
+        except ValueError:
+            return None
+
+    def get_tree_from_sha(self, ref):
+        '''
+        Return a git.Tree object matching a SHA
+        '''
+        try:
+            return self.repo.rev_parse(ref).tree
+        except (gitdb.exc.ODBError, AttributeError):
+            return None
+
+    def write_file(self, blob, dest):
+        '''
+        Using the blob object, write the file to the destination path
+        '''
+        with salt.utils.files.fopen(dest, 'wb+') as fp_:
+            blob.stream_data(fp_)
+
+
+class Pygit2(GitProvider):
+    '''
+    Interface to Pygit2
+    '''
+    def __init__(self, opts, remote, per_remote_defaults, per_remote_only,
+                 override_params, cache_root, role='gitfs'):
+        self.provider = 'pygit2'
+        super(Pygit2, self).__init__(
+            opts, remote, per_remote_defaults, per_remote_only,
+            override_params, cache_root, role
+        )
+
+    def peel(self, obj):
+        '''
+        Compatibility function for pygit2.Reference objects. Older versions of
+        pygit2 use .get_object() to return the object to which the reference
+        points, while newer versions use .peel(). In pygit2 0.27.4,
+        .get_object() was removed. This function will try .peel() first and
+        fall back to .get_object().
+        '''
+        try:
+            return obj.peel()
+        except AttributeError:
+            return obj.get_object()
+
+    def checkout(self):
+        '''
+        Checkout the configured branch/tag
+        '''
+        tgt_ref = self.get_checkout_target()
+        local_ref = 'refs/heads/' + tgt_ref
+        remote_ref = 'refs/remotes/origin/' + tgt_ref
+        tag_ref = 'refs/tags/' + tgt_ref
+
+        try:
+            local_head = self.repo.lookup_reference('HEAD')
+        except KeyError:
+            log.warning(
+                'HEAD not present in %s remote \'%s\'', self.role, self.id
+            )
+            return None
+
+        try:
+            head_sha = self.peel(local_head).hex
+        except AttributeError:
+            # Shouldn't happen, but just in case a future pygit2 API change
+            # breaks things, avoid a traceback and log an error.
+            log.error(
+                'Unable to get SHA of HEAD for %s remote \'%s\'',
+                self.role, self.id
+            )
+            return None
+        except KeyError:
+            head_sha = None
+
+        refs = self.repo.listall_references()
+
+        def _perform_checkout(checkout_ref, branch=True):
+            '''
+            DRY function for checking out either a branch or a tag
+            '''
+            try:
+                with self.gen_lock(lock_type='checkout'):
+                    # Checkout the local branch corresponding to the
+                    # remote ref.
+                    self.repo.checkout(checkout_ref)
+                    if branch:
+                        self.repo.reset(oid, pygit2.GIT_RESET_HARD)
+                return True
+            except GitLockError as exc:
+                if exc.errno == errno.EEXIST:
+                    # Re-raise with a different strerror containing a
+                    # more meaningful error message for the calling
+                    # function.
+                    raise GitLockError(
+                        exc.errno,
+                        'Checkout lock exists for {0} remote \'{1}\''
+                        .format(self.role, self.id)
+                    )
+                else:
+                    log.error(
+                        'Error %d encountered obtaining checkout lock '
+                        'for %s remote \'%s\'',
+                        exc.errno,
+                        self.role,
+                        self.id
+                    )
+            return False
+
+        try:
+            if remote_ref in refs:
+                # Get commit id for the remote ref
+                oid = self.peel(self.repo.lookup_reference(remote_ref)).id
+                if local_ref not in refs:
+                    # No local branch for this remote, so create one and point
+                    # it at the commit id of the remote ref
+                    self.repo.create_reference(local_ref, oid)
+
+                try:
+                    target_sha = \
+                        self.peel(self.repo.lookup_reference(remote_ref)).hex
+                except KeyError:
+                    log.error(
+                        'pygit2 was unable to get SHA for %s in %s remote '
+                        '\'%s\'', local_ref, self.role, self.id,
+                        exc_info=True
+                    )
+                    return None
+
+                # Only perform a checkout if HEAD and target are not pointing
+                # at the same SHA1.
+                if head_sha != target_sha:
+                    # Check existence of the ref in refs/heads/ which
+                    # corresponds to the local HEAD. Checking out local_ref
+                    # below when no local ref for HEAD is missing will raise an
+                    # exception in pygit2 >= 0.21. If this ref is not present,
+                    # create it. The "head_ref != local_ref" check ensures we
+                    # don't try to add this ref if it is not necessary, as it
+                    # would have been added above already. head_ref would be
+                    # the same as local_ref if the branch name was changed but
+                    # the cachedir was not (for example if a "name" parameter
+                    # was used in a git_pillar remote, or if we are using
+                    # winrepo which takes the basename of the repo as the
+                    # cachedir).
+                    head_ref = local_head.target
+                    # If head_ref is not a string, it will point to a
+                    # pygit2.Oid object and we are in detached HEAD mode.
+                    # Therefore, there is no need to add a local reference. If
+                    # head_ref == local_ref, then the local reference for HEAD
+                    # in refs/heads/ already exists and again, no need to add.
+                    if isinstance(head_ref, six.string_types) \
+                            and head_ref not in refs and head_ref != local_ref:
+                        branch_name = head_ref.partition('refs/heads/')[-1]
+                        if not branch_name:
+                            # Shouldn't happen, but log an error if it does
+                            log.error(
+                                'pygit2 was unable to resolve branch name from '
+                                'HEAD ref \'%s\' in %s remote \'%s\'',
+                                head_ref, self.role, self.id
+                            )
+                            return None
+                        remote_head = 'refs/remotes/origin/' + branch_name
+                        if remote_head not in refs:
+                            # No remote ref for HEAD exists. This can happen in
+                            # the first-time git_pillar checkout when when the
+                            # remote repo does not have a master branch. Since
+                            # we need a HEAD reference to keep pygit2 from
+                            # throwing an error, and none exists in
+                            # refs/remotes/origin, we'll just point HEAD at the
+                            # remote_ref.
+                            remote_head = remote_ref
+                        self.repo.create_reference(
+                            head_ref,
+                            self.repo.lookup_reference(remote_head).target
+                        )
+
+                    if not _perform_checkout(local_ref, branch=True):
+                        return None
+
+                # Return the relative root, if present
+                return self.check_root()
+
+            elif tag_ref in refs:
+                tag_obj = self.repo.revparse_single(tag_ref)
+                if not isinstance(tag_obj, pygit2.Commit):
+                    log.error(
+                        '%s does not correspond to pygit2.Commit object',
+                        tag_ref
+                    )
+                else:
+                    try:
+                        # If no AttributeError raised, this is an annotated tag
+                        tag_sha = tag_obj.target.hex
+                    except AttributeError:
+                        try:
+                            tag_sha = tag_obj.hex
+                        except AttributeError:
+                            # Shouldn't happen, but could if a future pygit2
+                            # API change breaks things.
+                            log.error(
+                                'Unable to resolve %s from %s remote \'%s\' '
+                                'to either an annotated or non-annotated tag',
+                                tag_ref, self.role, self.id,
+                                exc_info=True
+                            )
+                            return None
+                    log.debug('SHA of tag %s: %s', tgt_ref, tag_sha)
+
+                    if head_sha != tag_sha:
+                        if not _perform_checkout(tag_ref, branch=False):
+                            return None
+
+                    # Return the relative root, if present
+                    return self.check_root()
+        except GitLockError:
+            raise
+        except Exception as exc:
+            log.error(
+                'Failed to checkout %s from %s remote \'%s\': %s',
+                tgt_ref, self.role, self.id, exc,
+                exc_info=True
+            )
+            return None
+        log.error(
+            'Failed to checkout %s from %s remote \'%s\': remote ref '
+            'does not exist', tgt_ref, self.role, self.id
+        )
+        return None
+
+    def clean_stale_refs(self, local_refs=None):  # pylint: disable=arguments-differ
+        '''
+        Clean stale local refs so they don't appear as fileserver environments
+        '''
+        try:
+            if pygit2.GIT_FETCH_PRUNE:
+                # Don't need to clean anything, pygit2 can do it by itself
+                return []
+        except AttributeError:
+            # However, only in 0.26.2 and newer
+            pass
+        if self.credentials is not None:
+            log.debug(
+                'The installed version of pygit2 (%s) does not support '
+                'detecting stale refs for authenticated remotes, saltenvs '
+                'will not reflect branches/tags removed from remote \'%s\'',
+                PYGIT2_VERSION, self.id
+            )
+            return []
+        return super(Pygit2, self).clean_stale_refs()
+
+    def init_remote(self):
+        '''
+        Initialize/attach to a remote using pygit2. Return a boolean which
+        will let the calling function know whether or not a new repo was
+        initialized by this function.
+        '''
+        # https://github.com/libgit2/pygit2/issues/339
+        # https://github.com/libgit2/libgit2/issues/2122
+        home = os.path.expanduser('~')
+        pygit2.settings.search_path[pygit2.GIT_CONFIG_LEVEL_GLOBAL] = home
+        new = False
+        if not os.listdir(self.cachedir):
+            # Repo cachedir is empty, initialize a new repo there
+            self.repo = pygit2.init_repository(self.cachedir)
+            new = True
+        else:
+            # Repo cachedir exists, try to attach
+            try:
+                self.repo = pygit2.Repository(self.cachedir)
+            except KeyError:
+                log.error(_INVALID_REPO, self.cachedir, self.url, self.role)
+                return new
+
+        self.gitdir = salt.utils.path.join(self.repo.workdir, '.git')
+        self.enforce_git_config()
+
+        return new
+
+    def dir_list(self, tgt_env):
+        '''
+        Get a list of directories for the target environment using pygit2
+        '''
+        def _traverse(tree, blobs, prefix):
+            '''
+            Traverse through a pygit2 Tree object recursively, accumulating all
+            the empty directories within it in the "blobs" list
+            '''
+            for entry in iter(tree):
+                if entry.oid not in self.repo:
+                    # Entry is a submodule, skip it
+                    continue
+                blob = self.repo[entry.oid]
+                if not isinstance(blob, pygit2.Tree):
+                    continue
+                blobs.append(
+                    salt.utils.path.join(prefix, entry.name, use_posixpath=True)
+                )
+                if len(blob):
+                    _traverse(
+                        blob, blobs, salt.utils.path.join(
+                            prefix, entry.name, use_posixpath=True)
+                    )
+
+        ret = set()
+        tree = self.get_tree(tgt_env)
+        if not tree:
+            return ret
+        if self.root(tgt_env):
+            try:
+                oid = tree[self.root(tgt_env)].oid
+                tree = self.repo[oid]
+            except KeyError:
+                return ret
+            if not isinstance(tree, pygit2.Tree):
+                return ret
+            relpath = lambda path: os.path.relpath(path, self.root(tgt_env))
+        else:
+            relpath = lambda path: path
+        blobs = []
+        if len(tree):
+            _traverse(tree, blobs, self.root(tgt_env))
+        add_mountpoint = lambda path: salt.utils.path.join(
+            self.mountpoint(tgt_env), path, use_posixpath=True)
+        for blob in blobs:
+            ret.add(add_mountpoint(relpath(blob)))
+        if self.mountpoint(tgt_env):
+            ret.add(self.mountpoint(tgt_env))
+        return ret
+
+    def envs(self):
+        '''
+        Check the refs and return a list of the ones which can be used as salt
+        environments.
+        '''
+        ref_paths = self.repo.listall_references()
+        return self._get_envs_from_ref_paths(ref_paths)
+
+    def _fetch(self):
+        '''
+        Fetch the repo. If the local copy was updated, return True. If the
+        local copy was already up-to-date, return False.
+        '''
+        origin = self.repo.remotes[0]
+        refs_pre = self.repo.listall_references()
+        fetch_kwargs = {}
+        # pygit2 radically changed fetchiing in 0.23.2
+        if self.remotecallbacks is not None:
+            fetch_kwargs['callbacks'] = self.remotecallbacks
+        else:
+            if self.credentials is not None:
+                origin.credentials = self.credentials
+        try:
+            fetch_kwargs['prune'] = pygit2.GIT_FETCH_PRUNE
+        except AttributeError:
+            # pruning only available in pygit2 >= 0.26.2
+            pass
+        try:
+            fetch_results = origin.fetch(**fetch_kwargs)
+        except GitError as exc:
+            exc_str = get_error_message(exc).lower()
+            if 'unsupported url protocol' in exc_str \
+                    and isinstance(self.credentials, pygit2.Keypair):
+                log.error(
+                    'Unable to fetch SSH-based %s remote \'%s\'. '
+                    'You may need to add ssh:// to the repo string or '
+                    'libgit2 must be compiled with libssh2 to support '
+                    'SSH authentication.', self.role, self.id,
+                    exc_info=True
+                )
+            elif 'authentication required but no callback set' in exc_str:
+                log.error(
+                    '%s remote \'%s\' requires authentication, but no '
+                    'authentication configured', self.role, self.id,
+                    exc_info=True
+                )
+            else:
+                log.error(
+                    'Error occurred fetching %s remote \'%s\': %s',
+                    self.role, self.id, exc,
+                    exc_info=True
+                )
+            return False
+        try:
+            # pygit2.Remote.fetch() returns a dict in pygit2 < 0.21.0
+            received_objects = fetch_results['received_objects']
+        except (AttributeError, TypeError):
+            # pygit2.Remote.fetch() returns a class instance in
+            # pygit2 >= 0.21.0
+            received_objects = fetch_results.received_objects
+        if received_objects != 0:
+            log.debug(
+                '%s received %s objects for remote \'%s\'',
+                self.role, received_objects, self.id
+            )
+        else:
+            log.debug('%s remote \'%s\' is up-to-date', self.role, self.id)
+        refs_post = self.repo.listall_references()
+        cleaned = self.clean_stale_refs(local_refs=refs_post)
+        return True \
+            if (received_objects or refs_pre != refs_post or cleaned) \
+            else None
+
+    def file_list(self, tgt_env):
+        '''
+        Get file list for the target environment using pygit2
+        '''
+        def _traverse(tree, blobs, prefix):
+            '''
+            Traverse through a pygit2 Tree object recursively, accumulating all
+            the file paths and symlink info in the "blobs" dict
+            '''
+            for entry in iter(tree):
+                if entry.oid not in self.repo:
+                    # Entry is a submodule, skip it
+                    continue
+                obj = self.repo[entry.oid]
+                if isinstance(obj, pygit2.Blob):
+                    repo_path = salt.utils.path.join(
+                        prefix, entry.name, use_posixpath=True)
+                    blobs.setdefault('files', []).append(repo_path)
+                    if stat.S_ISLNK(tree[entry.name].filemode):
+                        link_tgt = self.repo[tree[entry.name].oid].data
+                        blobs.setdefault('symlinks', {})[repo_path] = link_tgt
+                elif isinstance(obj, pygit2.Tree):
+                    _traverse(
+                        obj, blobs, salt.utils.path.join(
+                            prefix, entry.name, use_posixpath=True)
+                    )
+
+        files = set()
+        symlinks = {}
+        tree = self.get_tree(tgt_env)
+        if not tree:
+            # Not found, return empty objects
+            return files, symlinks
+        if self.root(tgt_env):
+            try:
+                # This might need to be changed to account for a root that
+                # spans more than one directory
+                oid = tree[self.root(tgt_env)].oid
+                tree = self.repo[oid]
+            except KeyError:
+                return files, symlinks
+            if not isinstance(tree, pygit2.Tree):
+                return files, symlinks
+            relpath = lambda path: os.path.relpath(path, self.root(tgt_env))
+        else:
+            relpath = lambda path: path
+        blobs = {}
+        if len(tree):
+            _traverse(tree, blobs, self.root(tgt_env))
+        add_mountpoint = lambda path: salt.utils.path.join(
+            self.mountpoint(tgt_env), path, use_posixpath=True)
+        for repo_path in blobs.get('files', []):
+            files.add(add_mountpoint(relpath(repo_path)))
+        for repo_path, link_tgt in six.iteritems(blobs.get('symlinks', {})):
+            symlinks[add_mountpoint(relpath(repo_path))] = link_tgt
+        return files, symlinks
+
+    def find_file(self, path, tgt_env):
+        '''
+        Find the specified file in the specified environment
+        '''
+        tree = self.get_tree(tgt_env)
+        if not tree:
+            # Branch/tag/SHA not found in repo
+            return None, None, None
+        blob = None
+        mode = None
+        depth = 0
+        while True:
+            depth += 1
+            if depth > SYMLINK_RECURSE_DEPTH:
+                blob = None
+                break
+            try:
+                entry = tree[path]
+                mode = entry.filemode
+                if stat.S_ISLNK(mode):
+                    # Path is a symlink. The blob data corresponding to this
+                    # path's object ID will be the target of the symlink. Follow
+                    # the symlink and set path to the location indicated
+                    # in the blob data.
+                    link_tgt = self.repo[entry.oid].data
+                    path = salt.utils.path.join(
+                        os.path.dirname(path), link_tgt, use_posixpath=True)
+                else:
+                    blob = self.repo[entry.oid]
+                    if isinstance(blob, pygit2.Tree):
+                        # Path is a directory, not a file.
+                        blob = None
+                    break
+            except KeyError:
+                blob = None
+                break
+        if isinstance(blob, pygit2.Blob):
+            return blob, blob.hex, mode
+        return None, None, None
+
+    def get_tree_from_branch(self, ref):
+        '''
+        Return a pygit2.Tree object matching a head ref fetched into
+        refs/remotes/origin/
+        '''
+        try:
+            return self.peel(self.repo.lookup_reference(
+                'refs/remotes/origin/{0}'.format(ref))).tree
+        except KeyError:
+            return None
+
+    def get_tree_from_tag(self, ref):
+        '''
+        Return a pygit2.Tree object matching a tag ref fetched into refs/tags/
+        '''
+        try:
+            return self.peel(self.repo.lookup_reference(
+                'refs/tags/{0}'.format(ref))).tree
+        except KeyError:
+            return None
+
+    def get_tree_from_sha(self, ref):
+        '''
+        Return a pygit2.Tree object matching a SHA
+        '''
+        try:
+            return self.repo.revparse_single(ref).tree
+        except (KeyError, TypeError, ValueError, AttributeError):
+            return None
+
+    def setup_callbacks(self):
+        '''
+        Assign attributes for pygit2 callbacks
+        '''
+        if PYGIT2_VERSION >= _LooseVersion('0.23.2'):
+            self.remotecallbacks = pygit2.RemoteCallbacks(
+                credentials=self.credentials)
+            if not self.ssl_verify:
+                # Override the certificate_check function with a lambda that
+                # just returns True, thus skipping the cert check.
+                self.remotecallbacks.certificate_check = \
+                    lambda *args, **kwargs: True
+        else:
+            self.remotecallbacks = None
+            if not self.ssl_verify:
+                warnings.warn(
+                    'pygit2 does not support disabling the SSL certificate '
+                    'check in versions prior to 0.23.2 (installed: {0}). '
+                    'Fetches for self-signed certificates will fail.'.format(
+                        PYGIT2_VERSION
+                    )
+                )
+
+    def verify_auth(self):
+        '''
+        Check the username and password/keypair info for validity. If valid,
+        set a 'credentials' attribute consisting of the appropriate Pygit2
+        credentials object. Return False if a required auth param is not
+        present. Return True if the required auth parameters are present (or
+        auth is not configured), otherwise failhard if there is a problem with
+        authenticaion.
+        '''
+        self.credentials = None
+
+        if os.path.isabs(self.url):
+            # If the URL is an absolute file path, there is no authentication.
+            return True
+        elif not any(getattr(self, x, None) for x in AUTH_PARAMS):
+            # Auth information not configured for this remote
+            return True
+
+        def _incomplete_auth(missing):
+            '''
+            Helper function to log errors about missing auth parameters
+            '''
+            log.critical(
+                'Incomplete authentication information for %s remote '
+                '\'%s\'. Missing parameters: %s',
+                self.role, self.id, ', '.join(missing)
+            )
+            failhard(self.role)
+
+        def _key_does_not_exist(key_type, path):
+            '''
+            Helper function to log errors about missing key file
+            '''
+            log.critical(
+                'SSH %s (%s) for %s remote \'%s\' could not be found, path '
+                'may be incorrect. Note that it may be necessary to clear '
+                'git_pillar locks to proceed once this is resolved and the '
+                'master has been started back up. A warning will be logged '
+                'if this is the case, with instructions.',
+                key_type, path, self.role, self.id
+            )
+            failhard(self.role)
+
+        transport, _, address = self.url.partition('://')
+        if not address:
+            # Assume scp-like SSH syntax (user@domain.tld:relative/path.git)
+            transport = 'ssh'
+            address = self.url
+
+        transport = transport.lower()
+
+        if transport in ('git', 'file'):
+            # These transports do not use auth
+            return True
+
+        elif 'ssh' in transport:
+            required_params = ('pubkey', 'privkey')
+            user = address.split('@')[0]
+            if user == address:
+                # No '@' sign == no user. This is a problem.
+                log.critical(
+                    'Keypair specified for %s remote \'%s\', but remote URL '
+                    'is missing a username', self.role, self.id
+                )
+                failhard(self.role)
+
+            self.user = user
+            if all(bool(getattr(self, x, None)) for x in required_params):
+                keypair_params = [getattr(self, x, None) for x in
+                                  ('user', 'pubkey', 'privkey', 'passphrase')]
+                # Check pubkey and privkey to make sure file exists
+                for idx, key_type in ((1, 'pubkey'), (2, 'privkey')):
+                    key_path = keypair_params[idx]
+                    if key_path is not None:
+                        try:
+                            if not os.path.isfile(key_path):
+                                _key_does_not_exist(key_type, key_path)
+                        except TypeError:
+                            _key_does_not_exist(key_type, key_path)
+                self.credentials = pygit2.Keypair(*keypair_params)
+                return True
+            else:
+                missing_auth = [x for x in required_params
+                                if not bool(getattr(self, x, None))]
+                _incomplete_auth(missing_auth)
+
+        elif 'http' in transport:
+            required_params = ('user', 'password')
+            password_ok = all(
+                bool(getattr(self, x, None)) for x in required_params
+            )
+            no_password_auth = not any(
+                bool(getattr(self, x, None)) for x in required_params
+            )
+            if no_password_auth:
+                # No auth params were passed, assuming this is unauthenticated
+                # http(s).
+                return True
+            if password_ok:
+                if transport == 'http' and not self.insecure_auth:
+                    log.critical(
+                        'Invalid configuration for %s remote \'%s\'. '
+                        'Authentication is disabled by default on http '
+                        'remotes. Either set %s_insecure_auth to True in the '
+                        'master configuration file, set a per-remote config '
+                        'option named \'insecure_auth\' to True, or use https '
+                        'or ssh-based authentication.',
+                        self.role, self.id, self.role
+                    )
+                    failhard(self.role)
+                self.credentials = pygit2.UserPass(self.user, self.password)
+                return True
+            else:
+                missing_auth = [x for x in required_params
+                                if not bool(getattr(self, x, None))]
+                _incomplete_auth(missing_auth)
+        else:
+            log.critical(
+                'Invalid configuration for %s remote \'%s\'. Unsupported '
+                'transport \'%s\'.', self.role, self.id, transport
+            )
+            failhard(self.role)
+
+    def write_file(self, blob, dest):
+        '''
+        Using the blob object, write the file to the destination path
+        '''
+        with salt.utils.files.fopen(dest, 'wb+') as fp_:
+            fp_.write(blob.data)
+
+
+GIT_PROVIDERS = {
+    'pygit2': Pygit2,
+    'gitpython': GitPython,
+}
+
+
+class GitBase(object):
+    '''
+    Base class for gitfs/git_pillar
+    '''
+    def __init__(self, opts, remotes=None, per_remote_overrides=(),
+                 per_remote_only=PER_REMOTE_ONLY, global_only=GLOBAL_ONLY,
+                 git_providers=None, cache_root=None, init_remotes=True):
+        '''
+        IMPORTANT: If specifying a cache_root, understand that this is also
+        where the remotes will be cloned. A non-default cache_root is only
+        really designed right now for winrepo, as its repos need to be checked
+        out into the winrepo locations and not within the cachedir.
+
+        As of the 2018.3 release cycle, the classes used to interface with
+        Pygit2 and GitPython can be overridden by passing the git_providers
+        argument when spawning a class instance. This allows for one to write
+        classes which inherit from salt.utils.gitfs.Pygit2 or
+        salt.utils.gitfs.GitPython, and then direct one of the GitBase
+        subclasses (GitFS, GitPillar, WinRepo) to use the custom class. For
+        example:
+
+        .. code-block:: Python
+
+            import salt.utils.gitfs
+            from salt.fileserver.gitfs import PER_REMOTE_OVERRIDES, PER_REMOTE_ONLY
+
+            class CustomPygit2(salt.utils.gitfs.Pygit2):
+                def fetch_remotes(self):
+                    ...
+                    Alternate fetch behavior here
+                    ...
+
+            git_providers = {
+                'pygit2': CustomPygit2,
+                'gitpython': salt.utils.gitfs.GitPython,
+            }
+
+            gitfs = salt.utils.gitfs.GitFS(
+                __opts__,
+                __opts__['gitfs_remotes'],
+                per_remote_overrides=PER_REMOTE_OVERRIDES,
+                per_remote_only=PER_REMOTE_ONLY,
+                git_providers=git_providers)
+
+            gitfs.fetch_remotes()
+        '''
+        self.opts = opts
+        self.git_providers = git_providers if git_providers is not None \
+            else GIT_PROVIDERS
+        self.verify_provider()
+        if cache_root is not None:
+            self.cache_root = self.remote_root = cache_root
+        else:
+            self.cache_root = salt.utils.path.join(self.opts['cachedir'],
+                                                   self.role)
+            self.remote_root = salt.utils.path.join(self.cache_root, 'remotes')
+        self.env_cache = salt.utils.path.join(self.cache_root, 'envs.p')
+        self.hash_cachedir = salt.utils.path.join(self.cache_root, 'hash')
+        self.file_list_cachedir = salt.utils.path.join(
+            self.opts['cachedir'], 'file_lists', self.role)
+        if init_remotes:
+            self.init_remotes(
+                remotes if remotes is not None else [],
+                per_remote_overrides,
+                per_remote_only,
+                global_only)
+
+    def init_remotes(self, remotes, per_remote_overrides=(),
+                     per_remote_only=PER_REMOTE_ONLY,
+                     global_only=GLOBAL_ONLY):
+        '''
+        Initialize remotes
+        '''
+        # The global versions of the auth params (gitfs_user,
+        # gitfs_password, etc.) default to empty strings. If any of them
+        # are defined and the provider is not one that supports auth, then
+        # error out and do not proceed.
+        override_params = copy.deepcopy(per_remote_overrides)
+        global_auth_params = [
+            '{0}_{1}'.format(self.role, x) for x in AUTH_PARAMS
+            if self.opts['{0}_{1}'.format(self.role, x)]
+        ]
+        if self.provider in AUTH_PROVIDERS:
+            override_params += AUTH_PARAMS
+        elif global_auth_params:
+            msg = (
+                '{0} authentication was configured, but the \'{1}\' '
+                '{0}_provider does not support authentication. The '
+                'providers for which authentication is supported in {0} '
+                'are: {2}.'.format(
+                    self.role, self.provider, ', '.join(AUTH_PROVIDERS)
+                )
+            )
+            if self.role == 'gitfs':
+                msg += (
+                    ' See the GitFS Walkthrough in the Salt documentation '
+                    'for further information.'
+                )
+            log.critical(msg)
+            failhard(self.role)
+
+        per_remote_defaults = {}
+        global_values = set(override_params)
+        global_values.update(set(global_only))
+        for param in global_values:
+            key = '{0}_{1}'.format(self.role, param)
+            if key not in self.opts:
+                log.critical(
+                    'Key \'%s\' not present in global configuration. This is '
+                    'a bug, please report it.', key
+                )
+                failhard(self.role)
+            per_remote_defaults[param] = enforce_types(key, self.opts[key])
+
+        self.remotes = []
+        for remote in remotes:
+            repo_obj = self.git_providers[self.provider](
+                self.opts,
+                remote,
+                per_remote_defaults,
+                per_remote_only,
+                override_params,
+                self.cache_root,
+                self.role
+            )
+            if hasattr(repo_obj, 'repo'):
+                # Sanity check and assign the credential parameter
+                repo_obj.verify_auth()
+                repo_obj.setup_callbacks()
+                if self.opts['__role'] == 'minion' and repo_obj.new:
+                    # Perform initial fetch on masterless minion
+                    repo_obj.fetch()
+
+                # Reverse map to be used when running envs() to detect the
+                # available envs.
+                repo_obj.saltenv_revmap = {}
+
+                for saltenv, saltenv_conf in six.iteritems(repo_obj.saltenv):
+                    if 'ref' in saltenv_conf:
+                        ref = saltenv_conf['ref']
+                        repo_obj.saltenv_revmap.setdefault(
+                            ref, []).append(saltenv)
+
+                        if saltenv == 'base':
+                            # Remove redundant 'ref' config for base saltenv
+                            repo_obj.saltenv[saltenv].pop('ref')
+                            if ref != repo_obj.base:
+                                log.warning(
+                                    'The \'base\' environment has been '
+                                    'defined in the \'saltenv\' param for %s '
+                                    'remote %s and will override the '
+                                    'branch/tag specified by %s_base (or a '
+                                    'per-remote \'base\' parameter).',
+                                    self.role, repo_obj.id, self.role
+                                )
+                                # Rewrite 'base' config param
+                                repo_obj.base = ref
+
+                # Build list of all envs defined by ref mappings in the
+                # per-remote 'saltenv' param. We won't add any matching envs
+                # from the global saltenv map to the revmap.
+                all_envs = []
+                for env_names in six.itervalues(repo_obj.saltenv_revmap):
+                    all_envs.extend(env_names)
+
+                # Add the global saltenv map to the reverse map, skipping envs
+                # explicitly mapped in the per-remote 'saltenv' param.
+                for key, conf in six.iteritems(repo_obj.global_saltenv):
+                    if key not in all_envs and 'ref' in conf:
+                        repo_obj.saltenv_revmap.setdefault(
+                            conf['ref'], []).append(key)
+
+                self.remotes.append(repo_obj)
+
+        # Don't allow collisions in cachedir naming
+        cachedir_map = {}
+        for repo in self.remotes:
+            cachedir_map.setdefault(repo.cachedir, []).append(repo.id)
+
+        collisions = [x for x in cachedir_map if len(cachedir_map[x]) > 1]
+        if collisions:
+            for dirname in collisions:
+                log.critical(
+                    'The following %s remotes have conflicting cachedirs: '
+                    '%s. Resolve this using a per-remote parameter called '
+                    '\'name\'.', self.role, ', '.join(cachedir_map[dirname])
+                )
+                failhard(self.role)
+
+        if any(x.new for x in self.remotes):
+            self.write_remote_map()
+
+    def clear_old_remotes(self):
+        '''
+        Remove cache directories for remotes no longer configured
+        '''
+        try:
+            cachedir_ls = os.listdir(self.cache_root)
+        except OSError:
+            cachedir_ls = []
+        # Remove actively-used remotes from list
+        for repo in self.remotes:
+            try:
+                cachedir_ls.remove(repo.cachedir_basename)
+            except ValueError:
+                pass
+        to_remove = []
+        for item in cachedir_ls:
+            if item in ('hash', 'refs'):
+                continue
+            path = salt.utils.path.join(self.cache_root, item)
+            if os.path.isdir(path):
+                to_remove.append(path)
+        failed = []
+        if to_remove:
+            for rdir in to_remove:
+                try:
+                    shutil.rmtree(rdir)
+                except OSError as exc:
+                    log.error(
+                        'Unable to remove old %s remote cachedir %s: %s',
+                        self.role, rdir, exc
+                    )
+                    failed.append(rdir)
+                else:
+                    log.debug('%s removed old cachedir %s', self.role, rdir)
+        for fdir in failed:
+            to_remove.remove(fdir)
+        ret = bool(to_remove)
+        if ret:
+            self.write_remote_map()
+        return ret
+
+    def clear_cache(self):
+        '''
+        Completely clear cache
+        '''
+        errors = []
+        for rdir in (self.cache_root, self.file_list_cachedir):
+            if os.path.exists(rdir):
+                try:
+                    shutil.rmtree(rdir)
+                except OSError as exc:
+                    errors.append(
+                        'Unable to delete {0}: {1}'.format(rdir, exc)
+                    )
+        return errors
+
+    def clear_lock(self, remote=None, lock_type='update'):
+        '''
+        Clear update.lk for all remotes
+        '''
+        cleared = []
+        errors = []
+        for repo in self.remotes:
+            if remote:
+                # Specific remote URL/pattern was passed, ensure that the URL
+                # matches or else skip this one
+                try:
+                    if not fnmatch.fnmatch(repo.url, remote):
+                        continue
+                except TypeError:
+                    # remote was non-string, try again
+                    if not fnmatch.fnmatch(repo.url, six.text_type(remote)):
+                        continue
+            success, failed = repo.clear_lock(lock_type=lock_type)
+            cleared.extend(success)
+            errors.extend(failed)
+        return cleared, errors
+
+    def fetch_remotes(self, remotes=None):
+        '''
+        Fetch all remotes and return a boolean to let the calling function know
+        whether or not any remotes were updated in the process of fetching
+        '''
+        if remotes is None:
+            remotes = []
+        elif not isinstance(remotes, list):
+            log.error(
+                'Invalid \'remotes\' argument (%s) for fetch_remotes. '
+                'Must be a list of strings', remotes
+            )
+            remotes = []
+
+        changed = False
+        for repo in self.remotes:
+            name = getattr(repo, 'name', None)
+            if not remotes or (repo.id, name) in remotes:
+                try:
+                    if repo.fetch():
+                        # We can't just use the return value from repo.fetch()
+                        # because the data could still have changed if old
+                        # remotes were cleared above. Additionally, we're
+                        # running this in a loop and later remotes without
+                        # changes would override this value and make it
+                        # incorrect.
+                        changed = True
+                except Exception as exc:
+                    log.error(
+                        'Exception caught while fetching %s remote \'%s\': %s',
+                        self.role, repo.id, exc,
+                        exc_info=True
+                    )
+        return changed
+
+    def lock(self, remote=None):
+        '''
+        Place an update.lk
+        '''
+        locked = []
+        errors = []
+        for repo in self.remotes:
+            if remote:
+                # Specific remote URL/pattern was passed, ensure that the URL
+                # matches or else skip this one
+                try:
+                    if not fnmatch.fnmatch(repo.url, remote):
+                        continue
+                except TypeError:
+                    # remote was non-string, try again
+                    if not fnmatch.fnmatch(repo.url, six.text_type(remote)):
+                        continue
+            success, failed = repo.lock()
+            locked.extend(success)
+            errors.extend(failed)
+        return locked, errors
+
+    def update(self, remotes=None):
+        '''
+        .. versionchanged:: 2018.3.0
+            The remotes argument was added. This being a list of remote URLs,
+            it will only update matching remotes. This actually matches on
+            repo.id
+
+        Execute a git fetch on all of the repos and perform maintenance on the
+        fileserver cache.
+        '''
+        # data for the fileserver event
+        data = {'changed': False,
+                'backend': 'gitfs'}
+
+        data['changed'] = self.clear_old_remotes()
+        if self.fetch_remotes(remotes=remotes):
+            data['changed'] = True
+
+        # A masterless minion will need a new env cache file even if no changes
+        # were fetched.
+        refresh_env_cache = self.opts['__role'] == 'minion'
+
+        if data['changed'] is True or not os.path.isfile(self.env_cache):
+            env_cachedir = os.path.dirname(self.env_cache)
+            if not os.path.exists(env_cachedir):
+                os.makedirs(env_cachedir)
+            refresh_env_cache = True
+
+        if refresh_env_cache:
+            new_envs = self.envs(ignore_cache=True)
+            serial = salt.payload.Serial(self.opts)
+            with salt.utils.files.fopen(self.env_cache, 'wb+') as fp_:
+                fp_.write(serial.dumps(new_envs))
+                log.trace('Wrote env cache data to %s', self.env_cache)
+
+        # if there is a change, fire an event
+        if self.opts.get('fileserver_events', False):
+            event = salt.utils.event.get_event(
+                    'master',
+                    self.opts['sock_dir'],
+                    self.opts['transport'],
+                    opts=self.opts,
+                    listen=False)
+            event.fire_event(
+                data,
+                tagify(['gitfs', 'update'], prefix='fileserver')
+            )
+        try:
+            salt.fileserver.reap_fileserver_cache_dir(
+                self.hash_cachedir,
+                self.find_file
+            )
+        except (OSError, IOError):
+            # Hash file won't exist if no files have yet been served up
+            pass
+
+    def update_intervals(self):
+        '''
+        Returns a dictionary mapping remote IDs to their intervals, designed to
+        be used for variable update intervals in salt.master.FileserverUpdate.
+
+        A remote's ID is defined here as a tuple of the GitPython/Pygit2
+        object's "id" and "name" attributes, with None being assumed as the
+        "name" value if the attribute is not present.
+        '''
+        return {(repo.id, getattr(repo, 'name', None)): repo.update_interval
+                for repo in self.remotes}
+
+    def verify_provider(self):
+        '''
+        Determine which provider to use
+        '''
+        if 'verified_{0}_provider'.format(self.role) in self.opts:
+            self.provider = self.opts['verified_{0}_provider'.format(self.role)]
+        else:
+            desired_provider = self.opts.get('{0}_provider'.format(self.role))
+            if not desired_provider:
+                if self.verify_pygit2(quiet=True):
+                    self.provider = 'pygit2'
+                elif self.verify_gitpython(quiet=True):
+                    self.provider = 'gitpython'
+            else:
+                # Ensure non-lowercase providers work
+                try:
+                    desired_provider = desired_provider.lower()
+                except AttributeError:
+                    # Should only happen if someone does something silly like
+                    # set the provider to a numeric value.
+                    desired_provider = six.text_type(desired_provider).lower()
+                if desired_provider not in self.git_providers:
+                    log.critical(
+                        'Invalid %s_provider \'%s\'. Valid choices are: %s',
+                        self.role,
+                        desired_provider,
+                        ', '.join(self.git_providers)
+                    )
+                    failhard(self.role)
+                elif desired_provider == 'pygit2' and self.verify_pygit2():
+                    self.provider = 'pygit2'
+                elif desired_provider == 'gitpython' and self.verify_gitpython():
+                    self.provider = 'gitpython'
+        if not hasattr(self, 'provider'):
+            log.critical(
+                'No suitable %s provider module is installed.', self.role
+            )
+            failhard(self.role)
+
+    def verify_gitpython(self, quiet=False):
+        '''
+        Check if GitPython is available and at a compatible version (>= 0.3.0)
+        '''
+        def _recommend():
+            if PYGIT2_VERSION and 'pygit2' in self.git_providers:
+                log.error(_RECOMMEND_PYGIT2, self.role, self.role)
+
+        if not GITPYTHON_VERSION:
+            if not quiet:
+                log.error(
+                    '%s is configured but could not be loaded, is GitPython '
+                    'installed?', self.role
+                )
+                _recommend()
+            return False
+        elif 'gitpython' not in self.git_providers:
+            return False
+
+        errors = []
+        if GITPYTHON_VERSION < GITPYTHON_MINVER:
+            errors.append(
+                '{0} is configured, but the GitPython version is earlier than '
+                '{1}. Version {2} detected.'.format(
+                    self.role,
+                    GITPYTHON_MINVER,
+                    GITPYTHON_VERSION
+                )
+            )
+        if not salt.utils.path.which('git'):
+            errors.append(
+                'The git command line utility is required when using the '
+                '\'gitpython\' {0}_provider.'.format(self.role)
+            )
+
+        if errors:
+            for error in errors:
+                log.error(error)
+            if not quiet:
+                _recommend()
+            return False
+
+        self.opts['verified_{0}_provider'.format(self.role)] = 'gitpython'
+        log.debug('gitpython %s_provider enabled', self.role)
+        return True
+
+    def verify_pygit2(self, quiet=False):
+        '''
+        Check if pygit2/libgit2 are available and at a compatible version.
+        Pygit2 must be at least 0.20.3 and libgit2 must be at least 0.20.0.
+        '''
+        def _recommend():
+            if GITPYTHON_VERSION and 'gitpython' in self.git_providers:
+                log.error(_RECOMMEND_GITPYTHON, self.role, self.role)
+
+        if not PYGIT2_VERSION:
+            if not quiet:
+                log.error(
+                    '%s is configured but could not be loaded, are pygit2 '
+                    'and libgit2 installed?', self.role
+                )
+                _recommend()
+            return False
+        elif 'pygit2' not in self.git_providers:
+            return False
+
+        errors = []
+        if PYGIT2_VERSION < PYGIT2_MINVER:
+            errors.append(
+                '{0} is configured, but the pygit2 version is earlier than '
+                '{1}. Version {2} detected.'.format(
+                    self.role,
+                    PYGIT2_MINVER,
+                    PYGIT2_VERSION
+                )
+            )
+        if LIBGIT2_VERSION < LIBGIT2_MINVER:
+            errors.append(
+                '{0} is configured, but the libgit2 version is earlier than '
+                '{1}. Version {2} detected.'.format(
+                    self.role,
+                    LIBGIT2_MINVER,
+                    LIBGIT2_VERSION
+                )
+            )
+        if not getattr(pygit2, 'GIT_FETCH_PRUNE', False) \
+                and not salt.utils.path.which('git'):
+            errors.append(
+                'The git command line utility is required when using the '
+                '\'pygit2\' {0}_provider.'.format(self.role)
+            )
+
+        if errors:
+            for error in errors:
+                log.error(error)
+            if not quiet:
+                _recommend()
+            return False
+
+        self.opts['verified_{0}_provider'.format(self.role)] = 'pygit2'
+        log.debug('pygit2 %s_provider enabled', self.role)
+        return True
+
+    def write_remote_map(self):
+        '''
+        Write the remote_map.txt
+        '''
+        remote_map = salt.utils.path.join(self.cache_root, 'remote_map.txt')
+        try:
+            with salt.utils.files.fopen(remote_map, 'w+') as fp_:
+                timestamp = \
+                    datetime.now().strftime('%d %b %Y %H:%M:%S.%f')
+                fp_.write(
+                    '# {0}_remote map as of {1}\n'.format(
+                        self.role,
+                        timestamp
+                    )
+                )
+                for repo in self.remotes:
+                    fp_.write(
+                        salt.utils.stringutils.to_str(
+                            '{0} = {1}\n'.format(
+                                repo.cachedir_basename,
+                                repo.id
+                            )
+                        )
+                    )
+        except OSError:
+            pass
+        else:
+            log.info('Wrote new %s remote map to %s', self.role, remote_map)
+
+    def do_checkout(self, repo):
+        '''
+        Common code for git_pillar/winrepo to handle locking and checking out
+        of a repo.
+        '''
+        time_start = time.time()
+        while time.time() - time_start <= 5:
+            try:
+                return repo.checkout()
+            except GitLockError as exc:
+                if exc.errno == errno.EEXIST:
+                    time.sleep(0.1)
+                    continue
+                else:
+                    log.error(
+                        'Error %d encountered while obtaining checkout '
+                        'lock for %s remote \'%s\': %s',
+                        exc.errno,
+                        repo.role,
+                        repo.id,
+                        exc,
+                        exc_info=True
+                    )
+                    break
+        else:
+            log.error(
+                'Timed out waiting for checkout lock to be released for '
+                '%s remote \'%s\'. If this error persists, run \'salt-run '
+                'cache.clear_git_lock %s type=checkout\' to clear it.',
+                self.role, repo.id, self.role
+            )
+        return None
+
+
+class GitFS(GitBase):
+    '''
+    Functionality specific to the git fileserver backend
+    '''
+    role = 'gitfs'
+    instance_map = weakref.WeakKeyDictionary()
+
+    def __new__(cls, opts, remotes=None, per_remote_overrides=(),
+                per_remote_only=PER_REMOTE_ONLY, git_providers=None,
+                cache_root=None, init_remotes=True):
+        '''
+        If we are not initializing remotes (such as in cases where we just want
+        to load the config so that we can run clear_cache), then just return a
+        new __init__'ed object. Otherwise, check the instance map and re-use an
+        instance if one exists for the current process. Weak references are
+        used to ensure that we garbage collect instances for threads which have
+        exited.
+        '''
+        # No need to get the ioloop reference if we're not initializing remotes
+        io_loop = tornado.ioloop.IOLoop.current() if init_remotes else None
+        if not init_remotes or io_loop not in cls.instance_map:
+            # We only evaluate the second condition in this if statement if
+            # we're initializing remotes, so we won't get here unless io_loop
+            # is something other than None.
+            obj = object.__new__(cls)
+            super(GitFS, obj).__init__(
+                opts,
+                remotes if remotes is not None else [],
+                per_remote_overrides=per_remote_overrides,
+                per_remote_only=per_remote_only,
+                git_providers=git_providers if git_providers is not None
+                    else GIT_PROVIDERS,
+                cache_root=cache_root,
+                init_remotes=init_remotes)
+            if not init_remotes:
+                log.debug('Created gitfs object with uninitialized remotes')
+            else:
+                log.debug('Created gitfs object for process %s', os.getpid())
+                # Add to the instance map so we can re-use later
+                cls.instance_map[io_loop] = obj
+            return obj
+        log.debug('Re-using gitfs object for process %s', os.getpid())
+        return cls.instance_map[io_loop]
+
+    def __init__(self, opts, remotes, per_remote_overrides=(),  # pylint: disable=super-init-not-called
+                 per_remote_only=PER_REMOTE_ONLY, git_providers=None,
+                 cache_root=None, init_remotes=True):
+        # Initialization happens above in __new__(), so don't do anything here
+        pass
+
+    def dir_list(self, load):
+        '''
+        Return a list of all directories on the master
+        '''
+        return self._file_lists(load, 'dirs')
+
+    def envs(self, ignore_cache=False):
+        '''
+        Return a list of refs that can be used as environments
+        '''
+        if not ignore_cache:
+            cache_match = salt.fileserver.check_env_cache(
+                self.opts,
+                self.env_cache
+            )
+            if cache_match is not None:
+                return cache_match
+        ret = set()
+        for repo in self.remotes:
+            repo_envs = repo.envs()
+            for env_list in six.itervalues(repo.saltenv_revmap):
+                repo_envs.update(env_list)
+            ret.update([x for x in repo_envs if repo.env_is_exposed(x)])
+        return sorted(ret)
+
+    def find_file(self, path, tgt_env='base', **kwargs):  # pylint: disable=W0613
+        '''
+        Find the first file to match the path and ref, read the file out of git
+        and send the path to the newly cached file
+        '''
+        fnd = {'path': '',
+               'rel': ''}
+        if os.path.isabs(path) or \
+                (not salt.utils.stringutils.is_hex(tgt_env) and tgt_env not in self.envs()):
+            return fnd
+
+        dest = salt.utils.path.join(self.cache_root, 'refs', tgt_env, path)
+        hashes_glob = salt.utils.path.join(self.hash_cachedir,
+                                           tgt_env,
+                                           '{0}.hash.*'.format(path))
+        blobshadest = salt.utils.path.join(self.hash_cachedir,
+                                           tgt_env,
+                                           '{0}.hash.blob_sha1'.format(path))
+        lk_fn = salt.utils.path.join(self.hash_cachedir,
+                                     tgt_env,
+                                     '{0}.lk'.format(path))
+        destdir = os.path.dirname(dest)
+        hashdir = os.path.dirname(blobshadest)
+        if not os.path.isdir(destdir):
+            try:
+                os.makedirs(destdir)
+            except OSError:
+                # Path exists and is a file, remove it and retry
+                os.remove(destdir)
+                os.makedirs(destdir)
+        if not os.path.isdir(hashdir):
+            try:
+                os.makedirs(hashdir)
+            except OSError:
+                # Path exists and is a file, remove it and retry
+                os.remove(hashdir)
+                os.makedirs(hashdir)
+
+        for repo in self.remotes:
+            if repo.mountpoint(tgt_env) \
+                    and not path.startswith(repo.mountpoint(tgt_env) + os.sep):
+                continue
+            repo_path = path[len(repo.mountpoint(tgt_env)):].lstrip(os.sep)
+            if repo.root(tgt_env):
+                repo_path = salt.utils.path.join(repo.root(tgt_env), repo_path)
+
+            blob, blob_hexsha, blob_mode = repo.find_file(repo_path, tgt_env)
+            if blob is None:
+                continue
+
+            def _add_file_stat(fnd, mode):
+                '''
+                Add a the mode to the return dict. In other fileserver backends
+                we stat the file to get its mode, and add the stat result
+                (passed through list() for better serialization) to the 'stat'
+                key in the return dict. However, since we aren't using the
+                stat result for anything but the mode at this time, we can
+                avoid unnecessary work by just manually creating the list and
+                not running an os.stat() on all files in the repo.
+                '''
+                if mode is not None:
+                    fnd['stat'] = [mode]
+                return fnd
+
+            salt.fileserver.wait_lock(lk_fn, dest)
+            try:
+                with salt.utils.files.fopen(blobshadest, 'r') as fp_:
+                    sha = salt.utils.stringutils.to_unicode(fp_.read())
+                    if sha == blob_hexsha:
+                        fnd['rel'] = path
+                        fnd['path'] = dest
+                        return _add_file_stat(fnd, blob_mode)
+            except IOError as exc:
+                if exc.errno != errno.ENOENT:
+                    raise exc
+
+            with salt.utils.files.fopen(lk_fn, 'w'):
+                pass
+
+            for filename in glob.glob(hashes_glob):
+                try:
+                    os.remove(filename)
+                except Exception:
+                    pass
+            # Write contents of file to their destination in the FS cache
+            repo.write_file(blob, dest)
+            with salt.utils.files.fopen(blobshadest, 'w+') as fp_:
+                fp_.write(blob_hexsha)
+            try:
+                os.remove(lk_fn)
+            except OSError:
+                pass
+            fnd['rel'] = path
+            fnd['path'] = dest
+            return _add_file_stat(fnd, blob_mode)
+
+        # No matching file was found in tgt_env. Return a dict with empty paths
+        # so the calling function knows the file could not be found.
+        return fnd
+
+    def serve_file(self, load, fnd):
+        '''
+        Return a chunk from a file based on the data received
+        '''
+        if 'env' in load:
+            # "env" is not supported; Use "saltenv".
+            load.pop('env')
+
+        ret = {'data': '',
+               'dest': ''}
+        required_load_keys = set(['path', 'loc', 'saltenv'])
+        if not all(x in load for x in required_load_keys):
+            log.debug(
+                'Not all of the required keys present in payload. Missing: %s',
+                ', '.join(required_load_keys.difference(load))
+            )
+            return ret
+        if not fnd['path']:
+            return ret
+        ret['dest'] = fnd['rel']
+        gzip = load.get('gzip', None)
+        fpath = os.path.normpath(fnd['path'])
+        with salt.utils.files.fopen(fpath, 'rb') as fp_:
+            fp_.seek(load['loc'])
+            data = fp_.read(self.opts['file_buffer_size'])
+            if data and six.PY3 and not salt.utils.files.is_binary(fpath):
+                data = data.decode(__salt_system_encoding__)
+            if gzip and data:
+                data = salt.utils.gzip_util.compress(data, gzip)
+                ret['gzip'] = gzip
+            ret['data'] = data
+        return ret
+
+    def file_hash(self, load, fnd):
+        '''
+        Return a file hash, the hash type is set in the master config file
+        '''
+        if 'env' in load:
+            # "env" is not supported; Use "saltenv".
+            load.pop('env')
+
+        if not all(x in load for x in ('path', 'saltenv')):
+            return '', None
+        ret = {'hash_type': self.opts['hash_type']}
+        relpath = fnd['rel']
+        path = fnd['path']
+        hashdest = salt.utils.path.join(self.hash_cachedir,
+                                        load['saltenv'],
+                                        '{0}.hash.{1}'.format(relpath,
+                                                              self.opts['hash_type']))
+        try:
+            with salt.utils.files.fopen(hashdest, 'rb') as fp_:
+                ret['hsum'] = fp_.read()
+            return ret
+        except IOError as exc:
+            if exc.errno != errno.ENOENT:
+                raise exc
+
+        try:
+            os.makedirs(os.path.dirname(hashdest))
+        except OSError as exc:
+            if exc.errno != errno.EEXIST:
+                raise exc
+
+        ret['hsum'] = salt.utils.hashutils.get_hash(path, self.opts['hash_type'])
+        with salt.utils.files.fopen(hashdest, 'w+') as fp_:
+            fp_.write(ret['hsum'])
+        return ret
+
+    def _file_lists(self, load, form):
+        '''
+        Return a dict containing the file lists for files and dirs
+        '''
+        if 'env' in load:
+            # "env" is not supported; Use "saltenv".
+            load.pop('env')
+
+        if not os.path.isdir(self.file_list_cachedir):
+            try:
+                os.makedirs(self.file_list_cachedir)
+            except os.error:
+                log.error('Unable to make cachedir %s', self.file_list_cachedir)
+                return []
+        list_cache = salt.utils.path.join(
+            self.file_list_cachedir,
+            '{0}.p'.format(load['saltenv'].replace(os.path.sep, '_|-'))
+        )
+        w_lock = salt.utils.path.join(
+            self.file_list_cachedir,
+            '.{0}.w'.format(load['saltenv'].replace(os.path.sep, '_|-'))
+        )
+        cache_match, refresh_cache, save_cache = \
+            salt.fileserver.check_file_list_cache(
+                self.opts, form, list_cache, w_lock
+            )
+        if cache_match is not None:
+            return cache_match
+        if refresh_cache:
+            ret = {'files': set(), 'symlinks': {}, 'dirs': set()}
+            if salt.utils.stringutils.is_hex(load['saltenv']) \
+                    or load['saltenv'] in self.envs():
+                for repo in self.remotes:
+                    repo_files, repo_symlinks = repo.file_list(load['saltenv'])
+                    ret['files'].update(repo_files)
+                    ret['symlinks'].update(repo_symlinks)
+                    ret['dirs'].update(repo.dir_list(load['saltenv']))
+            ret['files'] = sorted(ret['files'])
+            ret['dirs'] = sorted(ret['dirs'])
+
+            if save_cache:
+                salt.fileserver.write_file_list_cache(
+                    self.opts, ret, list_cache, w_lock
+                )
+            # NOTE: symlinks are organized in a dict instead of a list, however
+            # the 'symlinks' key will be defined above so it will never get to
+            # the default value in the call to ret.get() below.
+            return ret.get(form, [])
+        # Shouldn't get here, but if we do, this prevents a TypeError
+        return {} if form == 'symlinks' else []
+
+    def file_list(self, load):
+        '''
+        Return a list of all files on the file server in a specified
+        environment
+        '''
+        return self._file_lists(load, 'files')
+
+    def file_list_emptydirs(self, load):  # pylint: disable=W0613
+        '''
+        Return a list of all empty directories on the master
+        '''
+        # Cannot have empty dirs in git
+        return []
+
+    def symlink_list(self, load):
+        '''
+        Return a dict of all symlinks based on a given path in the repo
+        '''
+        if 'env' in load:
+            # "env" is not supported; Use "saltenv".
+            load.pop('env')
+
+        if not salt.utils.stringutils.is_hex(load['saltenv']) \
+                and load['saltenv'] not in self.envs():
+            return {}
+        if 'prefix' in load:
+            prefix = load['prefix'].strip('/')
+        else:
+            prefix = ''
+        symlinks = self._file_lists(load, 'symlinks')
+        return dict([(key, val)
+                     for key, val in six.iteritems(symlinks)
+                     if key.startswith(prefix)])
+
+
+class GitPillar(GitBase):
+    '''
+    Functionality specific to the git external pillar
+    '''
+    role = 'git_pillar'
+
+    def checkout(self):
+        '''
+        Checkout the targeted branches/tags from the git_pillar remotes
+        '''
+        self.pillar_dirs = OrderedDict()
+        self.pillar_linked_dirs = []
+        for repo in self.remotes:
+            cachedir = self.do_checkout(repo)
+            if cachedir is not None:
+                # Figure out which environment this remote should be assigned
+                if repo.branch == '__env__' and hasattr(repo, 'all_saltenvs'):
+                    env = self.opts.get('pillarenv') \
+                        or self.opts.get('saltenv') \
+                        or 'base'
+                elif repo.env:
+                    env = repo.env
+                else:
+                    if repo.branch == repo.base:
+                        env = 'base'
+                    else:
+                        tgt = repo.get_checkout_target()
+                        env = 'base' if tgt == repo.base else tgt
+                if repo._mountpoint:
+                    if self.link_mountpoint(repo):
+                        self.pillar_dirs[repo.linkdir] = env
+                        self.pillar_linked_dirs.append(repo.linkdir)
+                else:
+                    self.pillar_dirs[cachedir] = env
+
+    def link_mountpoint(self, repo):
+        '''
+        Ensure that the mountpoint is present in the correct location and
+        points at the correct path
+        '''
+        lcachelink = salt.utils.path.join(repo.linkdir, repo._mountpoint)
+        lcachedest = salt.utils.path.join(repo.cachedir, repo.root()).rstrip(os.sep)
+        wipe_linkdir = False
+        create_link = False
+        try:
+            with repo.gen_lock(lock_type='mountpoint', timeout=10):
+                walk_results = list(os.walk(repo.linkdir, followlinks=False))
+                if walk_results != repo.linkdir_walk:
+                    log.debug(
+                        'Results of walking %s differ from expected results',
+                        repo.linkdir
+                    )
+                    log.debug('Walk results: %s', walk_results)
+                    log.debug('Expected results: %s', repo.linkdir_walk)
+                    wipe_linkdir = True
+                else:
+                    if not all(not salt.utils.path.islink(x[0])
+                               and os.path.isdir(x[0])
+                               for x in walk_results[:-1]):
+                        log.debug(
+                            'Linkdir parents of %s are not all directories',
+                            lcachelink
+                        )
+                        wipe_linkdir = True
+                    elif not salt.utils.path.islink(lcachelink):
+                        wipe_linkdir = True
+                    else:
+                        try:
+                            ldest = salt.utils.path.readlink(lcachelink)
+                        except Exception:
+                            log.debug(
+                                'Failed to read destination of %s', lcachelink
+                            )
+                            wipe_linkdir = True
+                        else:
+                            if ldest != lcachedest:
+                                log.debug(
+                                    'Destination of %s (%s) does not match '
+                                    'the expected value (%s)',
+                                    lcachelink, ldest, lcachedest
+                                )
+                                # Since we know that the parent dirs of the
+                                # link are set up properly, all we need to do
+                                # is remove the symlink and let it be created
+                                # below.
+                                try:
+                                    if salt.utils.platform.is_windows() \
+                                            and not ldest.startswith('\\\\') \
+                                            and os.path.isdir(ldest):
+                                        # On Windows, symlinks to directories
+                                        # must be removed as if they were
+                                        # themselves directories.
+                                        shutil.rmtree(lcachelink)
+                                    else:
+                                        os.remove(lcachelink)
+                                except Exception as exc:
+                                    log.exception(
+                                        'Failed to remove existing git_pillar '
+                                        'mountpoint link %s: %s',
+                                        lcachelink, exc.__str__()
+                                    )
+                                wipe_linkdir = False
+                                create_link = True
+
+                if wipe_linkdir:
+                    # Wiping implies that we need to create the link
+                    create_link = True
+                    try:
+                        shutil.rmtree(repo.linkdir)
+                    except OSError:
+                        pass
+                    try:
+                        ldirname = os.path.dirname(lcachelink)
+                        os.makedirs(ldirname)
+                        log.debug('Successfully made linkdir parent %s', ldirname)
+                    except OSError as exc:
+                        log.error(
+                            'Failed to os.makedirs() linkdir parent %s: %s',
+                            ldirname, exc.__str__()
+                        )
+                        return False
+
+                if create_link:
+                    try:
+                        os.symlink(lcachedest, lcachelink)
+                        log.debug(
+                            'Successfully linked %s to cachedir %s',
+                            lcachelink, lcachedest
+                        )
+                        return True
+                    except OSError as exc:
+                        log.error(
+                            'Failed to create symlink to %s at path %s: %s',
+                            lcachedest, lcachelink, exc.__str__()
+                        )
+                        return False
+        except GitLockError:
+            log.error(
+                'Timed out setting mountpoint lock for %s remote \'%s\'. If '
+                'this error persists, it may be because an earlier %s '
+                'checkout was interrupted. The lock can be cleared by running '
+                '\'salt-run cache.clear_git_lock %s type=mountpoint\', or by '
+                'manually removing %s.',
+                self.role, repo.id, self.role, self.role,
+                repo._get_lock_file(lock_type='mountpoint')
+            )
+            return False
+        return True
+
+
+class WinRepo(GitBase):
+    '''
+    Functionality specific to the winrepo runner
+    '''
+    role = 'winrepo'
+    # Need to define this in case we try to reference it before checking
+    # out the repos.
+    winrepo_dirs = {}
+
+    def checkout(self):
+        '''
+        Checkout the targeted branches/tags from the winrepo remotes
+        '''
+        self.winrepo_dirs = {}
+        for repo in self.remotes:
+            cachedir = self.do_checkout(repo)
+            if cachedir is not None:
+                self.winrepo_dirs[repo.id] = cachedir
diff -Naur a/salt/utils/http.py c/salt/utils/http.py
--- a/salt/utils/http.py	2019-07-02 10:15:07.023874717 -0600
+++ c/salt/utils/http.py	2019-07-02 10:58:03.179938595 -0600
@@ -66,15 +66,24 @@
 # pylint: enable=import-error,no-name-in-module
 
 # Don't need a try/except block, since Salt depends on tornado
-import tornado.httputil
-import tornado.simple_httpclient
-from tornado.httpclient import HTTPClient
-
 try:
-    import tornado.curl_httpclient
-    HAS_CURL_HTTPCLIENT = True
+    from tornado4.httputil import url_concat
+    from tornado4.simple_httpclient import SimpleAsyncHTTPClient
+    from tornado4.httpclient import HTTPClient, HTTPError
+    try:
+        from tornado4.curl_httpclient import CurlAsyncHTTPClient
+        HAS_CURL_HTTPCLIENT = True
+    except ImportError:
+        HAS_CURL_HTTPCLIENT = False
 except ImportError:
-    HAS_CURL_HTTPCLIENT = False
+    from tornado.httputil import url_concat
+    from tornado.simple_httpclient import SimpleAsyncHTTPClient
+    from tornado.httpclient import HTTPClient, HTTPError
+    try:
+        from tornado.curl_httpclient import CurlAsyncHTTPClient
+        HAS_CURL_HTTPCLIENT = True
+    except ImportError:
+        HAS_CURL_HTTPCLIENT = False
 
 try:
     import requests
@@ -212,7 +221,7 @@
 
     # Some libraries don't support separation of url and GET parameters
     # Don't need a try/except block, since Salt depends on tornado
-    url_full = tornado.httputil.url_concat(url, params) if params else url
+    url_full = url_concat(url, params) if params else url
 
     if ca_bundle is None:
         ca_bundle = get_ca_bundle(opts)
@@ -525,12 +534,12 @@
                 log.error(ret['error'])
                 return ret
 
-            tornado.httpclient.AsyncHTTPClient.configure('tornado.curl_httpclient.CurlAsyncHTTPClient')
+            AsyncHTTPClient.configure('CurlAsyncHTTPClient')
             client_argspec = salt.utils.args.get_function_argspec(
-                    tornado.curl_httpclient.CurlAsyncHTTPClient.initialize)
+                    CurlAsyncHTTPClient.initialize)
         else:
             client_argspec = salt.utils.args.get_function_argspec(
-                    tornado.simple_httpclient.SimpleAsyncHTTPClient.initialize)
+                    SimpleAsyncHTTPClient.initialize)
 
         supports_max_body_size = 'max_body_size' in client_argspec.args
 
@@ -564,7 +573,7 @@
                 if supports_max_body_size \
                 else HTTPClient()
             result = download_client.fetch(url_full, **req_kwargs)
-        except tornado.httpclient.HTTPError as exc:
+        except HTTPError as exc:
             ret['status'] = exc.code
             ret['error'] = six.text_type(exc)
             return ret
diff -Naur a/salt/utils/process.py c/salt/utils/process.py
--- a/salt/utils/process.py	2019-07-02 10:15:07.027874717 -0600
+++ c/salt/utils/process.py	2019-07-02 10:58:03.179938595 -0600
@@ -33,7 +33,10 @@
 # Import 3rd-party libs
 from salt.ext import six
 from salt.ext.six.moves import queue, range  # pylint: disable=import-error,redefined-builtin
-from tornado import gen
+try:
+    from tornado4 import gen
+except ImportError:
+    from tornado import gen
 
 log = logging.getLogger(__name__)
 
diff -Naur a/salt/utils/thin.py c/salt/utils/thin.py
--- a/salt/utils/thin.py	2019-07-02 10:15:07.027874717 -0600
+++ c/salt/utils/thin.py	2019-07-02 10:58:03.179938595 -0600
@@ -21,7 +21,10 @@
 import yaml
 import msgpack
 import salt.ext.six as _six
-import tornado
+try:
+    import tornado4 as tornado
+except ImportError:
+    import tornado
 
 try:
     import zlib
diff -Naur a/salt/utils/zeromq.py c/salt/utils/zeromq.py
--- a/salt/utils/zeromq.py	2019-07-02 10:15:07.027874717 -0600
+++ c/salt/utils/zeromq.py	2019-07-02 10:58:03.179938595 -0600
@@ -6,7 +6,13 @@
 from __future__ import absolute_import, print_function, unicode_literals
 
 import logging
-import tornado.ioloop
+
+try:
+    from tornado4.ioloop import IOLoop
+    from tornado4 import version_info as tornado_version_info
+except:
+    from tornado.ioloop import IOLoop
+    from tornado import version_info as tornado_version_info
 from salt.exceptions import SaltSystemExit
 from salt._compat import ipaddress
 
@@ -27,7 +33,7 @@
         ZMQ_VERSION_INFO = tuple([int(v_el) for v_el in zmq.__version__.split('.')])
         LIBZMQ_VERSION_INFO = tuple([int(v_el) for v_el in zmq.zmq_version().split('.')])
         if ZMQ_VERSION_INFO[0] > 16:  # 17.0.x+ deprecates zmq's ioloops
-            ZMQDefaultLoop = tornado.ioloop.IOLoop
+            ZMQDefaultLoop = IOLoop
 except Exception:
     log.exception('Error while getting LibZMQ/PyZMQ library version')
 
@@ -37,12 +43,12 @@
         # Support for ZeroMQ 13.x
         if not hasattr(zmq.eventloop.ioloop, 'ZMQIOLoop'):
             zmq.eventloop.ioloop.ZMQIOLoop = zmq.eventloop.ioloop.IOLoop
-        if tornado.version_info < (5,):
+        if tornado_version_info < (5,):
             ZMQDefaultLoop = zmq.eventloop.ioloop.ZMQIOLoop
     except ImportError:
         ZMQDefaultLoop = None
     if ZMQDefaultLoop is None:
-        ZMQDefaultLoop = tornado.ioloop.IOLoop
+        ZMQDefaultLoop = IOLoop
 
 
 def install_zmq():
@@ -56,7 +62,7 @@
     # instead of checking the first element of ZMQ_VERSION_INFO will prevent an
     # IndexError when this function is invoked during the docs build.
     if zmq and ZMQ_VERSION_INFO < (17,):
-        if tornado.version_info < (5,):
+        if tornado_version_info < (5,):
             zmq.eventloop.ioloop.install()
 
 
diff -Naur a/salt/utils/zeromq.py.orig c/salt/utils/zeromq.py.orig
--- a/salt/utils/zeromq.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/utils/zeromq.py.orig	2019-07-02 10:57:25.387937657 -0600
@@ -0,0 +1,87 @@
+# -*- coding: utf-8 -*-
+'''
+ZMQ-specific functions
+'''
+# Import Python libs
+from __future__ import absolute_import, print_function, unicode_literals
+
+import logging
+import tornado.ioloop
+from salt.exceptions import SaltSystemExit
+from salt._compat import ipaddress
+
+log = logging.getLogger(__name__)
+
+try:
+    import zmq
+except ImportError:
+    zmq = None
+    log.debug('ZMQ module is not found')
+
+ZMQDefaultLoop = None
+ZMQ_VERSION_INFO = (-1, -1, -1)
+LIBZMQ_VERSION_INFO = (-1, -1, -1)
+
+try:
+    if zmq:
+        ZMQ_VERSION_INFO = tuple([int(v_el) for v_el in zmq.__version__.split('.')])
+        LIBZMQ_VERSION_INFO = tuple([int(v_el) for v_el in zmq.zmq_version().split('.')])
+        if ZMQ_VERSION_INFO[0] > 16:  # 17.0.x+ deprecates zmq's ioloops
+            ZMQDefaultLoop = tornado.ioloop.IOLoop
+except Exception:
+    log.exception('Error while getting LibZMQ/PyZMQ library version')
+
+if ZMQDefaultLoop is None:
+    try:
+        import zmq.eventloop.ioloop
+        # Support for ZeroMQ 13.x
+        if not hasattr(zmq.eventloop.ioloop, 'ZMQIOLoop'):
+            zmq.eventloop.ioloop.ZMQIOLoop = zmq.eventloop.ioloop.IOLoop
+        if tornado.version_info < (5,):
+            ZMQDefaultLoop = zmq.eventloop.ioloop.ZMQIOLoop
+    except ImportError:
+        ZMQDefaultLoop = None
+    if ZMQDefaultLoop is None:
+        ZMQDefaultLoop = tornado.ioloop.IOLoop
+
+
+def install_zmq():
+    '''
+    While pyzmq 17 no longer needs any special integration for tornado,
+    older version still need one.
+    :return:
+    '''
+    # The zmq module is mocked in Sphinx, so when we build the docs
+    # ZMQ_VERSION_INFO ends up being an empty tuple. Using a tuple comparison
+    # instead of checking the first element of ZMQ_VERSION_INFO will prevent an
+    # IndexError when this function is invoked during the docs build.
+    if zmq and ZMQ_VERSION_INFO < (17,):
+        if tornado.version_info < (5,):
+            zmq.eventloop.ioloop.install()
+
+
+def check_ipc_path_max_len(uri):
+    # The socket path is limited to 107 characters on Solaris and
+    # Linux, and 103 characters on BSD-based systems.
+    if zmq is None:
+        return
+    ipc_path_max_len = getattr(zmq, 'IPC_PATH_MAX_LEN', 103)
+    if ipc_path_max_len and len(uri) > ipc_path_max_len:
+        raise SaltSystemExit(
+            'The socket path is longer than allowed by OS. '
+            '\'{0}\' is longer than {1} characters. '
+            'Either try to reduce the length of this setting\'s '
+            'path or switch to TCP; in the configuration file, '
+            'set "ipc_mode: tcp".'.format(
+                uri, ipc_path_max_len
+            )
+        )
+
+
+def ip_bracket(addr):
+    '''
+    Convert IP address representation to ZMQ (URL) format. ZMQ expects
+    brackets around IPv6 literals, since they are used in URLs.
+    '''
+    addr = ipaddress.ip_address(addr)
+    return ('[{}]' if addr.version == 6 else '{}').format(addr)
diff -Naur a/salt/version.py c/salt/version.py
--- a/salt/version.py	2019-07-02 10:15:07.059874718 -0600
+++ c/salt/version.py	2019-07-02 10:58:03.179938595 -0600
@@ -590,7 +590,7 @@
         ('RAET', 'raet', '__version__'),
         ('ZMQ', 'zmq', 'zmq_version'),
         ('Mako', 'mako', '__version__'),
-        ('Tornado', 'tornado', 'version'),
+        ('Tornado', 'tornado4', 'version'),
         ('timelib', 'timelib', 'version'),
         ('dateutil', 'dateutil', '__version__'),
         ('pygit2', 'pygit2', '__version__'),
@@ -616,7 +616,13 @@
             yield name, attr
             continue
         try:
-            imp = __import__(imp)
+            try:
+                imp = __import__(imp)
+            except ImportError:
+                if imp == 'tornado4':
+                    imp = __import__('tornado')
+                else:
+                    raise
             version = getattr(imp, attr)
             if callable(version):
                 version = version()
diff -Naur a/salt/version.py.orig c/salt/version.py.orig
--- a/salt/version.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/salt/version.py.orig	2019-07-02 10:57:25.423937658 -0600
@@ -0,0 +1,790 @@
+# -*- coding: utf-8 -*-
+'''
+Set up the version of Salt
+'''
+
+# Import python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import re
+import sys
+import platform
+import warnings
+
+# linux_distribution deprecated in py3.7
+try:
+    from platform import linux_distribution as _deprecated_linux_distribution
+
+    def linux_distribution(**kwargs):
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            return _deprecated_linux_distribution(**kwargs)
+except ImportError:
+    from distro import linux_distribution
+
+# pylint: disable=invalid-name,redefined-builtin
+# Import 3rd-party libs
+from salt.ext import six
+from salt.ext.six.moves import map
+
+# Don't rely on external packages in this module since it's used at install time
+if sys.version_info[0] == 3:
+    MAX_SIZE = sys.maxsize
+    string_types = (str,)
+else:
+    MAX_SIZE = sys.maxint
+    string_types = (six.string_types,)
+# pylint: enable=invalid-name,redefined-builtin
+
+# ----- ATTENTION --------------------------------------------------------------------------------------------------->
+#
+# ALL major version bumps, new release codenames, MUST be defined in the SaltStackVersion.NAMES dictionary, i.e.:
+#
+#    class SaltStackVersion(object):
+#
+#        NAMES = {
+#            'Hydrogen': (2014, 1),   # <- This is the tuple to bump versions
+#            ( ... )
+#        }
+#
+#
+# ONLY UPDATE CODENAMES AFTER BRANCHING
+#
+# As an example, The Helium codename must only be properly defined with "(2014, 7)" after Hydrogen, "(2014, 1)", has
+# been branched out into it's own branch.
+#
+# ALL OTHER VERSION INFORMATION IS EXTRACTED FROM THE GIT TAGS
+#
+# <---- ATTENTION ----------------------------------------------------------------------------------------------------
+
+
+class SaltStackVersion(object):
+    '''
+    Handle SaltStack versions class.
+
+    Knows how to parse ``git describe`` output, knows about release candidates
+    and also supports version comparison.
+    '''
+
+    __slots__ = ('name', 'major', 'minor', 'bugfix', 'mbugfix', 'pre_type', 'pre_num', 'noc', 'sha')
+
+    git_describe_regex = re.compile(
+        r'(?:[^\d]+)?(?P<major>[\d]{1,4})'
+        r'\.(?P<minor>[\d]{1,2})'
+        r'(?:\.(?P<bugfix>[\d]{0,2}))?'
+        r'(?:\.(?P<mbugfix>[\d]{0,2}))?'
+        r'(?:(?P<pre_type>rc|a|b|alpha|beta|nb)(?P<pre_num>[\d]{1}))?'
+        r'(?:(?:.*)-(?P<noc>(?:[\d]+|n/a))-(?P<sha>[a-z0-9]{8}))?'
+    )
+    git_sha_regex = r'(?P<sha>[a-z0-9]{7})'
+    if six.PY2:
+        git_sha_regex = git_sha_regex.decode(__salt_system_encoding__)
+    git_sha_regex = re.compile(git_sha_regex)
+
+    # Salt versions after 0.17.0 will be numbered like:
+    #   <4-digit-year>.<month>.<bugfix>
+    #
+    # Since the actual version numbers will only be know on release dates, the
+    # periodic table element names will be what's going to be used to name
+    # versions and to be able to mention them.
+
+    NAMES = {
+        # Let's keep at least 3 version names uncommented counting from the
+        # latest release so we can map deprecation warnings to versions.
+
+
+        # pylint: disable=E8203
+        # ----- Please refrain from fixing PEP-8 E203 and E265 ----->
+        # The idea is to keep this readable.
+        # -----------------------------------------------------------
+        'Hydrogen'      : (2014, 1),
+        'Helium'        : (2014, 7),
+        'Lithium'       : (2015, 5),
+        'Beryllium'     : (2015, 8),
+        'Boron'         : (2016, 3),
+        'Carbon'        : (2016, 11),
+        'Nitrogen'      : (2017, 7),
+        'Oxygen'        : (2018, 3),
+        'Fluorine'      : (2019, 2),
+        'Neon'          : (MAX_SIZE - 99, 0),
+        'Sodium'        : (MAX_SIZE - 98, 0),
+        'Magnesium'     : (MAX_SIZE - 97, 0),
+        # pylint: disable=E8265
+        #'Aluminium'    : (MAX_SIZE - 96, 0),
+        #'Silicon'      : (MAX_SIZE - 95, 0),
+        #'Phosphorus'   : (MAX_SIZE - 94, 0),
+        #'Sulfur'       : (MAX_SIZE - 93, 0),
+        #'Chlorine'     : (MAX_SIZE - 92, 0),
+        #'Argon'        : (MAX_SIZE - 91, 0),
+        #'Potassium'    : (MAX_SIZE - 90, 0),
+        #'Calcium'      : (MAX_SIZE - 89, 0),
+        #'Scandium'     : (MAX_SIZE - 88, 0),
+        #'Titanium'     : (MAX_SIZE - 87, 0),
+        #'Vanadium'     : (MAX_SIZE - 86, 0),
+        #'Chromium'     : (MAX_SIZE - 85, 0),
+        #'Manganese'    : (MAX_SIZE - 84, 0),
+        #'Iron'         : (MAX_SIZE - 83, 0),
+        #'Cobalt'       : (MAX_SIZE - 82, 0),
+        #'Nickel'       : (MAX_SIZE - 81, 0),
+        #'Copper'       : (MAX_SIZE - 80, 0),
+        #'Zinc'         : (MAX_SIZE - 79, 0),
+        #'Gallium'      : (MAX_SIZE - 78, 0),
+        #'Germanium'    : (MAX_SIZE - 77, 0),
+        #'Arsenic'      : (MAX_SIZE - 76, 0),
+        #'Selenium'     : (MAX_SIZE - 75, 0),
+        #'Bromine'      : (MAX_SIZE - 74, 0),
+        #'Krypton'      : (MAX_SIZE - 73, 0),
+        #'Rubidium'     : (MAX_SIZE - 72, 0),
+        #'Strontium'    : (MAX_SIZE - 71, 0),
+        #'Yttrium'      : (MAX_SIZE - 70, 0),
+        #'Zirconium'    : (MAX_SIZE - 69, 0),
+        #'Niobium'      : (MAX_SIZE - 68, 0),
+        #'Molybdenum'   : (MAX_SIZE - 67, 0),
+        #'Technetium'   : (MAX_SIZE - 66, 0),
+        #'Ruthenium'    : (MAX_SIZE - 65, 0),
+        #'Rhodium'      : (MAX_SIZE - 64, 0),
+        #'Palladium'    : (MAX_SIZE - 63, 0),
+        #'Silver'       : (MAX_SIZE - 62, 0),
+        #'Cadmium'      : (MAX_SIZE - 61, 0),
+        #'Indium'       : (MAX_SIZE - 60, 0),
+        #'Tin'          : (MAX_SIZE - 59, 0),
+        #'Antimony'     : (MAX_SIZE - 58, 0),
+        #'Tellurium'    : (MAX_SIZE - 57, 0),
+        #'Iodine'       : (MAX_SIZE - 56, 0),
+        #'Xenon'        : (MAX_SIZE - 55, 0),
+        #'Caesium'      : (MAX_SIZE - 54, 0),
+        #'Barium'       : (MAX_SIZE - 53, 0),
+        #'Lanthanum'    : (MAX_SIZE - 52, 0),
+        #'Cerium'       : (MAX_SIZE - 51, 0),
+        #'Praseodymium' : (MAX_SIZE - 50, 0),
+        #'Neodymium'    : (MAX_SIZE - 49, 0),
+        #'Promethium'   : (MAX_SIZE - 48, 0),
+        #'Samarium'     : (MAX_SIZE - 47, 0),
+        #'Europium'     : (MAX_SIZE - 46, 0),
+        #'Gadolinium'   : (MAX_SIZE - 45, 0),
+        #'Terbium'      : (MAX_SIZE - 44, 0),
+        #'Dysprosium'   : (MAX_SIZE - 43, 0),
+        #'Holmium'      : (MAX_SIZE - 42, 0),
+        #'Erbium'       : (MAX_SIZE - 41, 0),
+        #'Thulium'      : (MAX_SIZE - 40, 0),
+        #'Ytterbium'    : (MAX_SIZE - 39, 0),
+        #'Lutetium'     : (MAX_SIZE - 38, 0),
+        #'Hafnium'      : (MAX_SIZE - 37, 0),
+        #'Tantalum'     : (MAX_SIZE - 36, 0),
+        #'Tungsten'     : (MAX_SIZE - 35, 0),
+        #'Rhenium'      : (MAX_SIZE - 34, 0),
+        #'Osmium'       : (MAX_SIZE - 33, 0),
+        #'Iridium'      : (MAX_SIZE - 32, 0),
+        #'Platinum'     : (MAX_SIZE - 31, 0),
+        #'Gold'         : (MAX_SIZE - 30, 0),
+        #'Mercury'      : (MAX_SIZE - 29, 0),
+        #'Thallium'     : (MAX_SIZE - 28, 0),
+        #'Lead'         : (MAX_SIZE - 27, 0),
+        #'Bismuth'      : (MAX_SIZE - 26, 0),
+        #'Polonium'     : (MAX_SIZE - 25, 0),
+        #'Astatine'     : (MAX_SIZE - 24, 0),
+        #'Radon'        : (MAX_SIZE - 23, 0),
+        #'Francium'     : (MAX_SIZE - 22, 0),
+        #'Radium'       : (MAX_SIZE - 21, 0),
+        #'Actinium'     : (MAX_SIZE - 20, 0),
+        #'Thorium'      : (MAX_SIZE - 19, 0),
+        #'Protactinium' : (MAX_SIZE - 18, 0),
+        #'Uranium'      : (MAX_SIZE - 17, 0),
+        #'Neptunium'    : (MAX_SIZE - 16, 0),
+        #'Plutonium'    : (MAX_SIZE - 15, 0),
+        #'Americium'    : (MAX_SIZE - 14, 0),
+        #'Curium'       : (MAX_SIZE - 13, 0),
+        #'Berkelium'    : (MAX_SIZE - 12, 0),
+        #'Californium'  : (MAX_SIZE - 11, 0),
+        #'Einsteinium'  : (MAX_SIZE - 10, 0),
+        #'Fermium'      : (MAX_SIZE - 9, 0),
+        #'Mendelevium'  : (MAX_SIZE - 8, 0),
+        #'Nobelium'     : (MAX_SIZE - 7, 0),
+        #'Lawrencium'   : (MAX_SIZE - 6, 0),
+        #'Rutherfordium': (MAX_SIZE - 5, 0),
+        #'Dubnium'      : (MAX_SIZE - 4, 0),
+        #'Seaborgium'   : (MAX_SIZE - 3, 0),
+        #'Bohrium'      : (MAX_SIZE - 2, 0),
+        #'Hassium'      : (MAX_SIZE - 1, 0),
+        #'Meitnerium'   : (MAX_SIZE - 0, 0),
+        # <---- Please refrain from fixing PEP-8 E203 and E265 ------
+        # pylint: enable=E8203,E8265
+    }
+
+    LNAMES = dict((k.lower(), v) for (k, v) in iter(NAMES.items()))
+    VNAMES = dict((v, k) for (k, v) in iter(NAMES.items()))
+    RMATCH = dict((v[:2], k) for (k, v) in iter(NAMES.items()))
+
+    def __init__(self,              # pylint: disable=C0103
+                 major,
+                 minor,
+                 bugfix=0,
+                 mbugfix=0,
+                 pre_type=None,
+                 pre_num=None,
+                 noc=0,
+                 sha=None):
+
+        if isinstance(major, string_types):
+            major = int(major)
+
+        if isinstance(minor, string_types):
+            minor = int(minor)
+
+        if bugfix is None:
+            bugfix = 0
+        elif isinstance(bugfix, string_types):
+            bugfix = int(bugfix)
+
+        if mbugfix is None:
+            mbugfix = 0
+        elif isinstance(mbugfix, string_types):
+            mbugfix = int(mbugfix)
+
+        if pre_type is None:
+            pre_type = ''
+        if pre_num is None:
+            pre_num = 0
+        elif isinstance(pre_num, string_types):
+            pre_num = int(pre_num)
+
+        if noc is None:
+            noc = 0
+        elif isinstance(noc, string_types) and noc == 'n/a':
+            noc = -1
+        elif isinstance(noc, string_types):
+            noc = int(noc)
+
+        self.major = major
+        self.minor = minor
+        self.bugfix = bugfix
+        self.mbugfix = mbugfix
+        self.pre_type = pre_type
+        self.pre_num = pre_num
+        self.name = self.VNAMES.get((major, minor), None)
+        self.noc = noc
+        self.sha = sha
+
+    @classmethod
+    def parse(cls, version_string):
+        if version_string.lower() in cls.LNAMES:
+            return cls.from_name(version_string)
+        vstr = version_string.decode() if isinstance(version_string, bytes) else version_string
+        match = cls.git_describe_regex.match(vstr)
+        if not match:
+            raise ValueError(
+                'Unable to parse version string: \'{0}\''.format(version_string)
+            )
+        return cls(*match.groups())
+
+    @classmethod
+    def from_name(cls, name):
+        if name.lower() not in cls.LNAMES:
+            raise ValueError(
+                'Named version \'{0}\' is not known'.format(name)
+            )
+        return cls(*cls.LNAMES[name.lower()])
+
+    @classmethod
+    def from_last_named_version(cls):
+        return cls.from_name(
+            cls.VNAMES[
+                max([version_info for version_info in
+                     cls.VNAMES if
+                     version_info[0] < (MAX_SIZE - 200)])
+            ]
+        )
+
+    @classmethod
+    def next_release(cls):
+        return cls.from_name(
+            cls.VNAMES[
+                min([version_info for version_info in
+                     cls.VNAMES if
+                     version_info > cls.from_last_named_version().info])
+            ]
+        )
+
+    @property
+    def sse(self):
+        # Higher than 0.17, lower than first date based
+        return 0 < self.major < 2014
+
+    @property
+    def info(self):
+        return (
+            self.major,
+            self.minor,
+            self.bugfix,
+            self.mbugfix
+        )
+
+    @property
+    def pre_info(self):
+        return (
+            self.major,
+            self.minor,
+            self.bugfix,
+            self.mbugfix,
+            self.pre_type,
+            self.pre_num
+        )
+
+    @property
+    def noc_info(self):
+        return (
+            self.major,
+            self.minor,
+            self.bugfix,
+            self.mbugfix,
+            self.pre_type,
+            self.pre_num,
+            self.noc
+        )
+
+    @property
+    def full_info(self):
+        return (
+            self.major,
+            self.minor,
+            self.bugfix,
+            self.mbugfix,
+            self.pre_type,
+            self.pre_num,
+            self.noc,
+            self.sha
+        )
+
+    @property
+    def string(self):
+        version_string = '{0}.{1}.{2}'.format(
+            self.major,
+            self.minor,
+            self.bugfix
+        )
+        if self.mbugfix:
+            version_string += '.{0}'.format(self.mbugfix)
+        if self.pre_type:
+            version_string += '{0}{1}'.format(self.pre_type, self.pre_num)
+        if self.noc and self.sha:
+            noc = self.noc
+            if noc < 0:
+                noc = 'n/a'
+            version_string += '-{0}-{1}'.format(noc, self.sha)
+        return version_string
+
+    @property
+    def formatted_version(self):
+        if self.name and self.major > 10000:
+            version_string = self.name
+            if self.sse:
+                version_string += ' Enterprise'
+            version_string += ' (Unreleased)'
+            return version_string
+        version_string = self.string
+        if self.sse:
+            version_string += ' Enterprise'
+        if (self.major, self.minor) in self.RMATCH:
+            version_string += ' ({0})'.format(self.RMATCH[(self.major, self.minor)])
+        return version_string
+
+    def __str__(self):
+        return self.string
+
+    def __compare__(self, other, method):
+        if not isinstance(other, SaltStackVersion):
+            if isinstance(other, string_types):
+                other = SaltStackVersion.parse(other)
+            elif isinstance(other, (list, tuple)):
+                other = SaltStackVersion(*other)
+            else:
+                raise ValueError(
+                    'Cannot instantiate Version from type \'{0}\''.format(
+                        type(other)
+                    )
+                )
+
+        if (self.pre_type and other.pre_type) or (not self.pre_type and not other.pre_type):
+            # Both either have or don't have pre-release information, regular compare is ok
+            return method(self.noc_info, other.noc_info)
+
+        if self.pre_type and not other.pre_type:
+            # We have pre-release information, the other side doesn't
+            other_noc_info = list(other.noc_info)
+            other_noc_info[4] = 'zzzzz'
+            return method(self.noc_info, tuple(other_noc_info))
+
+        if not self.pre_type and other.pre_type:
+            # The other side has pre-release informatio, we don't
+            noc_info = list(self.noc_info)
+            noc_info[4] = 'zzzzz'
+            return method(tuple(noc_info), other.noc_info)
+
+    def __lt__(self, other):
+        return self.__compare__(other, lambda _self, _other: _self < _other)
+
+    def __le__(self, other):
+        return self.__compare__(other, lambda _self, _other: _self <= _other)
+
+    def __eq__(self, other):
+        return self.__compare__(other, lambda _self, _other: _self == _other)
+
+    def __ne__(self, other):
+        return self.__compare__(other, lambda _self, _other: _self != _other)
+
+    def __ge__(self, other):
+        return self.__compare__(other, lambda _self, _other: _self >= _other)
+
+    def __gt__(self, other):
+        return self.__compare__(other, lambda _self, _other: _self > _other)
+
+    def __repr__(self):
+        parts = []
+        if self.name:
+            parts.append('name=\'{0}\''.format(self.name))
+        parts.extend([
+            'major={0}'.format(self.major),
+            'minor={0}'.format(self.minor),
+            'bugfix={0}'.format(self.bugfix)
+        ])
+        if self.mbugfix:
+            parts.append('minor-bugfix={0}'.format(self.mbugfix))
+        if self.pre_type:
+            parts.append('{0}={1}'.format(self.pre_type, self.pre_num))
+        noc = self.noc
+        if noc == -1:
+            noc = 'n/a'
+        if noc and self.sha:
+            parts.extend([
+                'noc={0}'.format(noc),
+                'sha={0}'.format(self.sha)
+            ])
+        return '<{0} {1}>'.format(self.__class__.__name__, ' '.join(parts))
+
+
+# ----- Hardcoded Salt Codename Version Information ----------------------------------------------------------------->
+#
+#   There's no need to do anything here. The last released codename will be picked up
+# --------------------------------------------------------------------------------------------------------------------
+__saltstack_version__ = SaltStackVersion.from_last_named_version()
+# <---- Hardcoded Salt Version Information ---------------------------------------------------------------------------
+
+
+# ----- Dynamic/Runtime Salt Version Information -------------------------------------------------------------------->
+def __discover_version(saltstack_version):
+    # This might be a 'python setup.py develop' installation type. Let's
+    # discover the version information at runtime.
+    import os
+    import subprocess
+
+    if 'SETUP_DIRNAME' in globals():
+        # This is from the exec() call in Salt's setup.py
+        cwd = SETUP_DIRNAME  # pylint: disable=E0602
+        if not os.path.exists(os.path.join(cwd, '.git')):
+            # This is not a Salt git checkout!!! Don't even try to parse...
+            return saltstack_version
+    else:
+        cwd = os.path.abspath(os.path.dirname(__file__))
+        if not os.path.exists(os.path.join(os.path.dirname(cwd), '.git')):
+            # This is not a Salt git checkout!!! Don't even try to parse...
+            return saltstack_version
+
+    try:
+        kwargs = dict(
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            cwd=cwd
+        )
+
+        if not sys.platform.startswith('win'):
+            # Let's not import `salt.utils` for the above check
+            kwargs['close_fds'] = True
+
+        process = subprocess.Popen(
+            ['git', 'describe', '--tags', '--first-parent', '--match', 'v[0-9]*', '--always'], **kwargs)
+
+        out, err = process.communicate()
+
+        if process.returncode != 0:
+            # The git version running this might not support --first-parent
+            # Revert to old command
+            process = subprocess.Popen(
+                ['git', 'describe', '--tags', '--match', 'v[0-9]*', '--always'], **kwargs)
+            out, err = process.communicate()
+        if six.PY3:
+            out = out.decode()
+            err = err.decode()
+        out = out.strip()
+        err = err.strip()
+
+        if not out or err:
+            return saltstack_version
+
+        try:
+            return SaltStackVersion.parse(out)
+        except ValueError:
+            if not SaltStackVersion.git_sha_regex.match(out):
+                raise
+
+            # We only define the parsed SHA and set NOC as ??? (unknown)
+            saltstack_version.sha = out.strip()
+            saltstack_version.noc = -1
+
+    except OSError as os_err:
+        if os_err.errno != 2:
+            # If the errno is not 2(The system cannot find the file
+            # specified), raise the exception so it can be catch by the
+            # developers
+            raise
+    return saltstack_version
+
+
+def __get_version(saltstack_version):
+    '''
+    If we can get a version provided at installation time or from Git, use
+    that instead, otherwise we carry on.
+    '''
+    try:
+        # Try to import the version information provided at install time
+        from salt._version import __saltstack_version__  # pylint: disable=E0611,F0401
+        return __saltstack_version__
+    except ImportError:
+        return __discover_version(saltstack_version)
+
+
+# Get additional version information if available
+__saltstack_version__ = __get_version(__saltstack_version__)
+# This function has executed once, we're done with it. Delete it!
+del __get_version
+# <---- Dynamic/Runtime Salt Version Information ---------------------------------------------------------------------
+
+
+# ----- Common version related attributes - NO NEED TO CHANGE ------------------------------------------------------->
+__version_info__ = __saltstack_version__.info
+__version__ = __saltstack_version__.string
+# <---- Common version related attributes - NO NEED TO CHANGE --------------------------------------------------------
+
+
+def salt_information():
+    '''
+    Report version of salt.
+    '''
+    yield 'Salt', __version__
+
+
+def dependency_information(include_salt_cloud=False):
+    '''
+    Report versions of library dependencies.
+    '''
+    libs = [
+        ('Python', None, sys.version.rsplit('\n')[0].strip()),
+        ('Jinja2', 'jinja2', '__version__'),
+        ('M2Crypto', 'M2Crypto', 'version'),
+        ('msgpack-python', 'msgpack', 'version'),
+        ('msgpack-pure', 'msgpack_pure', 'version'),
+        ('pycrypto', 'Crypto', '__version__'),
+        ('pycryptodome', 'Cryptodome', 'version_info'),
+        ('libnacl', 'libnacl', '__version__'),
+        ('PyYAML', 'yaml', '__version__'),
+        ('ioflo', 'ioflo', '__version__'),
+        ('PyZMQ', 'zmq', '__version__'),
+        ('RAET', 'raet', '__version__'),
+        ('ZMQ', 'zmq', 'zmq_version'),
+        ('Mako', 'mako', '__version__'),
+        ('Tornado', 'tornado', 'version'),
+        ('timelib', 'timelib', 'version'),
+        ('dateutil', 'dateutil', '__version__'),
+        ('pygit2', 'pygit2', '__version__'),
+        ('libgit2', 'pygit2', 'LIBGIT2_VERSION'),
+        ('smmap', 'smmap', '__version__'),
+        ('cffi', 'cffi', '__version__'),
+        ('pycparser', 'pycparser', '__version__'),
+        ('gitdb', 'gitdb', '__version__'),
+        ('gitpython', 'git', '__version__'),
+        ('python-gnupg', 'gnupg', '__version__'),
+        ('mysql-python', 'MySQLdb', '__version__'),
+        ('cherrypy', 'cherrypy', '__version__'),
+        ('docker-py', 'docker', '__version__'),
+    ]
+
+    if include_salt_cloud:
+        libs.append(
+            ('Apache Libcloud', 'libcloud', '__version__'),
+        )
+
+    for name, imp, attr in libs:
+        if imp is None:
+            yield name, attr
+            continue
+        try:
+            imp = __import__(imp)
+            version = getattr(imp, attr)
+            if callable(version):
+                version = version()
+            if isinstance(version, (tuple, list)):
+                version = '.'.join(map(str, version))
+            yield name, version
+        except Exception:
+            yield name, None
+
+
+def system_information():
+    '''
+    Report system versions.
+    '''
+    def system_version():
+        '''
+        Return host system version.
+        '''
+        lin_ver = linux_distribution()
+        mac_ver = platform.mac_ver()
+        win_ver = platform.win32_ver()
+
+        if lin_ver[0]:
+            return ' '.join(lin_ver)
+        elif mac_ver[0]:
+            if isinstance(mac_ver[1], (tuple, list)) and ''.join(mac_ver[1]):
+                return ' '.join([mac_ver[0], '.'.join(mac_ver[1]), mac_ver[2]])
+            else:
+                return ' '.join([mac_ver[0], mac_ver[2]])
+        elif win_ver[0]:
+            return ' '.join(win_ver)
+        else:
+            return ''
+
+    if platform.win32_ver()[0]:
+        # Get the version and release info based on the Windows Operating
+        # System Product Name. As long as Microsoft maintains a similar format
+        # this should be future proof
+        import win32api  # pylint: disable=3rd-party-module-not-gated
+        import win32con  # pylint: disable=3rd-party-module-not-gated
+
+        # Get the product name from the registry
+        hkey = win32con.HKEY_LOCAL_MACHINE
+        key = 'SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion'
+        value_name = 'ProductName'
+        reg_handle = win32api.RegOpenKey(hkey, key)
+
+        # Returns a tuple of (product_name, value_type)
+        product_name, _ = win32api.RegQueryValueEx(reg_handle, value_name)
+
+        version = 'Unknown'
+        release = ''
+        if 'Server' in product_name:
+            for item in product_name.split(' '):
+                # If it's all digits, then it's version
+                if re.match(r'\d+', item):
+                    version = item
+                # If it starts with R and then numbers, it's the release
+                # ie: R2
+                if re.match(r'^R\d+$', item):
+                    release = item
+            release = '{0}Server{1}'.format(version, release)
+        else:
+            for item in product_name.split(' '):
+                # If it's a number, decimal number, Thin or Vista, then it's the
+                # version
+                if re.match(r'^(\d+(\.\d+)?)|Thin|Vista$', item):
+                    version = item
+            release = version
+
+        _, ver, sp, extra = platform.win32_ver()
+        version = ' '.join([release, ver, sp, extra])
+    else:
+        version = system_version()
+        release = platform.release()
+
+    system = [
+        ('system', platform.system()),
+        ('dist', ' '.join(linux_distribution(full_distribution_name=False))),
+        ('release', release),
+        ('machine', platform.machine()),
+        ('version', version),
+        ('locale', __salt_system_encoding__),
+    ]
+
+    for name, attr in system:
+        yield name, attr
+        continue
+
+
+def versions_information(include_salt_cloud=False):
+    '''
+    Report the versions of dependent software.
+    '''
+    salt_info = list(salt_information())
+    lib_info = list(dependency_information(include_salt_cloud))
+    sys_info = list(system_information())
+
+    return {'Salt Version': dict(salt_info),
+            'Dependency Versions': dict(lib_info),
+            'System Versions': dict(sys_info)}
+
+
+def versions_report(include_salt_cloud=False):
+    '''
+    Yield each version properly formatted for console output.
+    '''
+    ver_info = versions_information(include_salt_cloud)
+
+    lib_pad = max(len(name) for name in ver_info['Dependency Versions'])
+    sys_pad = max(len(name) for name in ver_info['System Versions'])
+    padding = max(lib_pad, sys_pad) + 1
+
+    fmt = '{0:>{pad}}: {1}'
+    info = []
+    for ver_type in ('Salt Version', 'Dependency Versions', 'System Versions'):
+        info.append('{0}:'.format(ver_type))
+        # List dependencies in alphabetical, case insensitive order
+        for name in sorted(ver_info[ver_type], key=lambda x: x.lower()):
+            ver = fmt.format(name,
+                             ver_info[ver_type][name] or 'Not Installed',
+                             pad=padding)
+            info.append(ver)
+        info.append(' ')
+
+    for line in info:
+        yield line
+
+
+def msi_conformant_version():
+    '''
+    An msi installer uninstalls/replaces a lower "internal version" of itself.
+    "internal version" is ivMAJOR.ivMINOR.ivBUILD with max values 255.255.65535.
+    Using the build nr allows continuous integration of the installer.
+    "Display version" is indipendent and free format: Year.Month.Bugfix as in Salt 2016.11.3.
+    Calculation of the internal version fields:
+        ivMAJOR = 'short year' (2 digits).
+        ivMINOR = 20*(month-1) + Bugfix
+            Combine Month and Bugfix to free ivBUILD for the build number
+            This limits Bugfix < 20.
+            The msi automatically replaces only 19 bugfixes of a month, one must uninstall manually.
+        ivBUILD = git commit count (noc)
+            noc for tags is 0, representing the final word, translates to the highest build number (65535).
+
+    Examples:
+      git checkout    Display version      Internal version    Remark
+      develop         2016.11.0-742        16.200.742          The develop branch has bugfix 0
+      2016.11         2016.11.2-78         16.202.78
+      2016.11         2016.11.9-88         16.209.88
+      2018.8          2018.3.2-1306        18.42.1306
+      v2016.11.0      2016.11.0            16.200.65535        Tags have noc 0
+      v2016.11.2      2016.11.2            16.202.65535
+
+    '''
+    short_year = int(six.text_type(__saltstack_version__.major)[2:])
+    month = __saltstack_version__.minor
+    bugfix = __saltstack_version__.bugfix
+    if bugfix > 19:
+        bugfix = 19
+    noc = __saltstack_version__.noc
+    if noc == 0:
+        noc = 65535
+    return '{}.{}.{}'.format(short_year, 20*(month-1)+bugfix, noc)
+
+
+if __name__ == '__main__':
+    if len(sys.argv) == 2 and sys.argv[1] == 'msi':
+        # Building the msi requires an msi-conformant version
+        print(msi_conformant_version())
+    else:
+        print(__version__)
diff -Naur a/tests/integration/files/engines/runtests_engine.py c/tests/integration/files/engines/runtests_engine.py
--- a/tests/integration/files/engines/runtests_engine.py	2019-07-02 10:15:07.015874717 -0600
+++ c/tests/integration/files/engines/runtests_engine.py	2019-07-02 10:58:03.179938595 -0600
@@ -24,10 +24,10 @@
 import salt.utils.asynchronous
 
 # Import 3rd-party libs
-from tornado import gen
-from tornado import ioloop
-from tornado import netutil
-from tornado import iostream
+try:
+    from tornado4 import gen, ioloop, netutil, iostream
+except:
+    from tornado import gen, ioloop, netutil, iostream
 
 log = logging.getLogger(__name__)
 
diff -Naur a/tests/integration/__init__.py c/tests/integration/__init__.py
--- a/tests/integration/__init__.py	2019-07-02 10:15:07.023874717 -0600
+++ c/tests/integration/__init__.py	2019-07-02 10:58:03.179938595 -0600
@@ -77,8 +77,12 @@
 except ImportError:
     import socketserver
 
-from tornado import gen
-from tornado import ioloop
+try:
+    from tornado4 import gen
+    from tornado4 import ioloop
+except:
+    from tornado import gen
+    from tornado import ioloop
 
 # Import salt tests support libs
 from tests.support.processes import SaltMaster, SaltMinion, SaltSyndic
diff -Naur a/tests/integration/modules/test_gem.py c/tests/integration/modules/test_gem.py
--- a/tests/integration/modules/test_gem.py	2019-07-02 10:15:07.019874717 -0600
+++ c/tests/integration/modules/test_gem.py	2019-07-02 10:58:03.179938595 -0600
@@ -15,7 +15,10 @@
 import salt.utils.path
 
 # Import 3rd-party libs
-from tornado.httpclient import HTTPClient
+try:
+    from tornado4.httpclient import HTTPClient
+except:
+    from tornado.httpclient import HTTPClient
 
 GEM = 'tidy'
 GEM_VER = '1.1.2'
diff -Naur a/tests/integration/modules/test_ssh.py c/tests/integration/modules/test_ssh.py
--- a/tests/integration/modules/test_ssh.py	2019-07-02 10:15:07.019874717 -0600
+++ c/tests/integration/modules/test_ssh.py	2019-07-02 10:58:03.179938595 -0600
@@ -18,7 +18,10 @@
 import salt.utils.platform
 
 # Import 3rd-party libs
-from tornado.httpclient import HTTPClient
+try:
+    from tornado4.httpclient import HTTPClient
+except:
+    from tornado.httpclient import HTTPClient
 
 SUBSALT_DIR = os.path.join(TMP, 'subsalt')
 AUTHORIZED_KEYS = os.path.join(SUBSALT_DIR, 'authorized_keys')
diff -Naur a/tests/support/helpers.py c/tests/support/helpers.py
--- a/tests/support/helpers.py	2019-07-02 10:15:07.003874717 -0600
+++ c/tests/support/helpers.py	2019-07-02 10:58:03.179938595 -0600
@@ -30,8 +30,14 @@
 import textwrap
 import threading
 import time
-import tornado.ioloop
-import tornado.web
+
+try:
+    from tornado4.ioloop import IOLoop
+    from tornado4.web import Application, StaticFileHandler, RequestHandler
+except:
+    from tornado.ioloop import IOLoop
+    from tornado.web import Application, StaticFileHandler, RequestHandler
+
 import types
 
 # Import 3rd-party libs
@@ -1398,7 +1404,7 @@
     .. code-block:: python
 
         @http_basic_auth(lambda u, p: u == 'foo' and p == 'bar')
-        class AuthenticatedHandler(tornado.web.RequestHandler):
+        class AuthenticatedHandler(RequestHandler):
             pass
     '''
     def wrapper(handler_class):
@@ -1512,16 +1518,16 @@
         self.wait = wait
         self.handler = handler \
             if handler is not None \
-            else tornado.web.StaticFileHandler
+            else StaticFileHandler
         self.web_root = None
 
     def target(self):
         '''
         Threading target which stands up the tornado application
         '''
-        self.ioloop = tornado.ioloop.IOLoop()
+        self.ioloop = IOLoop()
         self.ioloop.make_current()
-        self.application = tornado.web.Application(
+        self.application = Application(
             [(r'/(.*)', self.handler, {'path': self.root})])
         self.application.listen(self.port)
         self.ioloop.start()
diff -Naur a/tests/support/helpers.py.orig c/tests/support/helpers.py.orig
--- a/tests/support/helpers.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/tests/support/helpers.py.orig	2019-07-02 10:57:25.367937657 -0600
@@ -0,0 +1,1632 @@
+# -*- coding: utf-8 -*-
+'''
+    :copyright: Copyright 2013-2017 by the SaltStack Team, see AUTHORS for more details.
+    :license: Apache 2.0, see LICENSE for more details.
+
+
+    tests.support.helpers
+    ~~~~~~~~~~~~~~~~~~~~~
+
+    Test support helpers
+'''
+# pylint: disable=repr-flag-used-in-string,wrong-import-order
+
+# Import Python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import base64
+import errno
+import functools
+import inspect
+import logging
+import os
+import random
+import shutil
+import signal
+import socket
+import string
+import subprocess
+import sys
+import tempfile
+import textwrap
+import threading
+import time
+import tornado.ioloop
+import tornado.web
+import types
+
+# Import 3rd-party libs
+import psutil  # pylint: disable=3rd-party-module-not-gated
+from salt.ext import six
+from salt.ext.six.moves import range, builtins  # pylint: disable=import-error,redefined-builtin
+try:
+    from pytestsalt.utils import get_unused_localhost_port  # pylint: disable=unused-import
+except ImportError:
+    def get_unused_localhost_port():
+        '''
+        Return a random unused port on localhost
+        '''
+        usock = socket.socket(family=socket.AF_INET, type=socket.SOCK_STREAM)
+        usock.bind(('127.0.0.1', 0))
+        port = usock.getsockname()[1]
+        usock.close()
+        return port
+
+# Import Salt Tests Support libs
+from tests.support.unit import skip, _id
+from tests.support.mock import patch
+from tests.support.paths import FILES, TMP
+
+# Import Salt libs
+import salt.utils.files
+import salt.utils.platform
+import salt.utils.stringutils
+
+if salt.utils.platform.is_windows():
+    import salt.utils.win_functions
+else:
+    import pwd
+
+log = logging.getLogger(__name__)
+
+HAS_SYMLINKS = None
+
+
+def no_symlinks():
+    '''
+    Check if git is installed and has symlinks enabled in the configuration.
+    '''
+    global HAS_SYMLINKS
+    if HAS_SYMLINKS is not None:
+        return not HAS_SYMLINKS
+    output = ''
+    try:
+        output = subprocess.Popen(
+            ['git', 'config', '--get', 'core.symlinks'],
+            cwd=TMP,
+            stdout=subprocess.PIPE).communicate()[0]
+    except OSError as exc:
+        if exc.errno != errno.ENOENT:
+            raise
+    except subprocess.CalledProcessError:
+        # git returned non-zero status
+        pass
+    HAS_SYMLINKS = False
+    if output.strip() == 'true':
+        HAS_SYMLINKS = True
+    return not HAS_SYMLINKS
+
+
+def destructiveTest(caller):
+    '''
+    Mark a test case as a destructive test for example adding or removing users
+    from your system.
+
+    .. code-block:: python
+
+        class MyTestCase(TestCase):
+
+            @destructiveTest
+            def test_create_user(self):
+                pass
+    '''
+    if inspect.isclass(caller):
+        # We're decorating a class
+        old_setup = getattr(caller, 'setUp', None)
+
+        def setUp(self, *args, **kwargs):
+            if os.environ.get('DESTRUCTIVE_TESTS', 'False').lower() == 'false':
+                self.skipTest('Destructive tests are disabled')
+            if old_setup is not None:
+                old_setup(self, *args, **kwargs)
+        caller.setUp = setUp
+        return caller
+
+    # We're simply decorating functions
+    @functools.wraps(caller)
+    def wrap(cls):
+        if os.environ.get('DESTRUCTIVE_TESTS', 'False').lower() == 'false':
+            cls.skipTest('Destructive tests are disabled')
+        return caller(cls)
+    return wrap
+
+
+def expensiveTest(caller):
+    '''
+    Mark a test case as an expensive test, for example, a test which can cost
+    money(Salt's cloud provider tests).
+
+    .. code-block:: python
+
+        class MyTestCase(TestCase):
+
+            @expensiveTest
+            def test_create_user(self):
+                pass
+    '''
+    if inspect.isclass(caller):
+        # We're decorating a class
+        old_setup = getattr(caller, 'setUp', None)
+
+        def setUp(self, *args, **kwargs):
+            if os.environ.get('EXPENSIVE_TESTS', 'False').lower() == 'false':
+                self.skipTest('Expensive tests are disabled')
+            if old_setup is not None:
+                old_setup(self, *args, **kwargs)
+        caller.setUp = setUp
+        return caller
+
+    # We're simply decorating functions
+    @functools.wraps(caller)
+    def wrap(cls):
+        if os.environ.get('EXPENSIVE_TESTS', 'False').lower() == 'false':
+            cls.skipTest('Expensive tests are disabled')
+        return caller(cls)
+    return wrap
+
+
+def flaky(caller=None, condition=True, attempts=4):
+    '''
+    Mark a test as flaky. The test will attempt to run five times,
+    looking for a successful run. After an immediate second try,
+    it will use an exponential backoff starting with one second.
+
+    .. code-block:: python
+
+        class MyTestCase(TestCase):
+
+        @flaky
+        def test_sometimes_works(self):
+            pass
+    '''
+    if caller is None:
+        return functools.partial(flaky, condition=condition, attempts=attempts)
+
+    if isinstance(condition, bool) and condition is False:
+        # Don't even decorate
+        return caller
+    elif callable(condition):
+        if condition() is False:
+            # Don't even decorate
+            return caller
+
+    if inspect.isclass(caller):
+        attrs = [n for n in dir(caller) if n.startswith('test_')]
+        for attrname in attrs:
+            try:
+                function = getattr(caller, attrname)
+                if not inspect.isfunction(function) and not inspect.ismethod(function):
+                    continue
+                setattr(caller, attrname, flaky(caller=function, condition=condition, attempts=attempts))
+            except Exception as exc:
+                log.exception(exc)
+                continue
+        return caller
+
+    @functools.wraps(caller)
+    def wrap(cls):
+        for attempt in range(0, attempts):
+            try:
+                return caller(cls)
+            except Exception as exc:
+                if attempt >= attempts -1:
+                    raise exc
+                backoff_time = attempt ** 2
+                log.info(
+                    'Found Exception. Waiting %s seconds to retry.',
+                    backoff_time
+                )
+                time.sleep(backoff_time)
+        return cls
+    return wrap
+
+
+def requires_sshd_server(caller):
+    '''
+    Mark a test as requiring the tests SSH daemon running.
+
+    .. code-block:: python
+
+        class MyTestCase(TestCase):
+
+            @requiresSshdServer
+            def test_create_user(self):
+                pass
+    '''
+    if inspect.isclass(caller):
+        # We're decorating a class
+        old_setup = getattr(caller, 'setUp', None)
+
+        def setUp(self, *args, **kwargs):
+            if os.environ.get('SSH_DAEMON_RUNNING', 'False').lower() == 'false':
+                self.skipTest('SSH tests are disabled')
+            if old_setup is not None:
+                old_setup(self, *args, **kwargs)
+        caller.setUp = setUp
+        return caller
+
+    # We're simply decorating functions
+    @functools.wraps(caller)
+    def wrap(cls):
+        if os.environ.get('SSH_DAEMON_RUNNING', 'False').lower() == 'false':
+            cls.skipTest('SSH tests are disabled')
+        return caller(cls)
+    return wrap
+
+
+class RedirectStdStreams(object):
+    '''
+    Temporarily redirect system output to file like objects.
+    Default is to redirect to `os.devnull`, which just mutes output, `stdout`
+    and `stderr`.
+    '''
+
+    def __init__(self, stdout=None, stderr=None):
+        # Late import
+        import salt.utils.files
+        if stdout is None:
+            stdout = salt.utils.files.fopen(os.devnull, 'w')  # pylint: disable=resource-leakage
+        if stderr is None:
+            stderr = salt.utils.files.fopen(os.devnull, 'w')  # pylint: disable=resource-leakage
+
+        self.__stdout = stdout
+        self.__stderr = stderr
+        self.__redirected = False
+        self.patcher = patch.multiple(sys, stderr=self.__stderr, stdout=self.__stdout)
+
+    def __enter__(self):
+        self.redirect()
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.unredirect()
+
+    def redirect(self):
+        self.old_stdout = sys.stdout
+        self.old_stdout.flush()
+        self.old_stderr = sys.stderr
+        self.old_stderr.flush()
+        self.patcher.start()
+        self.__redirected = True
+
+    def unredirect(self):
+        if not self.__redirected:
+            return
+        try:
+            self.__stdout.flush()
+            self.__stdout.close()
+        except ValueError:
+            # already closed?
+            pass
+        try:
+            self.__stderr.flush()
+            self.__stderr.close()
+        except ValueError:
+            # already closed?
+            pass
+        self.patcher.stop()
+
+    def flush(self):
+        if self.__redirected:
+            try:
+                self.__stdout.flush()
+            except Exception:
+                pass
+            try:
+                self.__stderr.flush()
+            except Exception:
+                pass
+
+
+class TestsLoggingHandler(object):
+    '''
+    Simple logging handler which can be used to test if certain logging
+    messages get emitted or not:
+
+    .. code-block:: python
+
+        with TestsLoggingHandler() as handler:
+            # (...)               Do what ever you wish here
+            handler.messages    # here are the emitted log messages
+
+    '''
+    def __init__(self, level=0, format='%(levelname)s:%(message)s'):
+        self.level = level
+        self.format = format
+        self.activated = False
+        self.prev_logging_level = None
+
+    def activate(self):
+        class Handler(logging.Handler):
+            def __init__(self, level):
+                logging.Handler.__init__(self, level)
+                self.messages = []
+
+            def emit(self, record):
+                self.messages.append(self.format(record))
+
+        self.handler = Handler(self.level)
+        formatter = logging.Formatter(self.format)
+        self.handler.setFormatter(formatter)
+        logging.root.addHandler(self.handler)
+        self.activated = True
+        # Make sure we're running with the lowest logging level with our
+        # tests logging handler
+        current_logging_level = logging.root.getEffectiveLevel()
+        if current_logging_level > logging.DEBUG:
+            self.prev_logging_level = current_logging_level
+            logging.root.setLevel(0)
+
+    def deactivate(self):
+        if not self.activated:
+            return
+        logging.root.removeHandler(self.handler)
+        # Restore previous logging level if changed
+        if self.prev_logging_level is not None:
+            logging.root.setLevel(self.prev_logging_level)
+
+    @property
+    def messages(self):
+        if not self.activated:
+            return []
+        return self.handler.messages
+
+    def clear(self):
+        self.handler.messages = []
+
+    def __enter__(self):
+        self.activate()
+        return self
+
+    def __exit__(self, type, value, traceback):
+        self.deactivate()
+        self.activated = False
+
+    # Mimic some handler attributes and methods
+    @property
+    def lock(self):
+        if self.activated:
+            return self.handler.lock
+
+    def createLock(self):
+        if self.activated:
+            return self.handler.createLock()
+
+    def acquire(self):
+        if self.activated:
+            return self.handler.acquire()
+
+    def release(self):
+        if self.activated:
+            return self.handler.release()
+
+
+def relative_import(import_name, relative_from='../'):
+    '''
+    Update sys.path to include `relative_from` before importing `import_name`
+    '''
+    try:
+        return __import__(import_name)
+    except ImportError:
+        previous_frame = inspect.getframeinfo(inspect.currentframe().f_back)
+        sys.path.insert(
+            0, os.path.realpath(
+                os.path.join(
+                    os.path.abspath(
+                        os.path.dirname(previous_frame.filename)
+                    ),
+                    relative_from
+                )
+            )
+        )
+    return __import__(import_name)
+
+
+class ForceImportErrorOn(object):
+    '''
+    This class is meant to be used in mock'ed test cases which require an
+    ``ImportError`` to be raised.
+
+    >>> import os.path
+    >>> with ForceImportErrorOn('os.path'):
+    ...     import os.path
+    ...
+    Traceback (most recent call last):
+      File "<stdin>", line 2, in <module>
+      File "salttesting/helpers.py", line 263, in __import__
+        'Forced ImportError raised for {0!r}'.format(name)
+    ImportError: Forced ImportError raised for 'os.path'
+    >>>
+
+
+    >>> with ForceImportErrorOn(('os', 'path')):
+    ...     import os.path
+    ...     sys.modules.pop('os', None)
+    ...     from os import path
+    ...
+    <module 'os' from '/usr/lib/python2.7/os.pyc'>
+    Traceback (most recent call last):
+      File "<stdin>", line 4, in <module>
+      File "salttesting/helpers.py", line 288, in __fake_import__
+        name, ', '.join(fromlist)
+    ImportError: Forced ImportError raised for 'from os import path'
+    >>>
+
+
+    >>> with ForceImportErrorOn(('os', 'path'), 'os.path'):
+    ...     import os.path
+    ...     sys.modules.pop('os', None)
+    ...     from os import path
+    ...
+    Traceback (most recent call last):
+      File "<stdin>", line 2, in <module>
+      File "salttesting/helpers.py", line 281, in __fake_import__
+        'Forced ImportError raised for {0!r}'.format(name)
+    ImportError: Forced ImportError raised for 'os.path'
+    >>>
+    '''
+    def __init__(self, *module_names):
+        self.__module_names = {}
+        for entry in module_names:
+            if isinstance(entry, (list, tuple)):
+                modname = entry[0]
+                self.__module_names[modname] = set(entry[1:])
+            else:
+                self.__module_names[entry] = None
+        self.__original_import = builtins.__import__
+        self.patcher = patch.object(builtins, '__import__', self.__fake_import__)
+
+    def patch_import_function(self):
+        self.patcher.start()
+
+    def restore_import_funtion(self):
+        self.patcher.stop()
+
+    def __fake_import__(self,
+                        name,
+                        globals_={} if six.PY2 else None,
+                        locals_={} if six.PY2 else None,
+                        fromlist=[] if six.PY2 else (),
+                        level=-1 if six.PY2 else 0):
+        if name in self.__module_names:
+            importerror_fromlist = self.__module_names.get(name)
+            if importerror_fromlist is None:
+                raise ImportError(
+                    'Forced ImportError raised for {0!r}'.format(name)
+                )
+
+            if importerror_fromlist.intersection(set(fromlist)):
+                raise ImportError(
+                    'Forced ImportError raised for {0!r}'.format(
+                        'from {0} import {1}'.format(
+                            name, ', '.join(fromlist)
+                        )
+                    )
+                )
+        return self.__original_import(name, globals_, locals_, fromlist, level)
+
+    def __enter__(self):
+        self.patch_import_function()
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.restore_import_funtion()
+
+
+class MockWraps(object):
+    '''
+    Helper class to be used with the mock library.
+    To be used in the ``wraps`` keyword of ``Mock`` or ``MagicMock`` where you
+    want to trigger a side effect for X times, and afterwards, call the
+    original and un-mocked method.
+
+    As an example:
+
+    >>> def original():
+    ...     print 'original'
+    ...
+    >>> def side_effect():
+    ...     print 'side effect'
+    ...
+    >>> mw = MockWraps(original, 2, side_effect)
+    >>> mw()
+    side effect
+    >>> mw()
+    side effect
+    >>> mw()
+    original
+    >>>
+
+    '''
+    def __init__(self, original, expected_failures, side_effect):
+        self.__original = original
+        self.__expected_failures = expected_failures
+        self.__side_effect = side_effect
+        self.__call_counter = 0
+
+    def __call__(self, *args, **kwargs):
+        try:
+            if self.__call_counter < self.__expected_failures:
+                if isinstance(self.__side_effect, types.FunctionType):
+                    return self.__side_effect()
+                raise self.__side_effect
+            return self.__original(*args, **kwargs)
+        finally:
+            self.__call_counter += 1
+
+
+def requires_network(only_local_network=False):
+    '''
+    Simple decorator which is supposed to skip a test case in case there's no
+    network connection to the internet.
+    '''
+    def decorator(func):
+        @functools.wraps(func)
+        def wrapper(cls):
+            has_local_network = False
+            # First lets try if we have a local network. Inspired in
+            # verify_socket
+            try:
+                pubsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                retsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                pubsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+                pubsock.bind(('', 18000))
+                pubsock.close()
+                retsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+                retsock.bind(('', 18001))
+                retsock.close()
+                has_local_network = True
+            except socket.error:
+                # I wonder if we just have IPV6 support?
+                try:
+                    pubsock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
+                    retsock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
+                    pubsock.setsockopt(
+                        socket.SOL_SOCKET, socket.SO_REUSEADDR, 1
+                    )
+                    pubsock.bind(('', 18000))
+                    pubsock.close()
+                    retsock.setsockopt(
+                        socket.SOL_SOCKET, socket.SO_REUSEADDR, 1
+                    )
+                    retsock.bind(('', 18001))
+                    retsock.close()
+                    has_local_network = True
+                except socket.error:
+                    # Let's continue
+                    pass
+
+            if only_local_network is True:
+                if has_local_network is False:
+                    # Since we're only supposed to check local network, and no
+                    # local network was detected, skip the test
+                    cls.skipTest('No local network was detected')
+                return func(cls)
+
+            # We are using the google.com DNS records as numerical IPs to avoid
+            # DNS lookups which could greatly slow down this check
+            for addr in ('173.194.41.198', '173.194.41.199', '173.194.41.200',
+                         '173.194.41.201', '173.194.41.206', '173.194.41.192',
+                         '173.194.41.193', '173.194.41.194', '173.194.41.195',
+                         '173.194.41.196', '173.194.41.197'):
+                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+                try:
+                    sock.settimeout(0.25)
+                    sock.connect((addr, 80))
+                    # We connected? Stop the loop
+                    break
+                except socket.error:
+                    # Let's check the next IP
+                    continue
+                else:
+                    cls.skipTest('No internet network connection was detected')
+                finally:
+                    sock.close()
+            return func(cls)
+        return wrapper
+    return decorator
+
+
+def with_system_user(username, on_existing='delete', delete=True, password=None, groups=None):
+    '''
+    Create and optionally destroy a system user to be used within a test
+    case. The system user is created using the ``user`` salt module.
+
+    The decorated testcase function must accept 'username' as an argument.
+
+    :param username: The desired username for the system user.
+    :param on_existing: What to do when the desired username is taken. The
+      available options are:
+
+      * nothing: Do nothing, act as if the user was created.
+      * delete: delete and re-create the existing user
+      * skip: skip the test case
+    '''
+    if on_existing not in ('nothing', 'delete', 'skip'):
+        raise RuntimeError(
+            'The value of \'on_existing\' can only be one of, '
+            '\'nothing\', \'delete\' and \'skip\''
+        )
+
+    if not isinstance(delete, bool):
+        raise RuntimeError(
+            'The value of \'delete\' can only be \'True\' or \'False\''
+        )
+
+    def decorator(func):
+
+        @functools.wraps(func)
+        def wrap(cls):
+
+            # Let's add the user to the system.
+            log.debug('Creating system user {0!r}'.format(username))
+            kwargs = {'timeout': 60, 'groups': groups}
+            if salt.utils.platform.is_windows():
+                kwargs.update({'password': password})
+            create_user = cls.run_function('user.add', [username], **kwargs)
+            if not create_user:
+                log.debug('Failed to create system user')
+                # The user was not created
+                if on_existing == 'skip':
+                    cls.skipTest(
+                        'Failed to create system user {0!r}'.format(
+                            username
+                        )
+                    )
+
+                if on_existing == 'delete':
+                    log.debug(
+                        'Deleting the system user {0!r}'.format(
+                            username
+                        )
+                    )
+                    delete_user = cls.run_function(
+                        'user.delete', [username, True, True]
+                    )
+                    if not delete_user:
+                        cls.skipTest(
+                            'A user named {0!r} already existed on the '
+                            'system and re-creating it was not possible'
+                            .format(username)
+                        )
+                    log.debug(
+                        'Second time creating system user {0!r}'.format(
+                            username
+                        )
+                    )
+                    create_user = cls.run_function('user.add', [username], **kwargs)
+                    if not create_user:
+                        cls.skipTest(
+                            'A user named {0!r} already existed, was deleted '
+                            'as requested, but re-creating it was not possible'
+                            .format(username)
+                        )
+
+            failure = None
+            try:
+                try:
+                    return func(cls, username)
+                except Exception as exc:  # pylint: disable=W0703
+                    log.error(
+                        'Running {0!r} raised an exception: {1}'.format(
+                            func, exc
+                        ),
+                        exc_info=True
+                    )
+                    # Store the original exception details which will be raised
+                    # a little further down the code
+                    failure = sys.exc_info()
+            finally:
+                if delete:
+                    delete_user = cls.run_function(
+                        'user.delete', [username, True, True], timeout=60
+                    )
+                    if not delete_user:
+                        if failure is None:
+                            log.warning(
+                                'Although the actual test-case did not fail, '
+                                'deleting the created system user {0!r} '
+                                'afterwards did.'.format(username)
+                            )
+                        else:
+                            log.warning(
+                                'The test-case failed and also did the removal'
+                                ' of the system user {0!r}'.format(username)
+                            )
+                if failure is not None:
+                    # If an exception was thrown, raise it
+                    six.reraise(failure[0], failure[1], failure[2])
+        return wrap
+    return decorator
+
+
+def with_system_group(group, on_existing='delete', delete=True):
+    '''
+    Create and optionally destroy a system group to be used within a test
+    case. The system user is crated using the ``group`` salt module.
+
+    The decorated testcase function must accept 'group' as an argument.
+
+    :param group: The desired group name for the system user.
+    :param on_existing: What to do when the desired username is taken. The
+      available options are:
+
+      * nothing: Do nothing, act as if the group was created
+      * delete: delete and re-create the existing user
+      * skip: skip the test case
+    '''
+    if on_existing not in ('nothing', 'delete', 'skip'):
+        raise RuntimeError(
+            'The value of \'on_existing\' can only be one of, '
+            '\'nothing\', \'delete\' and \'skip\''
+        )
+
+    if not isinstance(delete, bool):
+        raise RuntimeError(
+            'The value of \'delete\' can only be \'True\' or \'False\''
+        )
+
+    def decorator(func):
+
+        @functools.wraps(func)
+        def wrap(cls):
+
+            # Let's add the user to the system.
+            log.debug('Creating system group {0!r}'.format(group))
+            create_group = cls.run_function('group.add', [group])
+            if not create_group:
+                log.debug('Failed to create system group')
+                # The group was not created
+                if on_existing == 'skip':
+                    cls.skipTest(
+                        'Failed to create system group {0!r}'.format(group)
+                    )
+
+                if on_existing == 'delete':
+                    log.debug(
+                        'Deleting the system group {0!r}'.format(group)
+                    )
+                    delete_group = cls.run_function('group.delete', [group])
+                    if not delete_group:
+                        cls.skipTest(
+                            'A group named {0!r} already existed on the '
+                            'system and re-creating it was not possible'
+                            .format(group)
+                        )
+                    log.debug(
+                        'Second time creating system group {0!r}'.format(
+                            group
+                        )
+                    )
+                    create_group = cls.run_function('group.add', [group])
+                    if not create_group:
+                        cls.skipTest(
+                            'A group named {0!r} already existed, was deleted '
+                            'as requested, but re-creating it was not possible'
+                            .format(group)
+                        )
+
+            failure = None
+            try:
+                try:
+                    return func(cls, group)
+                except Exception as exc:  # pylint: disable=W0703
+                    log.error(
+                        'Running {0!r} raised an exception: {1}'.format(
+                            func, exc
+                        ),
+                        exc_info=True
+                    )
+                    # Store the original exception details which will be raised
+                    # a little further down the code
+                    failure = sys.exc_info()
+            finally:
+                if delete:
+                    delete_group = cls.run_function('group.delete', [group])
+                    if not delete_group:
+                        if failure is None:
+                            log.warning(
+                                'Although the actual test-case did not fail, '
+                                'deleting the created system group {0!r} '
+                                'afterwards did.'.format(group)
+                            )
+                        else:
+                            log.warning(
+                                'The test-case failed and also did the removal'
+                                ' of the system group {0!r}'.format(group)
+                            )
+                if failure is not None:
+                    # If an exception was thrown, raise it
+                    six.reraise(failure[0], failure[1], failure[2])
+        return wrap
+    return decorator
+
+
+def with_system_user_and_group(username, group,
+                               on_existing='delete', delete=True):
+    '''
+    Create and optionally destroy a system user and group to be used within a
+    test case. The system user is crated using the ``user`` salt module, and
+    the system group is created with the ``group`` salt module.
+
+    The decorated testcase function must accept both the 'username' and 'group'
+    arguments.
+
+    :param username: The desired username for the system user.
+    :param group: The desired name for the system group.
+    :param on_existing: What to do when the desired username is taken. The
+      available options are:
+
+      * nothing: Do nothing, act as if the user was created.
+      * delete: delete and re-create the existing user
+      * skip: skip the test case
+    '''
+    if on_existing not in ('nothing', 'delete', 'skip'):
+        raise RuntimeError(
+            'The value of \'on_existing\' can only be one of, '
+            '\'nothing\', \'delete\' and \'skip\''
+        )
+
+    if not isinstance(delete, bool):
+        raise RuntimeError(
+            'The value of \'delete\' can only be \'True\' or \'False\''
+        )
+
+    def decorator(func):
+
+        @functools.wraps(func)
+        def wrap(cls):
+
+            # Let's add the user to the system.
+            log.debug('Creating system user {0!r}'.format(username))
+            create_user = cls.run_function('user.add', [username])
+            log.debug('Creating system group {0!r}'.format(group))
+            create_group = cls.run_function('group.add', [group])
+            if not create_user:
+                log.debug('Failed to create system user')
+                # The user was not created
+                if on_existing == 'skip':
+                    cls.skipTest(
+                        'Failed to create system user {0!r}'.format(
+                            username
+                        )
+                    )
+
+                if on_existing == 'delete':
+                    log.debug(
+                        'Deleting the system user {0!r}'.format(
+                            username
+                        )
+                    )
+                    delete_user = cls.run_function(
+                        'user.delete', [username, True, True]
+                    )
+                    if not delete_user:
+                        cls.skipTest(
+                            'A user named {0!r} already existed on the '
+                            'system and re-creating it was not possible'
+                            .format(username)
+                        )
+                    log.debug(
+                        'Second time creating system user {0!r}'.format(
+                            username
+                        )
+                    )
+                    create_user = cls.run_function('user.add', [username])
+                    if not create_user:
+                        cls.skipTest(
+                            'A user named {0!r} already existed, was deleted '
+                            'as requested, but re-creating it was not possible'
+                            .format(username)
+                        )
+            if not create_group:
+                log.debug('Failed to create system group')
+                # The group was not created
+                if on_existing == 'skip':
+                    cls.skipTest(
+                        'Failed to create system group {0!r}'.format(group)
+                    )
+
+                if on_existing == 'delete':
+                    log.debug(
+                        'Deleting the system group {0!r}'.format(group)
+                    )
+                    delete_group = cls.run_function('group.delete', [group])
+                    if not delete_group:
+                        cls.skipTest(
+                            'A group named {0!r} already existed on the '
+                            'system and re-creating it was not possible'
+                            .format(group)
+                        )
+                    log.debug(
+                        'Second time creating system group {0!r}'.format(
+                            group
+                        )
+                    )
+                    create_group = cls.run_function('group.add', [group])
+                    if not create_group:
+                        cls.skipTest(
+                            'A group named {0!r} already existed, was deleted '
+                            'as requested, but re-creating it was not possible'
+                            .format(group)
+                        )
+
+            failure = None
+            try:
+                try:
+                    return func(cls, username, group)
+                except Exception as exc:  # pylint: disable=W0703
+                    log.error(
+                        'Running {0!r} raised an exception: {1}'.format(
+                            func, exc
+                        ),
+                        exc_info=True
+                    )
+                    # Store the original exception details which will be raised
+                    # a little further down the code
+                    failure = sys.exc_info()
+            finally:
+                if delete:
+                    delete_user = cls.run_function(
+                        'user.delete', [username, True, True]
+                    )
+                    delete_group = cls.run_function('group.delete', [group])
+                    if not delete_user:
+                        if failure is None:
+                            log.warning(
+                                'Although the actual test-case did not fail, '
+                                'deleting the created system user {0!r} '
+                                'afterwards did.'.format(username)
+                            )
+                        else:
+                            log.warning(
+                                'The test-case failed and also did the removal'
+                                ' of the system user {0!r}'.format(username)
+                            )
+                    if not delete_group:
+                        if failure is None:
+                            log.warning(
+                                'Although the actual test-case did not fail, '
+                                'deleting the created system group {0!r} '
+                                'afterwards did.'.format(group)
+                            )
+                        else:
+                            log.warning(
+                                'The test-case failed and also did the removal'
+                                ' of the system group {0!r}'.format(group)
+                            )
+                if failure is not None:
+                    # If an exception was thrown, raise it
+                    six.reraise(failure[0], failure[1], failure[2])
+        return wrap
+    return decorator
+
+
+class WithTempfile(object):
+    def __init__(self, **kwargs):
+        self.create = kwargs.pop('create', True)
+        if 'dir' not in kwargs:
+            kwargs['dir'] = TMP
+        if 'prefix' not in kwargs:
+            kwargs['prefix'] = '__salt.test.'
+        self.kwargs = kwargs
+
+    def __call__(self, func):
+        self.func = func
+        return functools.wraps(func)(
+            lambda testcase, *args, **kwargs: self.wrap(testcase, *args, **kwargs)  # pylint: disable=W0108
+        )
+
+    def wrap(self, testcase, *args, **kwargs):
+        name = salt.utils.files.mkstemp(**self.kwargs)
+        if not self.create:
+            os.remove(name)
+        try:
+            return self.func(testcase, name, *args, **kwargs)
+        finally:
+            try:
+                os.remove(name)
+            except OSError:
+                pass
+
+
+with_tempfile = WithTempfile
+
+
+class WithTempdir(object):
+    def __init__(self, **kwargs):
+        self.create = kwargs.pop('create', True)
+        if 'dir' not in kwargs:
+            kwargs['dir'] = TMP
+        self.kwargs = kwargs
+
+    def __call__(self, func):
+        self.func = func
+        return functools.wraps(func)(
+            lambda testcase, *args, **kwargs: self.wrap(testcase, *args, **kwargs)  # pylint: disable=W0108
+        )
+
+    def wrap(self, testcase, *args, **kwargs):
+        tempdir = tempfile.mkdtemp(**self.kwargs)
+        if not self.create:
+            os.rmdir(tempdir)
+        try:
+            return self.func(testcase, tempdir, *args, **kwargs)
+        finally:
+            shutil.rmtree(tempdir, ignore_errors=True)
+
+
+with_tempdir = WithTempdir
+
+
+def requires_system_grains(func):
+    '''
+    Function decorator which loads and passes the system's grains to the test
+    case.
+    '''
+    @functools.wraps(func)
+    def decorator(cls):
+        if not hasattr(cls, 'run_function'):
+            raise RuntimeError(
+                '{0} does not have the \'run_function\' method which is '
+                'necessary to collect the system grains'.format(
+                    cls.__class__.__name__
+                )
+            )
+        return func(cls, grains=cls.run_function('grains.items'))
+    return decorator
+
+
+def requires_salt_modules(*names):
+    '''
+    Makes sure the passed salt module is available. Skips the test if not
+
+    .. versionadded:: 0.5.2
+    '''
+    def decorator(caller):
+
+        if inspect.isclass(caller):
+            # We're decorating a class
+            old_setup = getattr(caller, 'setUp', None)
+
+            def setUp(self, *args, **kwargs):
+                if old_setup is not None:
+                    old_setup(self, *args, **kwargs)
+
+                if not hasattr(self, 'run_function'):
+                    raise RuntimeError(
+                        '{0} does not have the \'run_function\' method which '
+                        'is necessary to collect the loaded modules'.format(
+                            self.__class__.__name__
+                        )
+                    )
+
+                not_found_modules = self.run_function('runtests_helpers.modules_available', names)
+                if not_found_modules:
+                    if len(not_found_modules) == 1:
+                        self.skipTest('Salt module {0!r} is not available'.format(not_found_modules[0]))
+                    self.skipTest('Salt modules not available: {0!r}'.format(not_found_modules))
+            caller.setUp = setUp
+            return caller
+
+        # We're simply decorating functions
+        @functools.wraps(caller)
+        def wrapper(cls):
+
+            if not hasattr(cls, 'run_function'):
+                raise RuntimeError(
+                    '{0} does not have the \'run_function\' method which is '
+                    'necessary to collect the loaded modules'.format(
+                        cls.__class__.__name__
+                    )
+                )
+
+            for name in names:
+                if name not in cls.run_function('sys.doc', [name]):
+                    cls.skipTest(
+                        'Salt module {0!r} is not available'.format(name)
+                    )
+                    break
+
+            return caller(cls)
+        return wrapper
+    return decorator
+
+
+def skip_if_binaries_missing(*binaries, **kwargs):
+    import salt.utils.path
+    if len(binaries) == 1:
+        if isinstance(binaries[0], (list, tuple, set, frozenset)):
+            binaries = binaries[0]
+    check_all = kwargs.pop('check_all', False)
+    message = kwargs.pop('message', None)
+    if kwargs:
+        raise RuntimeError(
+            'The only supported keyword argument is \'check_all\' and '
+            '\'message\'. Invalid keyword arguments: {0}'.format(
+                ', '.join(kwargs.keys())
+            )
+        )
+    if check_all:
+        for binary in binaries:
+            if salt.utils.path.which(binary) is None:
+                return skip(
+                    '{0}The {1!r} binary was not found'.format(
+                        message and '{0}. '.format(message) or '',
+                        binary
+                    )
+                )
+    elif salt.utils.path.which_bin(binaries) is None:
+        return skip(
+            '{0}None of the following binaries was found: {1}'.format(
+                message and '{0}. '.format(message) or '',
+                ', '.join(binaries)
+            )
+        )
+    return _id
+
+
+def skip_if_not_root(func):
+    if not sys.platform.startswith('win'):
+        if os.getuid() != 0:
+            func.__unittest_skip__ = True
+            func.__unittest_skip_why__ = 'You must be logged in as root to run this test'
+    else:
+        current_user = salt.utils.win_functions.get_current_user()
+        if current_user != 'SYSTEM':
+            if not salt.utils.win_functions.is_admin(current_user):
+                func.__unittest_skip__ = True
+                func.__unittest_skip_why__ = 'You must be logged in as an Administrator to run this test'
+    return func
+
+
+if sys.platform.startswith('win'):
+    SIGTERM = signal.CTRL_BREAK_EVENT  # pylint: disable=no-member
+else:
+    SIGTERM = signal.SIGTERM
+
+
+def collect_child_processes(pid):
+    '''
+    Try to collect any started child processes of the provided pid
+    '''
+    # Let's get the child processes of the started subprocess
+    try:
+        parent = psutil.Process(pid)
+        if hasattr(parent, 'children'):
+            children = parent.children(recursive=True)
+        else:
+            children = []
+    except psutil.NoSuchProcess:
+        children = []
+    return children[::-1]  # return a reversed list of the children
+
+
+def _terminate_process_list(process_list, kill=False, slow_stop=False):
+    for process in process_list[:][::-1]:  # Iterate over a reversed copy of the list
+        if not psutil.pid_exists(process.pid):
+            process_list.remove(process)
+            continue
+        try:
+            if not kill and process.status() == psutil.STATUS_ZOMBIE:
+                # Zombie processes will exit once child processes also exit
+                continue
+            try:
+                cmdline = process.cmdline()
+            except psutil.AccessDenied:
+                # OSX is more restrictive about the above information
+                cmdline = None
+            if not cmdline:
+                try:
+                    cmdline = process.as_dict()
+                except Exception:
+                    cmdline = 'UNKNOWN PROCESS'
+            if kill:
+                log.info('Killing process(%s): %s', process.pid, cmdline)
+                process.kill()
+            else:
+                log.info('Terminating process(%s): %s', process.pid, cmdline)
+                try:
+                    if slow_stop:
+                        # Allow coverage data to be written down to disk
+                        process.send_signal(SIGTERM)
+                        try:
+                            process.wait(2)
+                        except psutil.TimeoutExpired:
+                            if psutil.pid_exists(process.pid):
+                                continue
+                    else:
+                        process.terminate()
+                except OSError as exc:
+                    if exc.errno not in (errno.ESRCH, errno.EACCES):
+                        raise
+            if not psutil.pid_exists(process.pid):
+                process_list.remove(process)
+        except psutil.NoSuchProcess:
+            process_list.remove(process)
+
+
+def terminate_process_list(process_list, kill=False, slow_stop=False):
+
+    def on_process_terminated(proc):
+        log.info('Process %s terminated with exit code: %s', getattr(proc, '_cmdline', proc), proc.returncode)
+
+    # Try to terminate processes with the provided kill and slow_stop parameters
+    log.info('Terminating process list. 1st step. kill: %s, slow stop: %s', kill, slow_stop)
+
+    # Cache the cmdline since that will be inaccessible once the process is terminated
+    for proc in process_list:
+        try:
+            cmdline = proc.cmdline()
+        except (psutil.NoSuchProcess, psutil.AccessDenied):
+            # OSX is more restrictive about the above information
+            cmdline = None
+        if not cmdline:
+            try:
+                cmdline = proc
+            except (psutil.NoSuchProcess, psutil.AccessDenied):
+                cmdline = '<could not be retrived; dead process: {0}>'.format(proc)
+        proc._cmdline = cmdline
+    _terminate_process_list(process_list, kill=kill, slow_stop=slow_stop)
+    psutil.wait_procs(process_list, timeout=15, callback=on_process_terminated)
+
+    if process_list:
+        # If there's still processes to be terminated, retry and kill them if slow_stop is False
+        log.info('Terminating process list. 2nd step. kill: %s, slow stop: %s', slow_stop is False, slow_stop)
+        _terminate_process_list(process_list, kill=slow_stop is False, slow_stop=slow_stop)
+        psutil.wait_procs(process_list, timeout=10, callback=on_process_terminated)
+
+    if process_list:
+        # If there's still processes to be terminated, just kill them, no slow stopping now
+        log.info('Terminating process list. 3rd step. kill: True, slow stop: False')
+        _terminate_process_list(process_list, kill=True, slow_stop=False)
+        psutil.wait_procs(process_list, timeout=5, callback=on_process_terminated)
+
+    if process_list:
+        # In there's still processes to be terminated, log a warning about it
+        log.warning('Some processes failed to properly terminate: %s', process_list)
+
+
+def terminate_process(pid=None, process=None, children=None, kill_children=False, slow_stop=False):
+    '''
+    Try to terminate/kill the started processe
+    '''
+    children = children or []
+    process_list = []
+
+    def on_process_terminated(proc):
+        if proc.returncode:
+            log.info('Process %s terminated with exit code: %s', getattr(proc, '_cmdline', proc), proc.returncode)
+        else:
+            log.info('Process %s terminated', getattr(proc, '_cmdline', proc))
+
+    if pid and not process:
+        try:
+            process = psutil.Process(pid)
+            process_list.append(process)
+        except psutil.NoSuchProcess:
+            # Process is already gone
+            process = None
+
+    if kill_children:
+        if process:
+            if not children:
+                children = collect_child_processes(process.pid)
+            else:
+                # Let's collect children again since there might be new ones
+                children.extend(collect_child_processes(pid))
+        if children:
+            process_list.extend(children)
+
+    if process_list:
+        if process:
+            log.info('Stopping process %s and respective children: %s', process, children)
+        else:
+            log.info('Terminating process list: %s', process_list)
+        terminate_process_list(process_list, kill=slow_stop is False, slow_stop=slow_stop)
+        if process and psutil.pid_exists(process.pid):
+            log.warning('Process left behind which we were unable to kill: %s', process)
+
+
+def terminate_process_pid(pid, only_children=False):
+    children = []
+    process = None
+
+    # Let's begin the shutdown routines
+    try:
+        process = psutil.Process(pid)
+        children = collect_child_processes(pid)
+    except psutil.NoSuchProcess:
+        log.info('No process with the PID %s was found running', pid)
+
+    if only_children:
+        return terminate_process(children=children, kill_children=True, slow_stop=True)
+    return terminate_process(pid=pid, process=process, children=children, kill_children=True, slow_stop=True)
+
+
+def repeat(caller=None, condition=True, times=5):
+    '''
+    Repeat a test X amount of times until the first failure.
+
+    .. code-block:: python
+
+        class MyTestCase(TestCase):
+
+        @repeat
+        def test_sometimes_works(self):
+            pass
+    '''
+    if caller is None:
+        return functools.partial(repeat, condition=condition, times=times)
+
+    if isinstance(condition, bool) and condition is False:
+        # Don't even decorate
+        return caller
+    elif callable(condition):
+        if condition() is False:
+            # Don't even decorate
+            return caller
+
+    if inspect.isclass(caller):
+        attrs = [n for n in dir(caller) if n.startswith('test_')]
+        for attrname in attrs:
+            try:
+                function = getattr(caller, attrname)
+                if not inspect.isfunction(function) and not inspect.ismethod(function):
+                    continue
+                setattr(caller, attrname, repeat(caller=function, condition=condition, times=times))
+            except Exception as exc:
+                log.exception(exc)
+                continue
+        return caller
+
+    @functools.wraps(caller)
+    def wrap(cls):
+        result = None
+        for attempt in range(1, times+1):
+            log.info('%s test run %d of %s times', cls, attempt, times)
+            caller(cls)
+        return cls
+    return wrap
+
+
+def http_basic_auth(login_cb=lambda username, password: False):
+    '''
+    A crude decorator to force a handler to request HTTP Basic Authentication
+
+    Example usage:
+
+    .. code-block:: python
+
+        @http_basic_auth(lambda u, p: u == 'foo' and p == 'bar')
+        class AuthenticatedHandler(tornado.web.RequestHandler):
+            pass
+    '''
+    def wrapper(handler_class):
+        def wrap_execute(handler_execute):
+            def check_auth(handler, kwargs):
+
+                auth = handler.request.headers.get('Authorization')
+
+                if auth is None or not auth.startswith('Basic '):
+                    # No username/password entered yet, we need to return a 401
+                    # and set the WWW-Authenticate header to request login.
+                    handler.set_status(401)
+                    handler.set_header(
+                        'WWW-Authenticate', 'Basic realm=Restricted')
+
+                else:
+                    # Strip the 'Basic ' from the beginning of the auth header
+                    # leaving the base64-encoded secret
+                    username, password = \
+                        base64.b64decode(auth[6:]).split(':', 1)
+
+                    if login_cb(username, password):
+                        # Authentication successful
+                        return
+                    else:
+                        # Authentication failed
+                        handler.set_status(403)
+
+                handler._transforms = []
+                handler.finish()
+
+            def _execute(self, transforms, *args, **kwargs):
+                check_auth(self, kwargs)
+                return handler_execute(self, transforms, *args, **kwargs)
+
+            return _execute
+
+        handler_class._execute = wrap_execute(handler_class._execute)
+        return handler_class
+    return wrapper
+
+
+def generate_random_name(prefix, size=6):
+    '''
+    Generates a random name by combining the provided prefix with a randomly generated
+    ascii string.
+
+    .. versionadded:: 2018.3.0
+
+    prefix
+        The string to prefix onto the randomly generated ascii string.
+
+    size
+        The number of characters to generate. Default: 6.
+    '''
+    return prefix + ''.join(
+        random.choice(string.ascii_uppercase + string.digits)
+        for x in range(size)
+    )
+
+
+class Webserver(object):
+    '''
+    Starts a tornado webserver on 127.0.0.1 on a random available port
+
+    USAGE:
+
+    .. code-block:: python
+
+        from tests.support.helpers import Webserver
+
+        webserver = Webserver('/path/to/web/root')
+        webserver.start()
+        webserver.stop()
+    '''
+    def __init__(self,
+                 root=None,
+                 port=None,
+                 wait=5,
+                 handler=None):
+        '''
+        root
+            Root directory of webserver. If not passed, it will default to the
+            location of the base environment of the integration suite's file
+            roots (tests/integration/files/file/base/)
+
+        port
+            Port on which to listen. If not passed, a random one will be chosen
+            at the time the start() function is invoked.
+
+        wait : 5
+            Number of seconds to wait for the socket to be open before raising
+            an exception
+
+        handler
+            Can be used to use a subclass of tornado.web.StaticFileHandler,
+            such as when enforcing authentication with the http_basic_auth
+            decorator.
+        '''
+        if port is not None and not isinstance(port, six.integer_types):
+            raise ValueError('port must be an integer')
+
+        if root is None:
+            root = os.path.join(FILES, 'file', 'base')
+        try:
+            self.root = os.path.realpath(root)
+        except AttributeError:
+            raise ValueError('root must be a string')
+
+        self.port = port
+        self.wait = wait
+        self.handler = handler \
+            if handler is not None \
+            else tornado.web.StaticFileHandler
+        self.web_root = None
+
+    def target(self):
+        '''
+        Threading target which stands up the tornado application
+        '''
+        self.ioloop = tornado.ioloop.IOLoop()
+        self.ioloop.make_current()
+        self.application = tornado.web.Application(
+            [(r'/(.*)', self.handler, {'path': self.root})])
+        self.application.listen(self.port)
+        self.ioloop.start()
+
+    @property
+    def listening(self):
+        if self.port is None:
+            return False
+        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        return sock.connect_ex(('127.0.0.1', self.port)) == 0
+
+    def url(self, path):
+        '''
+        Convenience function which, given a file path, will return a URL that
+        points to that path. If the path is relative, it will just be appended
+        to self.web_root.
+        '''
+        if self.web_root is None:
+            raise RuntimeError('Webserver instance has not been started')
+        err_msg = 'invalid path, must be either a relative path or a path ' \
+                  'within {0}'.format(self.root)
+        try:
+            relpath = path \
+                if not os.path.isabs(path) \
+                else os.path.relpath(path, self.root)
+            if relpath.startswith('..' + os.sep):
+                raise ValueError(err_msg)
+            return '/'.join((self.web_root, relpath))
+        except AttributeError:
+            raise ValueError(err_msg)
+
+    def start(self):
+        '''
+        Starts the webserver
+        '''
+        if self.port is None:
+            self.port = get_unused_localhost_port()
+
+        self.web_root = 'http://127.0.0.1:{0}'.format(self.port)
+
+        self.server_thread = threading.Thread(target=self.target)
+        self.server_thread.daemon = True
+        self.server_thread.start()
+
+        for idx in range(self.wait + 1):
+            if self.listening:
+                break
+            if idx != self.wait:
+                time.sleep(1)
+        else:
+            raise Exception(
+                'Failed to start tornado webserver on 127.0.0.1:{0} within '
+                '{1} seconds'.format(self.port, self.wait)
+            )
+
+    def stop(self):
+        '''
+        Stops the webserver
+        '''
+        self.ioloop.add_callback(self.ioloop.stop)
+        self.server_thread.join()
+
+
+def win32_kill_process_tree(pid, sig=signal.SIGTERM, include_parent=True,
+        timeout=None, on_terminate=None):
+    '''
+    Kill a process tree (including grandchildren) with signal "sig" and return
+    a (gone, still_alive) tuple.  "on_terminate", if specified, is a callabck
+    function which is called as soon as a child terminates.
+    '''
+    if pid == os.getpid():
+        raise RuntimeError("I refuse to kill myself")
+    try:
+        parent = psutil.Process(pid)
+    except psutil.NoSuchProcess:
+        log.debug("PID not found alive: %d", pid)
+        return ([], [])
+    children = parent.children(recursive=True)
+    if include_parent:
+        children.append(parent)
+    for p in children:
+        p.send_signal(sig)
+    gone, alive = psutil.wait_procs(children, timeout=timeout,
+                                    callback=on_terminate)
+    return (gone, alive)
+
+
+def this_user():
+    '''
+    Get the user associated with the current process.
+    '''
+    if salt.utils.platform.is_windows():
+        return salt.utils.win_functions.get_current_user(with_domain=False)
+    return pwd.getpwuid(os.getuid())[0]
+
+
+def dedent(text, linesep=os.linesep):
+    '''
+    A wrapper around textwrap.dedent that also sets line endings.
+    '''
+    linesep = salt.utils.stringutils.to_unicode(linesep)
+    unicode_text = textwrap.dedent(salt.utils.stringutils.to_unicode(text))
+    clean_text = linesep.join(unicode_text.splitlines())
+    if unicode_text.endswith(u'\n'):
+        clean_text += linesep
+    if not isinstance(text, six.text_type):
+        return salt.utils.stringutils.to_bytes(clean_text)
+    return clean_text
diff -Naur a/tests/unit/fileserver/test_gitfs.py c/tests/unit/fileserver/test_gitfs.py
--- a/tests/unit/fileserver/test_gitfs.py	2019-07-02 10:15:06.987874716 -0600
+++ c/tests/unit/fileserver/test_gitfs.py	2019-07-02 10:58:03.179938595 -0600
@@ -11,7 +11,6 @@
 import shutil
 import tempfile
 import textwrap
-import tornado.ioloop
 import logging
 import stat
 try:
@@ -19,6 +18,11 @@
 except ImportError:
     pass
 
+try:
+    from tornado4.ioloop import IOLoop
+except:
+    from tornado.ioloop import IOLoop
+
 # Import Salt Testing Libs
 from tests.support.mixins import LoaderModuleMockMixin
 from tests.support.unit import TestCase, skipIf
@@ -109,7 +113,7 @@
 
 def _clear_instance_map():
     try:
-        del salt.utils.gitfs.GitFS.instance_map[tornado.ioloop.IOLoop.current()]
+        del salt.utils.gitfs.GitFS.instance_map[IOLoop.current()]
     except KeyError:
         pass
 
diff -Naur a/tests/unit/modules/test_random_org.py c/tests/unit/modules/test_random_org.py
--- a/tests/unit/modules/test_random_org.py	2019-07-02 10:15:06.995874717 -0600
+++ c/tests/unit/modules/test_random_org.py	2019-07-02 10:58:03.179938595 -0600
@@ -15,7 +15,10 @@
 import salt.modules.random_org as random_org
 
 # Import 3rd-party libs
-from tornado.httpclient import HTTPClient
+try:
+    from tornado4.httpclient import HTTPClient
+except:
+    from tornado.httpclient import HTTPClient
 
 
 def check_status():
diff -Naur a/tests/unit/netapi/test_rest_tornado.py c/tests/unit/netapi/test_rest_tornado.py
--- a/tests/unit/netapi/test_rest_tornado.py	2019-07-02 10:15:07.003874717 -0600
+++ c/tests/unit/netapi/test_rest_tornado.py	2019-07-02 10:58:03.179938595 -0600
@@ -25,12 +25,20 @@
 # Import 3rd-party libs
 # pylint: disable=import-error
 try:
-    import tornado.escape
-    import tornado.testing
-    import tornado.concurrent
-    from tornado.testing import AsyncTestCase, AsyncHTTPTestCase, gen_test
-    from tornado.httpclient import HTTPRequest, HTTPError
-    from tornado.websocket import websocket_connect
+    try;
+        from tornado4.escape import native_str
+        from tornado4.concurrent import Future as TornadoFuture
+        from tornado4.testing import AsyncTestCase, AsyncHTTPTestCase, gen_test
+        from tornado4.httpclient import HTTPRequest, HTTPError
+        from tornado4.web import Application
+        from tornado4.websocket import websocket_connect
+    except ImportError:
+        from tornado.escape import native_str
+        from tornado.concurrent import Future as TornadoFuture
+        from tornado.testing import AsyncTestCase, AsyncHTTPTestCase, gen_test
+        from tornado.httpclient import HTTPRequest, HTTPError
+        from tornado.web import Application
+        from tornado.websocket import websocket_connect
     import salt.netapi.rest_tornado as rest_tornado
     from salt.netapi.rest_tornado import saltnado
     HAS_TORNADO = True
@@ -125,7 +133,7 @@
             del self.application
 
     def build_tornado_app(self, urls):
-        application = tornado.web.Application(urls, debug=True)
+        application = Application(urls, debug=True)
 
         application.auth = self.auth
         application.opts = self.opts
@@ -143,7 +151,7 @@
             if response.headers.get('Content-Type') == 'application/json':
                 response._body = response.body.decode('utf-8')
             else:
-                response._body = tornado.escape.native_str(response.body)
+                response._body = native_str(response.body)
         return response
 
     def fetch(self, path, **kwargs):
@@ -772,7 +780,7 @@
         # create a few futures
         futures = []
         for x in range(0, 3):
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             future.add_done_callback(self.stop)
             futures.append(future)
 
diff -Naur a/tests/unit/test_minion.py c/tests/unit/test_minion.py
--- a/tests/unit/test_minion.py	2019-07-02 10:15:06.991874717 -0600
+++ c/tests/unit/test_minion.py	2019-07-02 10:58:03.179938595 -0600
@@ -18,8 +18,19 @@
 import salt.utils.event as event
 from salt.exceptions import SaltSystemExit, SaltMasterUnresolvableError
 import salt.syspaths
-import tornado
-import tornado.testing
+try:
+    from tornado4.concurrent import Future as TornadoFuture
+    from tornado4.ioloop import IOLoop
+    from tornado4 import gen
+    from tornado4.testing import AsyncTestCase
+    TORNADO_MODULE_NAME = "tornado4"
+except:
+    from tornado.concurrent import Future as TornadoFuture
+    from tornado.ioloop import IOLoop
+    from tornado import gen
+    from tornado.testing import AsyncTestCase
+    TORNADO_MODULE_NAME = "tornado"
+
 from salt.ext.six.moves import range
 
 
@@ -120,7 +131,7 @@
         mock_data = {'fun': 'foo.bar',
                      'jid': 123}
         mock_jid_queue = [123]
-        minion = salt.minion.Minion(mock_opts, jid_queue=copy.copy(mock_jid_queue), io_loop=tornado.ioloop.IOLoop())
+        minion = salt.minion.Minion(mock_opts, jid_queue=copy.copy(mock_jid_queue), io_loop=IOLoop())
         try:
             ret = minion._handle_decoded_payload(mock_data).result()
             self.assertEqual(minion.jid_queue, mock_jid_queue)
@@ -141,7 +152,7 @@
             mock_data = {'fun': 'foo.bar',
                          'jid': mock_jid}
             mock_jid_queue = [123, 456]
-            minion = salt.minion.Minion(mock_opts, jid_queue=copy.copy(mock_jid_queue), io_loop=tornado.ioloop.IOLoop())
+            minion = salt.minion.Minion(mock_opts, jid_queue=copy.copy(mock_jid_queue), io_loop=IOLoop())
             try:
 
                 # Assert that the minion's jid_queue attribute matches the mock_jid_queue as a baseline
@@ -170,7 +181,7 @@
             mock_data = {'fun': 'foo.bar',
                          'jid': 789}
             mock_jid_queue = [123, 456]
-            minion = salt.minion.Minion(mock_opts, jid_queue=copy.copy(mock_jid_queue), io_loop=tornado.ioloop.IOLoop())
+            minion = salt.minion.Minion(mock_opts, jid_queue=copy.copy(mock_jid_queue), io_loop=IOLoop())
             try:
 
                 # Assert that the minion's jid_queue attribute matches the mock_jid_queue as a baseline
@@ -194,13 +205,13 @@
                 patch('salt.utils.process.SignalHandlingMultiprocessingProcess.start', MagicMock(return_value=True)), \
                 patch('salt.utils.process.SignalHandlingMultiprocessingProcess.join', MagicMock(return_value=True)), \
                 patch('salt.utils.minion.running', MagicMock(return_value=[])), \
-                patch('tornado.gen.sleep', MagicMock(return_value=tornado.concurrent.Future())):
+                patch(TORNADO_MODULE_NAME + '.gen.sleep', MagicMock(return_value=TornadoFuture())):
             process_count_max = 10
             mock_opts = salt.config.DEFAULT_MINION_OPTS
             mock_opts['minion_jid_queue_hwm'] = 100
             mock_opts["process_count_max"] = process_count_max
 
-            io_loop = tornado.ioloop.IOLoop()
+            io_loop = IOLoop()
             minion = salt.minion.Minion(mock_opts, jid_queue=[], io_loop=io_loop)
             try:
 
@@ -241,7 +252,7 @@
                 patch('salt.utils.process.SignalHandlingMultiprocessingProcess.join', MagicMock(return_value=True)):
             mock_opts = self.get_config('minion', from_scratch=True)
             mock_opts['beacons_before_connect'] = True
-            io_loop = tornado.ioloop.IOLoop()
+            io_loop = IOLoop()
             io_loop.make_current()
             minion = salt.minion.Minion(mock_opts, io_loop=io_loop)
             try:
@@ -267,7 +278,7 @@
                 patch('salt.utils.process.SignalHandlingMultiprocessingProcess.join', MagicMock(return_value=True)):
             mock_opts = self.get_config('minion', from_scratch=True)
             mock_opts['scheduler_before_connect'] = True
-            io_loop = tornado.ioloop.IOLoop()
+            io_loop = IOLoop()
             io_loop.make_current()
             minion = salt.minion.Minion(mock_opts, io_loop=io_loop)
             try:
@@ -295,7 +306,7 @@
 
 
 @skipIf(NO_MOCK, NO_MOCK_REASON)
-class MinionAsyncTestCase(TestCase, AdaptedConfigurationTestCaseMixin, tornado.testing.AsyncTestCase):
+class MinionAsyncTestCase(TestCase, AdaptedConfigurationTestCaseMixin, AsyncTestCase):
 
     @skip_if_not_root
     def test_sock_path_len(self):
diff -Naur a/tests/unit/transport/test_ipc.py c/tests/unit/transport/test_ipc.py
--- a/tests/unit/transport/test_ipc.py	2019-07-02 10:15:07.003874717 -0600
+++ c/tests/unit/transport/test_ipc.py	2019-07-02 10:58:03.179938595 -0600
@@ -11,9 +11,14 @@
 import threading
 import logging
 
-import tornado.gen
-import tornado.ioloop
-import tornado.testing
+try:
+    import tornado4.gen as tornado_gen
+    from tornado4.testing import AsyncTestCase
+    from tornado4.ioloop 
+except ImportError:
+    import tornado.gen as tornado_gen
+    from tornado.testing import AsyncTestCase
+    from tornado.ioloop 
 
 import salt.config
 import salt.exceptions
@@ -34,7 +39,7 @@
 
 
 @skipIf(salt.utils.platform.is_windows(), 'Windows does not support Posix IPC')
-class BaseIPCReqCase(tornado.testing.AsyncTestCase):
+class BaseIPCReqCase(AsyncTestCase):
     '''
     Test the req server/client pair
     '''
@@ -72,7 +77,7 @@
         del self.server_channel
         #del self._start_handlers
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(self, payload, reply_func):
         self.payloads.append(payload)
         yield reply_func(payload)
diff -Naur a/tests/unit/transport/test_ipc.py.orig c/tests/unit/transport/test_ipc.py.orig
--- a/tests/unit/transport/test_ipc.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/tests/unit/transport/test_ipc.py.orig	2019-07-02 10:57:25.367937657 -0600
@@ -0,0 +1,268 @@
+# -*- coding: utf-8 -*-
+'''
+    :codeauthor: Mike Place <mp@saltstack.com>
+'''
+
+# Import python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import os
+import errno
+import socket
+import threading
+import logging
+
+import tornado.gen
+import tornado.ioloop
+import tornado.testing
+
+import salt.config
+import salt.exceptions
+import salt.transport.ipc
+import salt.transport.server
+import salt.transport.client
+import salt.utils.platform
+
+from salt.ext import six
+from salt.ext.six.moves import range
+
+# Import Salt Testing libs
+from tests.support.mock import MagicMock
+from tests.support.paths import TMP
+from tests.support.unit import skipIf
+
+log = logging.getLogger(__name__)
+
+
+@skipIf(salt.utils.platform.is_windows(), 'Windows does not support Posix IPC')
+class BaseIPCReqCase(tornado.testing.AsyncTestCase):
+    '''
+    Test the req server/client pair
+    '''
+    def setUp(self):
+        super(BaseIPCReqCase, self).setUp()
+        #self._start_handlers = dict(self.io_loop._handlers)
+        self.socket_path = os.path.join(TMP, 'ipc_test.ipc')
+
+        self.server_channel = salt.transport.ipc.IPCMessageServer(
+            self.socket_path,
+            io_loop=self.io_loop,
+            payload_handler=self._handle_payload,
+        )
+        self.server_channel.start()
+
+        self.payloads = []
+
+    def tearDown(self):
+        super(BaseIPCReqCase, self).tearDown()
+        #failures = []
+        try:
+            self.server_channel.close()
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+        os.unlink(self.socket_path)
+        #for k, v in six.iteritems(self.io_loop._handlers):
+        #    if self._start_handlers.get(k) != v:
+        #        failures.append((k, v))
+        #if len(failures) > 0:
+        #    raise Exception('FDs still attached to the IOLoop: {0}'.format(failures))
+        del self.payloads
+        del self.socket_path
+        del self.server_channel
+        #del self._start_handlers
+
+    @tornado.gen.coroutine
+    def _handle_payload(self, payload, reply_func):
+        self.payloads.append(payload)
+        yield reply_func(payload)
+        if isinstance(payload, dict) and payload.get('stop'):
+            self.stop()
+
+
+class IPCMessageClient(BaseIPCReqCase):
+    '''
+    Test all of the clear msg stuff
+    '''
+
+    def _get_channel(self):
+        if not hasattr(self, 'channel') or self.channel is None:
+            self.channel = salt.transport.ipc.IPCMessageClient(
+                socket_path=self.socket_path,
+                io_loop=self.io_loop,
+            )
+            self.channel.connect(callback=self.stop)
+            self.wait()
+        return self.channel
+
+    def setUp(self):
+        super(IPCMessageClient, self).setUp()
+        self.channel = self._get_channel()
+
+    def tearDown(self):
+        super(IPCMessageClient, self).tearDown()
+        try:
+            # Make sure we close no matter what we've done in the tests
+            del self.channel
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+        finally:
+            self.channel = None
+
+    def test_singleton(self):
+        channel = self._get_channel()
+        assert self.channel is channel
+        # Delete the local channel. Since there's still one more refefence
+        # __del__ wasn't called
+        del channel
+        assert self.channel
+        msg = {'foo': 'bar', 'stop': True}
+        self.channel.send(msg)
+        self.wait()
+        self.assertEqual(self.payloads[0], msg)
+
+    def test_basic_send(self):
+        msg = {'foo': 'bar', 'stop': True}
+        self.channel.send(msg)
+        self.wait()
+        self.assertEqual(self.payloads[0], msg)
+
+    def test_many_send(self):
+        msgs = []
+        self.server_channel.stream_handler = MagicMock()
+
+        for i in range(0, 1000):
+            msgs.append('test_many_send_{0}'.format(i))
+
+        for i in msgs:
+            self.channel.send(i)
+        self.channel.send({'stop': True})
+        self.wait()
+        self.assertEqual(self.payloads[:-1], msgs)
+
+    def test_very_big_message(self):
+        long_str = ''.join([six.text_type(num) for num in range(10**5)])
+        msg = {'long_str': long_str, 'stop': True}
+        self.channel.send(msg)
+        self.wait()
+        self.assertEqual(msg, self.payloads[0])
+
+    def test_multistream_sends(self):
+        local_channel = self._get_channel()
+
+        for c in (self.channel, local_channel):
+            c.send('foo')
+
+        self.channel.send({'stop': True})
+        self.wait()
+        self.assertEqual(self.payloads[:-1], ['foo', 'foo'])
+
+    def test_multistream_errors(self):
+        local_channel = self._get_channel()
+
+        for c in (self.channel, local_channel):
+            c.send(None)
+
+        for c in (self.channel, local_channel):
+            c.send('foo')
+
+        self.channel.send({'stop': True})
+        self.wait()
+        self.assertEqual(self.payloads[:-1], [None, None, 'foo', 'foo'])
+
+
+@skipIf(salt.utils.platform.is_windows(), 'Windows does not support Posix IPC')
+class IPCMessagePubSubCase(tornado.testing.AsyncTestCase):
+    '''
+    Test all of the clear msg stuff
+    '''
+    def setUp(self):
+        super(IPCMessagePubSubCase, self).setUp()
+        self.opts = {'ipc_write_buffer': 0}
+        self.socket_path = os.path.join(TMP, 'ipc_test.ipc')
+        self.pub_channel = self._get_pub_channel()
+        self.sub_channel = self._get_sub_channel()
+
+    def _get_pub_channel(self):
+        pub_channel = salt.transport.ipc.IPCMessagePublisher(
+                self.opts,
+                self.socket_path,
+                )
+        pub_channel.start()
+        return pub_channel
+
+    def _get_sub_channel(self):
+        sub_channel = salt.transport.ipc.IPCMessageSubscriber(
+            socket_path=self.socket_path,
+            io_loop=self.io_loop,
+        )
+        sub_channel.connect(callback=self.stop)
+        self.wait()
+        return sub_channel
+
+    def tearDown(self):
+        super(IPCMessagePubSubCase, self).tearDown()
+        try:
+            self.pub_channel.close()
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+        try:
+            self.sub_channel.close()
+        except socket.error as exc:
+            if exc.errno != errno.EBADF:
+                # If its not a bad file descriptor error, raise
+                raise
+        os.unlink(self.socket_path)
+        del self.pub_channel
+        del self.sub_channel
+
+    def test_multi_client_reading(self):
+        # To be completely fair let's create 2 clients.
+        client1 = self.sub_channel
+        client2 = self._get_sub_channel()
+        call_cnt = []
+
+        # Create a watchdog to be safe from hanging in sync loops (what old code did)
+        evt = threading.Event()
+
+        def close_server():
+            if evt.wait(1):
+                return
+            client2.close()
+            self.stop()
+
+        watchdog = threading.Thread(target=close_server)
+        watchdog.start()
+
+        # Runs in ioloop thread so we're safe from race conditions here
+        def handler(raw):
+            call_cnt.append(raw)
+            if len(call_cnt) >= 2:
+                evt.set()
+                self.stop()
+
+        # Now let both waiting data at once
+        client1.read_async(handler)
+        client2.read_async(handler)
+        self.pub_channel.publish('TEST')
+        self.wait()
+        self.assertEqual(len(call_cnt), 2)
+        self.assertEqual(call_cnt[0], 'TEST')
+        self.assertEqual(call_cnt[1], 'TEST')
+
+    def test_sync_reading(self):
+        # To be completely fair let's create 2 clients.
+        client1 = self.sub_channel
+        client2 = self._get_sub_channel()
+        call_cnt = []
+
+        # Now let both waiting data at once
+        self.pub_channel.publish('TEST')
+        ret1 = client1.read_sync()
+        ret2 = client2.read_sync()
+        self.assertEqual(ret1, 'TEST')
+        self.assertEqual(ret2, 'TEST')
diff -Naur a/tests/unit/transport/test_tcp.py c/tests/unit/transport/test_tcp.py
--- a/tests/unit/transport/test_tcp.py	2019-07-02 10:15:07.003874717 -0600
+++ c/tests/unit/transport/test_tcp.py	2019-07-02 10:58:03.179938595 -0600
@@ -7,10 +7,16 @@
 from __future__ import absolute_import, print_function, unicode_literals
 import threading
 
-import tornado.gen
-import tornado.ioloop
-import tornado.concurrent
-from tornado.testing import AsyncTestCase, gen_test
+try:
+    import tornado4.gen as tornado_gen
+    from tornado4.ioloop import IOLoop, TimeoutError as TornadoTimeoutError
+    from tornado4.concurrent import Future as TornadoFuture
+    from tornado4.testing import AsyncTestCase, gen_test
+except ImportError:
+    import tornado.gen as tornado_gen
+    from tornado.ioloop import IOLoop, TimeoutError as TornadoTimeoutError
+    from tornado.concurrent import Future as TornadoFuture
+    from tornado.testing import AsyncTestCase, gen_test
 
 import salt.config
 from salt.ext import six
@@ -69,7 +75,7 @@
         cls.server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_config)
         cls.server_channel.pre_fork(cls.process_manager)
 
-        cls.io_loop = tornado.ioloop.IOLoop()
+        cls.io_loop = IOLoop()
 
         def run_loop_in_thread(loop):
             loop.make_current()
@@ -93,12 +99,12 @@
             del cls.server_channel
 
     @classmethod
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(cls, payload):
         '''
         TODO: something besides echo
         '''
-        raise tornado.gen.Return((payload, {'fun': 'send_clear'}))
+        raise tornado_gen.Return((payload, {'fun': 'send_clear'}))
 
 
 @skipIf(salt.utils.platform.is_darwin(), 'hanging test suite on MacOS')
@@ -113,12 +119,12 @@
         del self.channel
 
     @classmethod
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(cls, payload):
         '''
         TODO: something besides echo
         '''
-        raise tornado.gen.Return((payload, {'fun': 'send_clear'}))
+        raise tornado_gen.Return((payload, {'fun': 'send_clear'}))
 
 
 @skipIf(salt.utils.platform.is_darwin(), 'hanging test suite on MacOS')
@@ -130,12 +136,12 @@
         del self.channel
 
     @classmethod
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(cls, payload):
         '''
         TODO: something besides echo
         '''
-        raise tornado.gen.Return((payload, {'fun': 'send'}))
+        raise tornado_gen.Return((payload, {'fun': 'send'}))
 
     # TODO: make failed returns have a specific framing so we can raise the same exception
     # on encrypted channels
@@ -192,7 +198,7 @@
         cls.req_server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_config)
         cls.req_server_channel.pre_fork(cls.process_manager)
 
-        cls._server_io_loop = tornado.ioloop.IOLoop()
+        cls._server_io_loop = IOLoop()
         cls.req_server_channel.post_fork(cls._handle_payload, io_loop=cls._server_io_loop)
 
         def run_loop_in_thread(loop):
@@ -291,7 +297,7 @@
             yield self.message_client_pool.connect()
 
         for message_client_mock in self.message_client_pool.message_clients:
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             future.set_result('foo')
             message_client_mock.connect.return_value = future
 
@@ -303,10 +309,10 @@
             yield self.message_client_pool.connect()
 
         for idx, message_client_mock in enumerate(self.message_client_pool.message_clients):
-            future = tornado.concurrent.Future()
+            future = TornadoFuture()
             if idx % 2 == 0:
                 future.set_result('foo')
             message_client_mock.connect.return_value = future
 
-        with self.assertRaises(tornado.ioloop.TimeoutError):
+        with self.assertRaises(TornadoTimeoutError):
             test_connect(self)
diff -Naur a/tests/unit/transport/test_zeromq.py c/tests/unit/transport/test_zeromq.py
--- a/tests/unit/transport/test_zeromq.py	2019-07-02 10:15:07.003874717 -0600
+++ c/tests/unit/transport/test_zeromq.py	2019-07-02 10:58:03.179938595 -0600
@@ -23,8 +23,13 @@
 # support pyzmq 13.0.x, TODO: remove once we force people to 14.0.x
 if not hasattr(zmq.eventloop.ioloop, 'ZMQIOLoop'):
     zmq.eventloop.ioloop.ZMQIOLoop = zmq.eventloop.ioloop.IOLoop
-from tornado.testing import AsyncTestCase
-import tornado.gen
+
+try:
+    import tornado4.gen as tornado_gen
+    from tornado4.testing import AsyncTestCase
+except ImportError:
+    import tornado.gen as tornado_gen
+    from tornado.testing import AsyncTestCase
 
 # Import Salt libs
 import salt.config
@@ -138,12 +143,12 @@
         del self.channel
 
     @classmethod
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(cls, payload):
         '''
         TODO: something besides echo
         '''
-        raise tornado.gen.Return((payload, {'fun': 'send_clear'}))
+        raise tornado_gen.Return((payload, {'fun': 'send_clear'}))
 
     def test_master_uri_override(self):
         '''
@@ -167,12 +172,12 @@
         del self.channel
 
     @classmethod
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def _handle_payload(cls, payload):
         '''
         TODO: something besides echo
         '''
-        raise tornado.gen.Return((payload, {'fun': 'send'}))
+        raise tornado_gen.Return((payload, {'fun': 'send'}))
 
     # TODO: make failed returns have a specific framing so we can raise the same exception
     # on encrypted channels
diff -Naur a/tests/unit/transport/test_zeromq.py.orig c/tests/unit/transport/test_zeromq.py.orig
--- a/tests/unit/transport/test_zeromq.py.orig	1969-12-31 17:00:00.000000000 -0700
+++ c/tests/unit/transport/test_zeromq.py.orig	2019-07-02 10:57:25.367937657 -0600
@@ -0,0 +1,687 @@
+# -*- coding: utf-8 -*-
+'''
+    :codeauthor: Thomas Jackson <jacksontj.89@gmail.com>
+'''
+
+# Import python libs
+from __future__ import absolute_import, print_function, unicode_literals
+import os
+import time
+import threading
+import multiprocessing
+import ctypes
+from concurrent.futures.thread import ThreadPoolExecutor
+
+# linux_distribution deprecated in py3.7
+try:
+    from platform import linux_distribution
+except ImportError:
+    from distro import linux_distribution
+
+# Import 3rd-party libs
+import zmq.eventloop.ioloop
+# support pyzmq 13.0.x, TODO: remove once we force people to 14.0.x
+if not hasattr(zmq.eventloop.ioloop, 'ZMQIOLoop'):
+    zmq.eventloop.ioloop.ZMQIOLoop = zmq.eventloop.ioloop.IOLoop
+from tornado.testing import AsyncTestCase
+import tornado.gen
+
+# Import Salt libs
+import salt.config
+import salt.log.setup
+from salt.ext import six
+import salt.utils.process
+import salt.utils.platform
+import salt.transport.server
+import salt.transport.client
+import salt.exceptions
+from salt.ext.six.moves import range
+from salt.transport.zeromq import AsyncReqMessageClientPool
+
+# Import test support libs
+from tests.support.paths import TMP_CONF_DIR
+from tests.support.unit import TestCase, skipIf
+from tests.support.helpers import flaky, get_unused_localhost_port
+from tests.support.mixins import AdaptedConfigurationTestCaseMixin
+from tests.support.mock import MagicMock, patch
+from tests.unit.transport.mixins import PubChannelMixin, ReqChannelMixin
+
+ON_SUSE = False
+if 'SuSE' in linux_distribution(full_distribution_name=False):
+    ON_SUSE = True
+
+
+class BaseZMQReqCase(TestCase, AdaptedConfigurationTestCaseMixin):
+    '''
+    Test the req server/client pair
+    '''
+    @classmethod
+    def setUpClass(cls):
+        if not hasattr(cls, '_handle_payload'):
+            return
+        ret_port = get_unused_localhost_port()
+        publish_port = get_unused_localhost_port()
+        tcp_master_pub_port = get_unused_localhost_port()
+        tcp_master_pull_port = get_unused_localhost_port()
+        tcp_master_publish_pull = get_unused_localhost_port()
+        tcp_master_workers = get_unused_localhost_port()
+        cls.master_config = cls.get_temp_config(
+            'master',
+            **{'transport': 'zeromq',
+               'auto_accept': True,
+               'ret_port': ret_port,
+               'publish_port': publish_port,
+               'tcp_master_pub_port': tcp_master_pub_port,
+               'tcp_master_pull_port': tcp_master_pull_port,
+               'tcp_master_publish_pull': tcp_master_publish_pull,
+               'tcp_master_workers': tcp_master_workers}
+        )
+
+        cls.minion_config = cls.get_temp_config(
+            'minion',
+            **{'transport': 'zeromq',
+               'master_ip': '127.0.0.1',
+               'master_port': ret_port,
+               'auth_timeout': 5,
+               'auth_tries': 1,
+               'master_uri': 'tcp://127.0.0.1:{0}'.format(ret_port)}
+        )
+
+        cls.process_manager = salt.utils.process.ProcessManager(name='ReqServer_ProcessManager')
+
+        cls.server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_config)
+        cls.server_channel.pre_fork(cls.process_manager)
+
+        cls.io_loop = zmq.eventloop.ioloop.ZMQIOLoop()
+        cls.io_loop.make_current()
+        cls.server_channel.post_fork(cls._handle_payload, io_loop=cls.io_loop)
+
+        cls.server_thread = threading.Thread(target=cls.io_loop.start)
+        cls.server_thread.daemon = True
+        cls.server_thread.start()
+
+    @classmethod
+    def tearDownClass(cls):
+        if not hasattr(cls, '_handle_payload'):
+            return
+        # Attempting to kill the children hangs the test suite.
+        # Let the test suite handle this instead.
+        cls.process_manager.stop_restarting()
+        cls.process_manager.kill_children()
+        cls.io_loop.add_callback(cls.io_loop.stop)
+        cls.server_thread.join()
+        time.sleep(2)  # Give the procs a chance to fully close before we stop the io_loop
+        cls.server_channel.close()
+        del cls.server_channel
+        del cls.io_loop
+        del cls.process_manager
+        del cls.server_thread
+        del cls.master_config
+        del cls.minion_config
+
+    @classmethod
+    def _handle_payload(cls, payload):
+        '''
+        TODO: something besides echo
+        '''
+        return payload, {'fun': 'send_clear'}
+
+
+class ClearReqTestCases(BaseZMQReqCase, ReqChannelMixin):
+    '''
+    Test all of the clear msg stuff
+    '''
+    def setUp(self):
+        self.channel = salt.transport.client.ReqChannel.factory(self.minion_config, crypt='clear')
+
+    def tearDown(self):
+        del self.channel
+
+    @classmethod
+    @tornado.gen.coroutine
+    def _handle_payload(cls, payload):
+        '''
+        TODO: something besides echo
+        '''
+        raise tornado.gen.Return((payload, {'fun': 'send_clear'}))
+
+    def test_master_uri_override(self):
+        '''
+        ensure master_uri kwarg is respected
+        '''
+        # minion_config should be 127.0.0.1, we want a different uri that still connects
+        uri = 'tcp://{master_ip}:{master_port}'.format(master_ip='localhost', master_port=self.minion_config['master_port'])
+
+        channel = salt.transport.Channel.factory(self.minion_config, master_uri=uri)
+        self.assertIn('localhost', channel.master_uri)
+        del channel
+
+
+@flaky
+@skipIf(ON_SUSE, 'Skipping until https://github.com/saltstack/salt/issues/32902 gets fixed')
+class AESReqTestCases(BaseZMQReqCase, ReqChannelMixin):
+    def setUp(self):
+        self.channel = salt.transport.client.ReqChannel.factory(self.minion_config)
+
+    def tearDown(self):
+        del self.channel
+
+    @classmethod
+    @tornado.gen.coroutine
+    def _handle_payload(cls, payload):
+        '''
+        TODO: something besides echo
+        '''
+        raise tornado.gen.Return((payload, {'fun': 'send'}))
+
+    # TODO: make failed returns have a specific framing so we can raise the same exception
+    # on encrypted channels
+    #
+    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+    #
+    # WARNING: This test will fail randomly on any system with > 1 CPU core!!!
+    #
+    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+    def test_badload(self):
+        '''
+        Test a variety of bad requests, make sure that we get some sort of error
+        '''
+        # TODO: This test should be re-enabled when Jenkins moves to C7.
+        # Once the version of salt-testing is increased to something newer than the September
+        # release of salt-testing, the @flaky decorator should be applied to this test.
+        msgs = ['', [], tuple()]
+        for msg in msgs:
+            with self.assertRaises(salt.exceptions.AuthenticationError):
+                ret = self.channel.send(msg, timeout=5)
+
+
+class BaseZMQPubCase(AsyncTestCase, AdaptedConfigurationTestCaseMixin):
+    '''
+    Test the req server/client pair
+    '''
+    @classmethod
+    def setUpClass(cls):
+        ret_port = get_unused_localhost_port()
+        publish_port = get_unused_localhost_port()
+        tcp_master_pub_port = get_unused_localhost_port()
+        tcp_master_pull_port = get_unused_localhost_port()
+        tcp_master_publish_pull = get_unused_localhost_port()
+        tcp_master_workers = get_unused_localhost_port()
+        cls.master_config = cls.get_temp_config(
+            'master',
+            **{'transport': 'zeromq',
+               'auto_accept': True,
+               'ret_port': ret_port,
+               'publish_port': publish_port,
+               'tcp_master_pub_port': tcp_master_pub_port,
+               'tcp_master_pull_port': tcp_master_pull_port,
+               'tcp_master_publish_pull': tcp_master_publish_pull,
+               'tcp_master_workers': tcp_master_workers}
+        )
+
+        cls.minion_config = salt.config.minion_config(os.path.join(TMP_CONF_DIR, 'minion'))
+        cls.minion_config = cls.get_temp_config(
+            'minion',
+            **{'transport': 'zeromq',
+               'master_ip': '127.0.0.1',
+               'master_port': ret_port,
+               'master_uri': 'tcp://127.0.0.1:{0}'.format(ret_port)}
+        )
+
+        cls.process_manager = salt.utils.process.ProcessManager(name='ReqServer_ProcessManager')
+
+        cls.server_channel = salt.transport.server.PubServerChannel.factory(cls.master_config)
+        cls.server_channel.pre_fork(cls.process_manager)
+
+        # we also require req server for auth
+        cls.req_server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_config)
+        cls.req_server_channel.pre_fork(cls.process_manager)
+
+        cls._server_io_loop = zmq.eventloop.ioloop.ZMQIOLoop()
+        cls.req_server_channel.post_fork(cls._handle_payload, io_loop=cls._server_io_loop)
+
+        cls.server_thread = threading.Thread(target=cls._server_io_loop.start)
+        cls.server_thread.daemon = True
+        cls.server_thread.start()
+
+    @classmethod
+    def tearDownClass(cls):
+        cls.process_manager.kill_children()
+        cls.process_manager.stop_restarting()
+        time.sleep(2)  # Give the procs a chance to fully close before we stop the io_loop
+        cls.io_loop.add_callback(cls.io_loop.stop)
+        cls.server_thread.join()
+        cls.req_server_channel.close()
+        cls.server_channel.close()
+        cls._server_io_loop.stop()
+        del cls.server_channel
+        del cls._server_io_loop
+        del cls.process_manager
+        del cls.server_thread
+        del cls.master_config
+        del cls.minion_config
+
+    @classmethod
+    def _handle_payload(cls, payload):
+        '''
+        TODO: something besides echo
+        '''
+        return payload, {'fun': 'send_clear'}
+
+    def setUp(self):
+        super(BaseZMQPubCase, self).setUp()
+        self._start_handlers = dict(self.io_loop._handlers)
+
+    def tearDown(self):
+        super(BaseZMQPubCase, self).tearDown()
+        failures = []
+        for k, v in six.iteritems(self.io_loop._handlers):
+            if self._start_handlers.get(k) != v:
+                failures.append((k, v))
+        del self._start_handlers
+        if len(failures) > 0:
+            raise Exception('FDs still attached to the IOLoop: {0}'.format(failures))
+
+
+@skipIf(True, 'Skip until we can devote time to fix this test')
+class AsyncPubChannelTest(BaseZMQPubCase, PubChannelMixin):
+    '''
+    Tests around the publish system
+    '''
+    def get_new_ioloop(self):
+        return zmq.eventloop.ioloop.ZMQIOLoop()
+
+
+class AsyncReqMessageClientPoolTest(TestCase):
+    def setUp(self):
+        super(AsyncReqMessageClientPoolTest, self).setUp()
+        sock_pool_size = 5
+        with patch('salt.transport.zeromq.AsyncReqMessageClient.__init__', MagicMock(return_value=None)):
+            self.message_client_pool = AsyncReqMessageClientPool({'sock_pool_size': sock_pool_size},
+                                                                 args=({}, ''))
+        self.original_message_clients = self.message_client_pool.message_clients
+        self.message_client_pool.message_clients = [MagicMock() for _ in range(sock_pool_size)]
+
+    def tearDown(self):
+        with patch('salt.transport.zeromq.AsyncReqMessageClient.destroy', MagicMock(return_value=None)):
+            del self.original_message_clients
+        super(AsyncReqMessageClientPoolTest, self).tearDown()
+
+    def test_send(self):
+        for message_client_mock in self.message_client_pool.message_clients:
+            message_client_mock.send_queue = [0, 0, 0]
+            message_client_mock.send.return_value = []
+
+        self.assertEqual([], self.message_client_pool.send())
+
+        self.message_client_pool.message_clients[2].send_queue = [0]
+        self.message_client_pool.message_clients[2].send.return_value = [1]
+        self.assertEqual([1], self.message_client_pool.send())
+
+    def test_destroy(self):
+        self.message_client_pool.destroy()
+        self.assertEqual([], self.message_client_pool.message_clients)
+
+
+class ZMQConfigTest(TestCase):
+    def test_master_uri(self):
+        '''
+        test _get_master_uri method
+        '''
+
+        m_ip = '127.0.0.1'
+        m_port = 4505
+        s_ip = '111.1.0.1'
+        s_port = 4058
+
+        m_ip6 = '1234:5678::9abc'
+        s_ip6 = '1234:5678::1:9abc'
+
+        with patch('salt.transport.zeromq.LIBZMQ_VERSION_INFO', (4, 1, 6)), \
+            patch('salt.transport.zeromq.ZMQ_VERSION_INFO', (16, 0, 1)):
+            # pass in both source_ip and source_port
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip,
+                                                         master_port=m_port,
+                                                         source_ip=s_ip,
+                                                         source_port=s_port) == 'tcp://{0}:{1};{2}:{3}'.format(s_ip, s_port, m_ip, m_port)
+
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip6,
+                                                         master_port=m_port,
+                                                         source_ip=s_ip6,
+                                                         source_port=s_port) == 'tcp://[{0}]:{1};[{2}]:{3}'.format(s_ip6, s_port, m_ip6, m_port)
+
+            # source ip and source_port empty
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip,
+                                                         master_port=m_port) == 'tcp://{0}:{1}'.format(m_ip, m_port)
+
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip6,
+                                                         master_port=m_port) == 'tcp://[{0}]:{1}'.format(m_ip6, m_port)
+
+            # pass in only source_ip
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip,
+                                                         master_port=m_port,
+                                                         source_ip=s_ip) == 'tcp://{0}:0;{1}:{2}'.format(s_ip, m_ip, m_port)
+
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip6,
+                                                         master_port=m_port,
+                                                         source_ip=s_ip6) == 'tcp://[{0}]:0;[{1}]:{2}'.format(s_ip6, m_ip6, m_port)
+
+            # pass in only source_port
+            assert salt.transport.zeromq._get_master_uri(master_ip=m_ip,
+                                                         master_port=m_port,
+                                                         source_port=s_port) == 'tcp://0.0.0.0:{0};{1}:{2}'.format(s_port, m_ip, m_port)
+
+
+class PubServerChannel(TestCase, AdaptedConfigurationTestCaseMixin):
+
+    @classmethod
+    def setUpClass(cls):
+        ret_port = get_unused_localhost_port()
+        publish_port = get_unused_localhost_port()
+        tcp_master_pub_port = get_unused_localhost_port()
+        tcp_master_pull_port = get_unused_localhost_port()
+        tcp_master_publish_pull = get_unused_localhost_port()
+        tcp_master_workers = get_unused_localhost_port()
+        cls.master_config = cls.get_temp_config(
+            'master',
+            **{'transport': 'zeromq',
+               'auto_accept': True,
+               'ret_port': ret_port,
+               'publish_port': publish_port,
+               'tcp_master_pub_port': tcp_master_pub_port,
+               'tcp_master_pull_port': tcp_master_pull_port,
+               'tcp_master_publish_pull': tcp_master_publish_pull,
+               'tcp_master_workers': tcp_master_workers,
+               'sign_pub_messages': False,
+            }
+        )
+        salt.master.SMaster.secrets['aes'] = {
+            'secret': multiprocessing.Array(
+                ctypes.c_char,
+                six.b(salt.crypt.Crypticle.generate_key_string()),
+            ),
+        }
+        cls.minion_config = cls.get_temp_config(
+            'minion',
+            **{'transport': 'zeromq',
+               'master_ip': '127.0.0.1',
+               'master_port': ret_port,
+               'auth_timeout': 5,
+               'auth_tries': 1,
+               'master_uri': 'tcp://127.0.0.1:{0}'.format(ret_port)}
+        )
+
+    @classmethod
+    def tearDownClass(cls):
+        del cls.minion_config
+        del cls.master_config
+
+    def setUp(self):
+        # Start the event loop, even though we dont directly use this with
+        # ZeroMQPubServerChannel, having it running seems to increase the
+        # likely hood of dropped messages.
+        self.io_loop = zmq.eventloop.ioloop.ZMQIOLoop()
+        self.io_loop.make_current()
+        self.io_loop_thread = threading.Thread(target=self.io_loop.start)
+        self.io_loop_thread.start()
+        self.process_manager = salt.utils.process.ProcessManager(name='PubServer_ProcessManager')
+
+    def tearDown(self):
+        self.io_loop.add_callback(self.io_loop.stop)
+        self.io_loop_thread.join()
+        self.process_manager.stop_restarting()
+        self.process_manager.kill_children()
+        del self.io_loop
+        del self.io_loop_thread
+        del self.process_manager
+
+    @staticmethod
+    def _gather_results(opts, pub_uri, results, timeout=120, messages=None):
+        '''
+        Gather results until then number of seconds specified by timeout passes
+        without reveiving a message
+        '''
+        ctx = zmq.Context()
+        sock = ctx.socket(zmq.SUB)
+        sock.setsockopt(zmq.LINGER, -1)
+        sock.setsockopt(zmq.SUBSCRIBE, b'')
+        sock.connect(pub_uri)
+        last_msg = time.time()
+        serial = salt.payload.Serial(opts)
+        crypticle = salt.crypt.Crypticle(opts, salt.master.SMaster.secrets['aes']['secret'].value)
+        while time.time() - last_msg < timeout:
+            try:
+                payload = sock.recv(zmq.NOBLOCK)
+            except zmq.ZMQError:
+                time.sleep(.01)
+            else:
+                if messages:
+                    if messages != 1:
+                        messages -= 1
+                        continue
+                payload = crypticle.loads(serial.loads(payload)['load'])
+                if 'stop' in payload:
+                    break
+                last_msg = time.time()
+                results.append(payload['jid'])
+        return results
+
+    @skipIf(salt.utils.platform.is_windows(), 'Skip on Windows OS')
+    def test_publish_to_pubserv_ipc(self):
+        '''
+        Test sending 10K messags to ZeroMQPubServerChannel using IPC transport
+
+        ZMQ's ipc transport not supported on Windows
+        '''
+        opts = dict(self.master_config, ipc_mode='ipc', pub_hwm=0)
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        server_channel.pre_fork(self.process_manager, kwargs={
+            'log_queue': salt.log.setup.get_multiprocessing_logging_queue()
+        })
+        pub_uri = 'tcp://{interface}:{publish_port}'.format(**server_channel.opts)
+        send_num = 10000
+        expect = []
+        results = []
+        gather = threading.Thread(target=self._gather_results, args=(self.minion_config, pub_uri, results,))
+        gather.start()
+        # Allow time for server channel to start, especially on windows
+        time.sleep(2)
+        for i in range(send_num):
+            expect.append(i)
+            load = {'tgt_type': 'glob', 'tgt': '*', 'jid': i}
+            server_channel.publish(load)
+        server_channel.publish(
+            {'tgt_type': 'glob', 'tgt': '*', 'stop': True}
+        )
+        gather.join()
+        server_channel.pub_close()
+        assert len(results) == send_num, (len(results), set(expect).difference(results))
+
+    def test_zeromq_zeromq_filtering_decode_message_no_match(self):
+        '''
+        test AsyncZeroMQPubChannel _decode_messages when
+        zmq_filtering enabled and minion does not match
+        '''
+        message = [b'4f26aeafdb2367620a393c973eddbe8f8b846eb',
+                   b'\x82\xa3enc\xa3aes\xa4load\xda\x00`\xeeR\xcf'
+                   b'\x0eaI#V\x17if\xcf\xae\x05\xa7\xb3bN\xf7\xb2\xe2'
+                   b'\xd0sF\xd1\xd4\xecB\xe8\xaf"/*ml\x80Q3\xdb\xaexg'
+                   b'\x8e\x8a\x8c\xd3l\x03\\,J\xa7\x01i\xd1:]\xe3\x8d'
+                   b'\xf4\x03\x88K\x84\n`\xe8\x9a\xad\xad\xc6\x8ea\x15>'
+                   b'\x92m\x9e\xc7aM\x11?\x18;\xbd\x04c\x07\x85\x99\xa3\xea[\x00D']
+
+        opts = dict(self.master_config, ipc_mode='ipc',
+                    pub_hwm=0, zmq_filtering=True, recon_randomize=False,
+                    recon_default=1, recon_max=2, master_ip='127.0.0.1',
+                    acceptance_wait_time=5, acceptance_wait_time_max=5)
+        opts['master_uri'] = 'tcp://{interface}:{publish_port}'.format(**opts)
+
+        server_channel = salt.transport.zeromq.AsyncZeroMQPubChannel(opts)
+        with patch('salt.crypt.AsyncAuth.crypticle',
+                   MagicMock(return_value={'tgt_type': 'glob', 'tgt': '*',
+                                           'jid': 1})) as mock_test:
+            res = server_channel._decode_messages(message)
+        assert res.result() is None
+
+    def test_zeromq_zeromq_filtering_decode_message(self):
+        '''
+        test AsyncZeroMQPubChannel _decode_messages
+        when zmq_filtered enabled
+        '''
+        message = [b'4f26aeafdb2367620a393c973eddbe8f8b846ebd',
+                   b'\x82\xa3enc\xa3aes\xa4load\xda\x00`\xeeR\xcf'
+                   b'\x0eaI#V\x17if\xcf\xae\x05\xa7\xb3bN\xf7\xb2\xe2'
+                   b'\xd0sF\xd1\xd4\xecB\xe8\xaf"/*ml\x80Q3\xdb\xaexg'
+                   b'\x8e\x8a\x8c\xd3l\x03\\,J\xa7\x01i\xd1:]\xe3\x8d'
+                   b'\xf4\x03\x88K\x84\n`\xe8\x9a\xad\xad\xc6\x8ea\x15>'
+                   b'\x92m\x9e\xc7aM\x11?\x18;\xbd\x04c\x07\x85\x99\xa3\xea[\x00D']
+
+        opts = dict(self.master_config, ipc_mode='ipc',
+                    pub_hwm=0, zmq_filtering=True, recon_randomize=False,
+                    recon_default=1, recon_max=2, master_ip='127.0.0.1',
+                    acceptance_wait_time=5, acceptance_wait_time_max=5)
+        opts['master_uri'] = 'tcp://{interface}:{publish_port}'.format(**opts)
+
+        server_channel = salt.transport.zeromq.AsyncZeroMQPubChannel(opts)
+        with patch('salt.crypt.AsyncAuth.crypticle',
+                   MagicMock(return_value={'tgt_type': 'glob', 'tgt': '*',
+                                           'jid': 1})) as mock_test:
+            res = server_channel._decode_messages(message)
+
+        assert res.result()['enc'] == 'aes'
+
+    @skipIf(salt.utils.platform.is_windows(), 'Skip on Windows OS')
+    def test_zeromq_filtering(self):
+        '''
+        Test sending messags to publisher using UDP
+        with zeromq_filtering enabled
+        '''
+        opts = dict(self.master_config, ipc_mode='ipc',
+                    pub_hwm=0, zmq_filtering=True, acceptance_wait_time=5)
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        server_channel.pre_fork(self.process_manager, kwargs={
+            'log_queue': salt.log.setup.get_multiprocessing_logging_queue()
+        })
+        pub_uri = 'tcp://{interface}:{publish_port}'.format(**server_channel.opts)
+        send_num = 1
+        expect = []
+        results = []
+        gather = threading.Thread(target=self._gather_results,
+                                  args=(self.minion_config, pub_uri, results,),
+                                  kwargs={'messages': 2})
+        gather.start()
+        # Allow time for server channel to start, especially on windows
+        time.sleep(2)
+        expect.append(send_num)
+        load = {'tgt_type': 'glob', 'tgt': '*', 'jid': send_num}
+        with patch('salt.utils.minions.CkMinions.check_minions',
+                   MagicMock(return_value={'minions': ['minion'], 'missing': [],
+                                           'ssh_minions': False})):
+            server_channel.publish(load)
+        server_channel.publish(
+            {'tgt_type': 'glob', 'tgt': '*', 'stop': True}
+        )
+        gather.join()
+        server_channel.pub_close()
+        assert len(results) == send_num, (len(results), set(expect).difference(results))
+
+    def test_publish_to_pubserv_tcp(self):
+        '''
+        Test sending 10K messags to ZeroMQPubServerChannel using TCP transport
+        '''
+        opts = dict(self.master_config, ipc_mode='tcp', pub_hwm=0)
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        server_channel.pre_fork(self.process_manager, kwargs={
+            'log_queue': salt.log.setup.get_multiprocessing_logging_queue()
+        })
+        pub_uri = 'tcp://{interface}:{publish_port}'.format(**server_channel.opts)
+        send_num = 10000
+        expect = []
+        results = []
+        gather = threading.Thread(target=self._gather_results, args=(self.minion_config, pub_uri, results,))
+        gather.start()
+        # Allow time for server channel to start, especially on windows
+        time.sleep(2)
+        for i in range(send_num):
+            expect.append(i)
+            load = {'tgt_type': 'glob', 'tgt': '*', 'jid': i}
+            server_channel.publish(load)
+        gather.join()
+        server_channel.pub_close()
+        assert len(results) == send_num, (len(results), set(expect).difference(results))
+
+    @staticmethod
+    def _send_small(opts, sid, num=10):
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        for i in range(num):
+            load = {'tgt_type': 'glob', 'tgt': '*', 'jid': '{}-{}'.format(sid, i)}
+            server_channel.publish(load)
+
+    @staticmethod
+    def _send_large(opts, sid, num=10, size=250000 * 3):
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        for i in range(num):
+            load = {'tgt_type': 'glob', 'tgt': '*', 'jid': '{}-{}'.format(sid, i), 'xdata': '0' * size}
+            server_channel.publish(load)
+
+    def test_issue_36469_tcp(self):
+        '''
+        Test sending both large and small messags to publisher using TCP
+
+        https://github.com/saltstack/salt/issues/36469
+        '''
+        opts = dict(self.master_config, ipc_mode='tcp', pub_hwm=0)
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        server_channel.pre_fork(self.process_manager, kwargs={
+            'log_queue': salt.log.setup.get_multiprocessing_logging_queue()
+        })
+        send_num = 10 * 4
+        expect = []
+        results = []
+        pub_uri = 'tcp://{interface}:{publish_port}'.format(**opts)
+        # Allow time for server channel to start, especially on windows
+        time.sleep(2)
+        gather = threading.Thread(target=self._gather_results, args=(self.minion_config, pub_uri, results,))
+        gather.start()
+        with ThreadPoolExecutor(max_workers=4) as executor:
+            executor.submit(self._send_small, opts, 1)
+            executor.submit(self._send_small, opts, 2)
+            executor.submit(self._send_small, opts, 3)
+            executor.submit(self._send_large, opts, 4)
+        expect = ['{}-{}'.format(a, b) for a in range(10) for b in (1, 2, 3, 4)]
+        server_channel.publish({'tgt_type': 'glob', 'tgt': '*', 'stop': True})
+        gather.join()
+        server_channel.pub_close()
+        assert len(results) == send_num, (len(results), set(expect).difference(results))
+
+    @skipIf(salt.utils.platform.is_windows(), 'Skip on Windows OS')
+    def test_issue_36469_udp(self):
+        '''
+        Test sending both large and small messags to publisher using UDP
+
+        https://github.com/saltstack/salt/issues/36469
+        '''
+        opts = dict(self.master_config, ipc_mode='udp', pub_hwm=0)
+        server_channel = salt.transport.zeromq.ZeroMQPubServerChannel(opts)
+        server_channel.pre_fork(self.process_manager, kwargs={
+            'log_queue': salt.log.setup.get_multiprocessing_logging_queue()
+        })
+        send_num = 10 * 4
+        expect = []
+        results = []
+        pub_uri = 'tcp://{interface}:{publish_port}'.format(**opts)
+        # Allow time for server channel to start, especially on windows
+        time.sleep(2)
+        gather = threading.Thread(target=self._gather_results, args=(self.minion_config, pub_uri, results,))
+        gather.start()
+        with ThreadPoolExecutor(max_workers=4) as executor:
+            executor.submit(self._send_small, opts, 1)
+            executor.submit(self._send_small, opts, 2)
+            executor.submit(self._send_small, opts, 3)
+            executor.submit(self._send_large, opts, 4)
+        expect = ['{}-{}'.format(a, b) for a in range(10) for b in (1, 2, 3, 4)]
+        time.sleep(0.1)
+        server_channel.publish({'tgt_type': 'glob', 'tgt': '*', 'stop': True})
+        gather.join()
+        server_channel.pub_close()
+        assert len(results) == send_num, (len(results), set(expect).difference(results))
diff -Naur a/tests/unit/utils/test_asynchronous.py c/tests/unit/utils/test_asynchronous.py
--- a/tests/unit/utils/test_asynchronous.py	2019-07-02 10:15:06.987874716 -0600
+++ c/tests/unit/utils/test_asynchronous.py	2019-07-02 10:58:03.179938595 -0600
@@ -4,9 +4,12 @@
 from __future__ import absolute_import, print_function, unicode_literals
 
 # Import 3rd-party libs
-import tornado.testing
-import tornado.gen
-from tornado.testing import AsyncTestCase
+try:
+    import tornado4.gen as tornado_gen
+    from tornado4.testing import AsyncTestCase, gen_test
+except ImportError:
+    import tornado.gen as tornado_gen
+    from tornado.testing import AsyncTestCase, gen_test
 
 import salt.utils.asynchronous as asynchronous
 
@@ -15,10 +18,10 @@
     def __init__(self, io_loop=None):
         pass
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def sleep(self):
-        yield tornado.gen.sleep(0.5)
-        raise tornado.gen.Return(True)
+        yield tornado_gen.sleep(0.5)
+        raise tornado_gen.Return(True)
 
 
 class HelperB(object):
@@ -27,15 +30,15 @@
             a = asynchronous.SyncWrapper(HelperA)
         self.a = a
 
-    @tornado.gen.coroutine
+    @tornado_gen.coroutine
     def sleep(self):
-        yield tornado.gen.sleep(0.5)
+        yield tornado_gen.sleep(0.5)
         self.a.sleep()
-        raise tornado.gen.Return(False)
+        raise tornado_gen.Return(False)
 
 
 class TestSyncWrapper(AsyncTestCase):
-    @tornado.testing.gen_test
+    @gen_test
     def test_helpers(self):
         '''
         Test that the helper classes do what we expect within a regular asynchronous env
diff -Naur a/tests/unit/utils/test_context.py c/tests/unit/utils/test_context.py
--- a/tests/unit/utils/test_context.py	2019-07-02 10:15:06.991874717 -0600
+++ c/tests/unit/utils/test_context.py	2019-07-02 10:58:03.179938595 -0600
@@ -5,9 +5,15 @@
 '''
 # Import python libs
 from __future__ import absolute_import
-import tornado.stack_context
-import tornado.gen
-from tornado.testing import AsyncTestCase, gen_test
+try:
+    import tornado4.gen as tornado_gen
+    from tornado4.testing import AsyncTestCase, gen_test
+    from tornado4.stack_context import StackContext, run_with_stack_context
+except ImportError:
+    import tornado.gen as tornado_gen
+    from tornado.testing import AsyncTestCase, gen_test
+    from tornado.stack_context import StackContext, run_with_stack_context
+
 import threading
 import time
 
@@ -66,11 +72,11 @@
     def test_coroutines(self):
         '''Verify that ContextDict overrides properly within coroutines
         '''
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def secondary_coroutine(over):
-            raise tornado.gen.Return(over.get('foo'))
+            raise tornado_gen.Return(over.get('foo'))
 
-        @tornado.gen.coroutine
+        @tornado_gen.coroutine
         def tgt(x, s, over):
             inner_ret = []
             # first grab the global
@@ -81,13 +87,13 @@
             over['foo'] = x
             inner_ret.append(over.get('foo'))
             # sleep for some time to let other coroutines do this section of code
-            yield tornado.gen.sleep(s)
+            yield tornado_gen.sleep(s)
             # get the value of the global again.
             inner_ret.append(over.get('foo'))
             # Call another coroutine to verify that we keep our context
             r = yield secondary_coroutine(over)
             inner_ret.append(r)
-            raise tornado.gen.Return(inner_ret)
+            raise tornado_gen.Return(inner_ret)
 
         futures = []
 
@@ -95,13 +101,13 @@
             s = self.num_concurrent_tasks - x
             over = self.cd.clone()
 
-            f = tornado.stack_context.run_with_stack_context(
-                tornado.stack_context.StackContext(lambda: over),  # pylint: disable=W0640
-                lambda: tgt(x, s/5.0, over),  # pylint: disable=W0640
-            )
+            f = run_with_stack_context(
+                    StackContext(lambda: over),  # pylint: disable=W0640
+                    lambda: tgt(x, s/5.0, over),  # pylint: disable=W0640
+                )
             futures.append(f)
 
-        wait_iterator = tornado.gen.WaitIterator(*futures)
+        wait_iterator = tornado_gen.WaitIterator(*futures)
         while not wait_iterator.done():
             r = yield wait_iterator.next()  # pylint: disable=incompatible-py3-code
             self.assertEqual(r[0], r[1])  # verify that the global value remails
diff -Naur a/tests/unit/utils/test_event.py c/tests/unit/utils/test_event.py
--- a/tests/unit/utils/test_event.py	2019-07-02 10:15:06.987874716 -0600
+++ c/tests/unit/utils/test_event.py	2019-07-02 10:58:03.183938595 -0600
@@ -12,7 +12,12 @@
 import os
 import hashlib
 import time
-from tornado.testing import AsyncTestCase
+
+try:
+    from tornado4.testing import AsyncTestCase
+except ImportError:
+    from tornado.testing import AsyncTestCase
+
 import zmq
 import zmq.eventloop.ioloop
 # support pyzmq 13.0.x, TODO: remove once we force people to 14.0.x
